# Memory Systems

## Main Memory

- We have a memory hierarchy to balance the tradeoff between cost and speed
- Want to exploit temporal and spatial locality
- Moore's law is long dead and never really applied to memory
- The basic element of main memory is a memory cell capable of being written or read
  - Need to indicate read/write, data input, and also an enable line
- When organising memory cells into a larger chip, it is important to maintain a structure approach and keep the circuit as compact as possible
  - For example, a 16 word x 8 bit memory chip requires 128 cells and 4-bit addresses
  - A 1024 bit device as a 128x8 array requires 7 address pins and 8 data pins
    - Alternatively, it is possible to organise it as a 1024x1 array, which would be really dumb as it would need 10 address pins
- Can use the same principle to build smaller ICs into larger ICs, using decoders/multiplexers to split address spaces
- Semiconductor memory is generally whats used for main store, Random Access Memory
- Two main technologies:
  - Static RAM (SRAM) uses a flip-flop as a storage element for each bit
  - Dynamic RAM (DRAM) uses the presence or lack of charge in a capacitor for each bit
    - Charge leaks away over time so needs refreshing, but DRAM is generally cheaper if the overhead of the refresh circuitry is sufficiently amortised
  - SRAM typically faster so is used for cache
  - DRAM used for main memory
- The interface to main memory is always a bottleneck so we can do some fancy DRAM organisations stuff
  - Synchronous DRAM exchanges data with the processor according to an external clock memory
    - Clock runs at the speed of the bus to avoid waiting on memory
    - Processor can perform other tasks while
- ROM typically used in microprogramming or systems stuff
  - ROM is mask-written read only memory
  - PROM is same as above, but electrically written
  - EPROM is same as above, but is erasable via UV light at the chip level
  - EEPROM is erasable electrically at the byte-level
  - Flash memory is erasable electrically at the block level

## Interleaved Memory

- A collection of multiple DRAM chips grouped to form a memory bank
- $n$ banks can service $n$ requests simultaneously, increading memory read/write rates by a factor of $n$
- If consecutive words of memory are stored in different banks, the transfer of a block of memory is speeded up
- Distributing addresses among memory units/banks is called interleaving
  - Interleaving addresses among n memory units is known as n-way interleaving
- Most effective when the number of memory banks is equal to number of words in a cache line

## Virtual Memory

- Virtual memory is a hierarchical system accross caches, main memory and swap that is managed by the OS
- Locality of reference principle: addresses generated by the CPU should be in the first level of memory as often as possible
  - Use temporal, spatial, sequential locality to predict
  - The working set of memory addresses usually changes slowly so should maintain it closest to CPU
- Performance measured has hit ratio $H = \frac{N_1}{N_1 + N_2}$ (assuming a two-level memory hierarchy with data in $M_1$ and $M_2$)
- The average access time $t_A = H t_{A1} + (1-H)t_{A2}$
  - When there is a miss, the block is swapped in from $M_2$ to $M_1$ then accessed
  - $t_B$ is the time to transfer a block, so $t_{A2} \approxeq t_B$
  - $r = t_{A2}/t_{A1}$, the access time ratio of the two levels
  - $e = t_{A1}/t_A$, the factor by which average access time differs from minimum, **access efficiency**
- Memory capacity is limited by cost considerations, so wastins space is bad
  - The efficiency which space is being used can be defined as the ratio of useful stuff in memory over total memory, $u = S_u / S$
  - Wasted space can be empty due to fragmentation, or inactive data that is never used
  - System also takes up some memory space
- Virtual memory space is usually much greater than physical
  - If a memory address is referenced that is not in main memory, then there is a page fault and the OS fetches the data
  - When virtual address space is much greater than physical, most page table entries are empty
    - Fixed by inverted hashed page tables, where page numbers are hashed to smaller values that index a page table where each entry corresponds to physical frames
    - Hash collisions handled by extra chain field in the page table which indicates where colliding entry lives
    - Lookup process is:
      - Hash page number
      - Index the page table using hash. If the tag matches then page found
      - If not then check chain field and go to that index
        - If chain field is null then page fault
    - Average number of probes for an inverted page table with good hashing algorithm is 1.5
      - Practical to have a page frame table with twice the number of entries than frames of memory
- Segmentation allows programmer to view memory as multiple address spaces - segments
  - Each segment has its own access and usage rights
  - Provides a number of advantages:
    - Simplifies dynamic data structures, as segments can grow/shrink
    - Programs can be altered and recompiled independently without relinking and reloading
    - Can be shared among processes
    - Access privileges give protection
  - Programs divided into segments which are logical parts of variable length
  - Segments make up pages, so segment table used to get offset of address within page table
    - Two levels of lookup tables, address split into 3
- Translation Lookaside Buffer (TLB) holds most recently reference table entries as a cache
  - When TLB misses, there is a significant overhead in searching main memory page tables
  - Average address translation time $t_t = t_{tlb} + (1-H_{tlb})t_{tlb}$
  - TLB miss ratio usually low, less than 0.01
- Page size $S_p$ has an impact on memory space utilisation factor
  - Too small, then excessive internal fragmentation
  - Too large, then page tables become large and reduces space utilisation
  - $S_s$ is the segment size in words, so when $S_s > S_p$, the last page assigned to a segment will contain on average $S_p/2$ words
  - Size of the page table associated with each segment is approx $S_s/S_p$ words, assuming each table entry is 1 word
  - Memory overhead for each segment is $S= \frac{S_p}{2} + \frac{S_s}{S_p}$
  - Space utilisation is therefore $u = \frac{S_s}{S_s + S} = \frac{2 S_s S_p}{S_p^2 + 2S_s (1+S_p)}$
  - Optimum page size = $\sqrt{2S_s}$
  - Optimum utilisation = $\frac{1}{1+\sqrt{2/S_s}}$
  - Hit ratio increases with page size up to a maximum, then begins to decrease again
    - Value of $S_p$ yielding max hit ratios can be greater than the optimum page size for utilisation
- When a page fault occurs, the memory management software is called to swap in a page from secondary storage
  - If memory is full, it is necessary to swap out a page
  - Efficient page replacement algorithm required
    - Doing it randomly would be fucking stupid, might evict something being used
    - FIFO is simple and removes oldest page, but still might evict something being used
    - Clock replacement algorithm modifies fifo, which keeps track of unused pages through a `use` bit
      - Use bit is set if page hasn't been used since last page fault
    - LRU algorithm works well but complex to implement, requires an `age` counter per entry
      - Usually approximated through `use` bits set at intervals
    - Working set replacement algorithm keeps track of the set of pages referenced during a time interval
      - Replaces the page which has not been referenced during the preceding time interval
      - As time passes, a moving window captures a working set of pages
      - Implementation is complex
- Thrashing occurs when there is too many processes in too little memory and OS
  - Get a better page replacement algorithm
  - Close some chrome tabs
  - [Download more RAM](https://downloadmoreram.com/)

## Cache

- Cache contains copies of sections of main memory and relies of locality of reference
- Objective of cache is to have as high a hit ratio as possible
- Three techniques used for cache mapping
  - Direct, maps each block of memory to only one possible cache line
  - Associative, permits each main memory block to be loaded into any line of cache
    - Cache control logic must examine each cache line for a match
  - Set associative, each cache line can be in one of a set of cache lines
- In direct mapping, address is divided into three fields: tag, line and word
  - Cache is accessed with the same line and word as main memory
  - Tag is stored with data in the cache
    - If tag matches that of the address, then that's a cache hit
    - If a miss occurs, the new data and tag is fetched to cache
  - Simple and inexpensive
  - Fixed cache location for each block means that if two needed blocks map to the same line than cache will thrash
  - Victim cache was originally proposed as a solution
    - A fully associative cache of 4-16 lines sat between L1 and L2
- Fully associative cache scheme divide the CPU address into tag and word
  - Cache accessed by same word
  - Tag stored with data, have to examine every tag to determine if theres a cache miss
    - Complex because of this
- Set associative combines the two, where a given block maps to any line in a given set
  - eg, a 4-way cache has 4 lines per set and a block can map to any one of these 4
  - Performance increases diminish as set size increases
- Performance can be improved with separate instruction and data caches, L1 usually split
- Principle of inclusion states that L1 should always be subset of L2, L2 subset of L3, etc
  - When L3 is fetched to, data is written to L2 and L1 also
- Writing to cache can result in cache and main memory having inconsistent data
  - It is necessary to be coherent if
    - I/O operates on main memory
    - Processors share main memory
  - There are two common methods for maintaining consistency
    - With write through, every write operation to cache is repeated to main memory in parallel
      - Adds overhead to write to memory, but usually there are several reads between each write
      - Average access time $t_a = t_c + (1-h)t_b + w(t_m-t_c) = (1-w)t_c + (1-h+w)t_m$
        - Assumes $t_b = t_m$ is time to transfer block to cache, and $w$ is fraction of write references
      - Main memory write operation must complete before any further cache operations
        - If size of block matches datapath width, then whole block can be transferred in one operation, $t_b = t_m$
          - If not, then $b$ transfers are required and $t_b = b t_m$
      - Write through often enhanced by buffers for writes to main memory, freeing cache for subsequent accesses
      - In some systems, cache is not fetched to when a miss occurs on a write operation, meaning data is written to main memory but not cache
        - Reduces average access time as read misses incur less overhead
    - With write back, a write operation to main memory is performed only at block replacement time
      - Increases efficiency if variables are changed a number of times
      - Simple write back refers to always writing back a block when a swap is required, even if data is unaltered
      - Tagged write back only writes back a block if the contents have altered
        - 1-bit tag stored with each block, and is set when block altered
        - Tags examined at replacement time
      - Write buffers can also be implemented
- Most modern processors have at least two cache levels
  - Normal memory hierarchy principles apply, though on an L2 miss data is written to L1 and L2
  - With two levels, average access time becomes $t_a + t_{c1} + (1-h_1)t_{c2} + (1-h_2)t_m$
- A replacement policy is required for evicting cache lines in associative and set-associative mappings
  - Most effective policy is LRU, implemented totally in hardware
  - Two possible implementations, counter and reference matrix
    - A counter associated with each line is incremented at regular intervals and reset when the line is referenced
      - Reset every time line is accessed
      - On a miss when the cache is full, the line with a counter set at the maximum value is replaced and counter reset, all other counters set to 0
  - Reference matrix is based on a matrix of status bits
    - If $B$ lines to consider, then the upper triangular matrix of a $B\times B$ matrix is formed without the diagonal, with $(B \times (B-1))/2$
    - When the $i$th line is referenced, all bits in the $i$th row are set to one and $i$th column is zeroed
    - The least recently used one is one that has all 0s in its row and all 1s in its column
