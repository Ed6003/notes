<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js rust">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Year 1 Notes</title>
                <meta name="robots" content="noindex" />
                

        <!-- Custom HTML head -->
        

        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

                <link rel="icon" href="favicon.svg">
                        <link rel="shortcut icon" href="favicon.png">
                <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
                <link rel="stylesheet" href="css/print.css" media="print">
        
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
                <link rel="stylesheet" href="fonts/fonts.css">
        
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        
            </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "rust";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('rust')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="intro.html">Introduction</a></li><li class="chapter-item expanded "><a href="cs118/index.html"><strong aria-hidden="true">1.</strong> CS118</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="cs118/floats.html"><strong aria-hidden="true">1.1.</strong> IEEE 754</a></li><li class="chapter-item "><a href="cs118/oop.html"><strong aria-hidden="true">1.2.</strong> OOP Principles</a></li><li class="chapter-item "><a href="cs118/exceptions.html"><strong aria-hidden="true">1.3.</strong> Java Exceptions &amp; Generics</a></li></ol></li><li class="chapter-item expanded "><a href="cs126/index.html"><strong aria-hidden="true">2.</strong> CS126</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="cs126/arrays.html"><strong aria-hidden="true">2.1.</strong> Arrays &amp; Linked Lists</a></li><li class="chapter-item "><a href="cs126/analysis.html"><strong aria-hidden="true">2.2.</strong> Analysis of Algorithms</a></li><li class="chapter-item "><a href="cs126/recursion.html"><strong aria-hidden="true">2.3.</strong> Recursive Algorithms</a></li><li class="chapter-item "><a href="cs126/stacks.html"><strong aria-hidden="true">2.4.</strong> Stacks &amp; Queues</a></li><li class="chapter-item "><a href="cs126/lists.html"><strong aria-hidden="true">2.5.</strong> Lists</a></li><li class="chapter-item "><a href="cs126/maps.html"><strong aria-hidden="true">2.6.</strong> Maps</a></li><li class="chapter-item "><a href="cs126/hash.html"><strong aria-hidden="true">2.7.</strong> Hash Tables</a></li><li class="chapter-item "><a href="cs126/sets.html"><strong aria-hidden="true">2.8.</strong> Sets</a></li><li class="chapter-item "><a href="cs126/trees.html"><strong aria-hidden="true">2.9.</strong> Trees</a></li><li class="chapter-item "><a href="cs126/pqs.html"><strong aria-hidden="true">2.10.</strong> Priority Queues</a></li><li class="chapter-item "><a href="cs126/heaps.html"><strong aria-hidden="true">2.11.</strong> Heaps</a></li><li class="chapter-item "><a href="cs126/skip-lists.html"><strong aria-hidden="true">2.12.</strong> Skip Lists</a></li><li class="chapter-item "><a href="cs126/graphs.html"><strong aria-hidden="true">2.13.</strong> Graphs</a></li></ol></li><li class="chapter-item expanded "><a href="cs132/index.html"><strong aria-hidden="true">3.</strong> CS132</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="cs132/logic.html"><strong aria-hidden="true">3.1.</strong> Digital Logic</a></li><li class="chapter-item "><a href="cs132/assembly.html"><strong aria-hidden="true">3.2.</strong> Assembly</a></li><li class="chapter-item "><a href="cs132/memory.html"><strong aria-hidden="true">3.3.</strong> Memory Systems</a></li><li class="chapter-item "><a href="cs132/io.html"><strong aria-hidden="true">3.4.</strong> I/O</a></li><li class="chapter-item "><a href="cs132/architecture.html"><strong aria-hidden="true">3.5.</strong> Microprocessor Architecture</a></li></ol></li><li class="chapter-item expanded "><a href="cs141/index.html"><strong aria-hidden="true">4.</strong> CS141</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="cs141/types.html"><strong aria-hidden="true">4.1.</strong> Types &amp; Typeclasses</a></li><li class="chapter-item "><a href="cs141/recursion.html"><strong aria-hidden="true">4.2.</strong> Recursion</a></li><li class="chapter-item "><a href="cs141/functions.html"><strong aria-hidden="true">4.3.</strong> Higher Order Functions</a></li><li class="chapter-item "><a href="cs141/lazy.html"><strong aria-hidden="true">4.4.</strong> Lazy Evaluation</a></li><li class="chapter-item "><a href="cs141/reasoning.html"><strong aria-hidden="true">4.5.</strong> Reasoning About Programs</a></li><li class="chapter-item "><a href="cs141/functors.html"><strong aria-hidden="true">4.6.</strong> Functors &amp; Foldables</a></li><li class="chapter-item "><a href="cs141/applicatives.html"><strong aria-hidden="true">4.7.</strong> Applicative Functors</a></li><li class="chapter-item "><a href="cs141/monads.html"><strong aria-hidden="true">4.8.</strong> Monads</a></li><li class="chapter-item "><a href="cs141/tlp.html"><strong aria-hidden="true">4.9.</strong> Type-Level Programming</a></li></ol></li><li class="chapter-item expanded "><a href="es191/index.html"><strong aria-hidden="true">5.</strong> ES191</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="es191/symbols-and-conventions.html"><strong aria-hidden="true">5.1.</strong> Circuit Symbols &amp; Conventions</a></li><li class="chapter-item "><a href="es191/nodal.html"><strong aria-hidden="true">5.2.</strong> Nodal Analysis</a></li><li class="chapter-item "><a href="es191/mesh.html"><strong aria-hidden="true">5.3.</strong> Mesh Analysis</a></li><li class="chapter-item "><a href="es191/thevenin.html"><strong aria-hidden="true">5.4.</strong> Thevenin Circuits</a></li><li class="chapter-item "><a href="es191/rc.html"><strong aria-hidden="true">5.5.</strong> First Order RC Circuits</a></li><li class="chapter-item "><a href="es191/rl.html"><strong aria-hidden="true">5.6.</strong> First Order RL Circuits</a></li><li class="chapter-item "><a href="es191/ac.html"><strong aria-hidden="true">5.7.</strong> AC Circuits</a></li><li class="chapter-item "><a href="es191/diodes.html"><strong aria-hidden="true">5.8.</strong> Diodes</a></li><li class="chapter-item "><a href="es191/transistors.html"><strong aria-hidden="true">5.9.</strong> Transistors</a></li><li class="chapter-item "><a href="es191/opamps.html"><strong aria-hidden="true">5.10.</strong> Op Amps</a></li><li class="chapter-item "><a href="es191/filters.html"><strong aria-hidden="true">5.11.</strong> Passive Filters</a></li><li class="chapter-item "><a href="es191/equations.html"><strong aria-hidden="true">5.12.</strong> Equation Reference</a></li></ol></li><li class="chapter-item expanded "><a href="es193/index.html"><strong aria-hidden="true">6.</strong> ES193</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="es193/functions.html"><strong aria-hidden="true">6.1.</strong> Functions, Conics &amp; Asymptotes</a></li><li class="chapter-item "><a href="es193/complex.html"><strong aria-hidden="true">6.2.</strong> Complex Numbers</a></li><li class="chapter-item "><a href="es193/vectors.html"><strong aria-hidden="true">6.3.</strong> Vectors</a></li><li class="chapter-item "><a href="es193/matrices.html"><strong aria-hidden="true">6.4.</strong> Matrices</a></li><li class="chapter-item "><a href="es193/equations.html"><strong aria-hidden="true">6.5.</strong> Simultaneous Linear Equations</a></li><li class="chapter-item "><a href="es193/diff.html"><strong aria-hidden="true">6.6.</strong> Differentiation</a></li><li class="chapter-item "><a href="es193/int.html"><strong aria-hidden="true">6.7.</strong> Integration</a></li><li class="chapter-item "><a href="es193/diffeq.html"><strong aria-hidden="true">6.8.</strong> Differential Equations</a></li><li class="chapter-item "><a href="es193/laplace.html"><strong aria-hidden="true">6.9.</strong> Laplace Transforms</a></li><li class="chapter-item "><a href="es193/stats.html"><strong aria-hidden="true">6.10.</strong> Probability &amp; Statistics</a></li></ol></li><li class="chapter-item expanded "><a href="es197/index.html"><strong aria-hidden="true">7.</strong> ES197</a><a class="toggle"><div>❱</div></a></li><li><ol class="section"><li class="chapter-item "><a href="es197/mech1.html"><strong aria-hidden="true">7.1.</strong> Translational Mechanical Systems</a></li><li class="chapter-item "><a href="es197/mech2.html"><strong aria-hidden="true">7.2.</strong> Rotational Mechanical Systems</a></li><li class="chapter-item "><a href="es197/electrical.html"><strong aria-hidden="true">7.3.</strong> Electrical Systems</a></li><li class="chapter-item "><a href="es197/thermal.html"><strong aria-hidden="true">7.4.</strong> Thermal Systems</a></li><li class="chapter-item "><a href="es197/data.html"><strong aria-hidden="true">7.5.</strong> Data Driven Models</a></li><li class="chapter-item "><a href="es197/step1.html"><strong aria-hidden="true">7.6.</strong> First Order Step Response</a></li><li class="chapter-item "><a href="es197/step2.html"><strong aria-hidden="true">7.7.</strong> Second Order Step Response</a></li><li class="chapter-item "><a href="es197/transfer.html"><strong aria-hidden="true">7.8.</strong> Transfer Functions</a></li><li class="chapter-item "><a href="es197/freq1.html"><strong aria-hidden="true">7.9.</strong> First Order Frequency Response</a></li><li class="chapter-item "><a href="es197/freq2.html"><strong aria-hidden="true">7.10.</strong> Second Order Frequency Response</a></li></ol></li></ol>            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                                                <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                                            </div>

                    <h1 class="menu-title">Year 1 Notes</h1>

                    <div class="right-buttons">
                                                <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                                                                        <a href="https://github.com/Joeyh021/first-year-notes" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                                                
                    </div>
                </div>

                                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This book is a collection of all my notes from the first year of my degree (computer systems engineering).</p>
<p>It exists for a few purposes:</p>
<ul>
<li>To consolidate knowledge</li>
<li>To aid revision</li>
<li>To act as a reference during exams</li>
</ul>
<h2 id="contributing"><a class="header" href="#contributing">Contributing</a></h2>
<p>If you wish to contribute to this, either to make any additions or just to fix any mistakes I've made, feel free.</p>
<p>The sources are all available on my <a href="https://github.com/Joeyh021/First-Year-Notes">Github</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs118"><a class="header" href="#cs118">CS118</a></h1>
<p>This section is mainly just a reference of some of the more detailed bits of the module. It assumes a pretty strong prior knowledge of object oriented programming so doesn't aim to be comprehensive, it just specifies some details to remember for the exam.</p>
<p>The version of Java on DCS systems at the time of writing is 11. This is also the version these notes refer to.</p>
<h2 id="useful-resources"><a class="header" href="#useful-resources">Useful Resources</a></h2>
<ul>
<li>https://en.wikipedia.org/wiki/Single-precision_floating-point_format</li>
<li>The Oracle documentation for specifics on how Java implements stuff</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ieee-754"><a class="header" href="#ieee-754">IEEE 754</a></h1>
<p>IEEE 754 is a standardised way of storing floating point numbers with three components</p>
<ul>
<li>A sign bit</li>
<li>A biased exponent</li>
<li>A normalised mantissa</li>
</ul>
<table><thead><tr><th>Type</th><th>Sign</th><th>Exponent</th><th>Mantissa</th><th>Bias</th></tr></thead><tbody>
<tr><td>Single Precision (32 bit)</td><td>1 (bit 31)</td><td>8 (bit 30 - 23)</td><td>23 (bit 22- 0)</td><td>127</td></tr>
<tr><td>Double Precision (64 bit)</td><td>1 (bit 63)</td><td>11 (bit 62 - 52)</td><td>52 (51 - 0)</td><td>1023</td></tr>
</tbody></table>
<p>The examples below all refer to 32 bit numbers, but the principles apply to 64 bit.</p>
<ul>
<li>The exponent is an 8 bit unsigned number in biased form
<ul>
<li>To get the true exponent, subtract 127 from the binary value</li>
</ul>
</li>
<li>The mantissa is a binary fraction, with the first bit representing $1/2$, second bit $1/4$, etc.
<ul>
<li>The mantissa has an implicit $1.$, so 1 must always be added to the mantissa</li>
</ul>
</li>
</ul>
<h2 id="formula"><a class="header" href="#formula">Formula</a></h2>
<p>$$-1^{sign} \times 2^{E-127} \times (1 + \sum_{i=1}^{23} b_{23 - i} ;2^{-i}) $$</p>
<h2 id="decimal-to-float"><a class="header" href="#decimal-to-float">Decimal to Float</a></h2>
<p>The number is converted to a binary fractional format, then adjusted to fit into the form we need. Take 12.375 for example:</p>
<ul>
<li>Integer part $(12)_{10} = (1100)_2$</li>
<li>Fraction part $(0.375)_{10} = (0.011)_2$</li>
</ul>
<p>Combining the two parts yields $1100.011$. However, the standard requires that the mantissa have an implicit 1, so it must be shifted to the right until the number is normalised (ie has only 1 as an integer part). This yields $(1.100011)_2$. As this has been shifted, it is actually $(1.100011)_2 \times 2^3$. The three $(10)$ is therefore the exponent, but this has to be normalised (+127) to yield 130 $(1000 0010)$. The number is positive (sign bit zero) so this yields:</p>
<table><thead><tr><th>Sign</th><th>Biased Exponent</th><th>Normalised Mantissa</th></tr></thead><tbody>
<tr><td>0</td><td>1000 0010</td><td>100011</td></tr>
</tbody></table>
<p>$$01000001010001100000000000000000$$</p>
<h2 id="float-to-decimal"><a class="header" href="#float-to-decimal">Float to Decimal</a></h2>
<p>Starting with the value 0x41C80000 = 01000001110010000000000000000000:</p>
<table><thead><tr><th>Sign</th><th>Biased Exponent</th><th>Normalised Mantissa</th></tr></thead><tbody>
<tr><td>0</td><td>1000 0011</td><td>1001</td></tr>
</tbody></table>
<ul>
<li>The exponent is 131, biasing (-127) gives 4</li>
<li>The mantissa is 0.5625, adding 1 (normalising) gives 1.5625</li>
<li>$2^4 \times 1.5625$ gives <strong>25</strong></li>
</ul>
<h2 id="special-values"><a class="header" href="#special-values">Special Values</a></h2>
<ul>
<li>Zero
<ul>
<li>When both exponent and mantissa are zero, the number is zero</li>
<li>Can have both positive and negative zero</li>
</ul>
</li>
<li>Infinity
<ul>
<li>Exponent is all 1s, mantissa is zero</li>
<li>Can be either positive or negative</li>
</ul>
</li>
<li>Denormalised
<ul>
<li>If the exponent is all zeros but the mantissa is non-zero, then the value is a denormalised number</li>
<li>The mantissa does <em>not</em> have an assumed leading one</li>
</ul>
</li>
<li>NaN (Not a Number)
<ul>
<li>Exponent is all 1s, mantissa is non-zero</li>
<li>Represents error values</li>
</ul>
</li>
</ul>
<table><thead><tr><th>Exponent</th><th>Mantissa</th><th>Value</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>$\pm 0$</td></tr>
<tr><td>255</td><td>0</td><td>$\pm \infty$</td></tr>
<tr><td>0</td><td>not 0</td><td>denormalised</td></tr>
<tr><td>255</td><td>not 0</td><td>NaN</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="oop-principles"><a class="header" href="#oop-principles">OOP Principles</a></h1>
<h2 id="constructors"><a class="header" href="#constructors">Constructors</a></h2>
<p>All Java classes have a constructor, which is the method called upon object instantiation.</p>
<ul>
<li>An object can have multiple overloaded constructors</li>
<li>A constructor can have any access modifier</li>
<li>Constructors can call other constructors through the <code>this()</code> method.</li>
<li>If no constructor is specified, a default constructor is generated which takes no arguments and does nothing.</li>
<li>The first call in any constructor is to the superclass constructor.
<ul>
<li>This can be elided, and the default constructor is called
<ul>
<li>If there is no default constructor, a constructor must be called explicitly</li>
</ul>
</li>
<li>Can call explicitly with <code>super()</code></li>
</ul>
</li>
</ul>
<h2 id="access-modifiers"><a class="header" href="#access-modifiers">Access Modifiers</a></h2>
<p>Access modifiers apply to methods and member variables.</p>
<ul>
<li><code>private</code>: only the members of the class can see</li>
<li><code>public</code>: anyone can see</li>
<li><code>protected</code>: only class and subclasses can see</li>
<li>Default: package-private, only members of the same package can see</li>
</ul>
<h2 id="inheritance"><a class="header" href="#inheritance">Inheritance</a></h2>
<ul>
<li>To avoid the diamond/multiple inheritance problem, Java only allows for single inheritance</li>
<li>This is done using the <code>extends</code> keyword in the class definition</li>
<li>Inherits all public and protected methods and members</li>
<li>Can, however, implement multiple interfaces</li>
</ul>
<p>Example:</p>
<pre><code class="language-java">public class Car extends Vehicle implements Drivable, Crashable{
    // insert class body here
}
</code></pre>
<p>The <code>Car</code> class extends the <code>Vehicle</code> base class (can be abstract or concrete) and implements the behaviours defined by the interfaces <code>Drivable</code> and <code>Crashable</code>.</p>
<h2 id="static"><a class="header" href="#static"><code>static</code></a></h2>
<p>The static keyword defines a method, a field, or a block of code that belongs to the class instead of the object.</p>
<ul>
<li>Static fields share a mutable state accross all instances of the class</li>
<li>Static methods are called from the class instead of from the object</li>
<li>Static blocks are executed once, the first time the class is loaded into memory</li>
</ul>
<h2 id="polymorphism"><a class="header" href="#polymorphism">Polymorphism</a></h2>
<p>Polymorphism: of many forms. A broad term describing a few things in java.</p>
<h3 id="dynamic-polymorphism"><a class="header" href="#dynamic-polymorphism">Dynamic Polymorphism</a></h3>
<p>An object is defined as polymorphic if it passes more than one <code>instanceof</code> checks. An object can be referred to as the type of any one of it's superclasses. Say for example there is a <code>Tiger</code> class, which subclasses <code>Cat</code>, which subclasses <code>Animal</code>, giving an inheritance chain of <code>Animal</code> &lt;- <code>Cat</code> &lt;- <code>Tiger</code>, then the following is valid:</p>
<pre><code class="language-java">Animal a = new Tiger();
Cat c = new Tiger();
Tiger t = new Tiger();
</code></pre>
<p>When referencing an object through one of it's superclass types, you can only call objects that the reference type implements. For example, if there was two methods, <code>Cat::meow</code> and <code>Tiger::roar</code>, then:</p>
<pre><code class="language-java">c.meow() //valid
t.meow() //valid
a.meow() //not valid - animal has no method meow
t.roar() //valid
c.roar() // not valid - cat has no method roar
</code></pre>
<p>Even though all these variables are of the same runtime type, they are being called from a reference of another type.</p>
<p>When calling a method of an object, the actual method run is the one that is <em>furthest down the inheritance chain</em>. This is dynamic/runtime dispatch.</p>
<pre><code class="language-java">public class Animal{
    public speak(){return &quot;...&quot;;}
}

public class Dog extends Animal{
    public speak(){return &quot;woof&quot;;}
}

public class Cat extends Animal{
    public speak(){return &quot;meow&quot;;}
}

Animal a = new Animal();
Animal d = new Dog();
Animal c = new Cat();

a.speak() // &quot;...&quot;
d.speak() // &quot;woof&quot;
c.speak() // &quot;meow&quot;
</code></pre>
<p>Even though the reference was of type <code>Animal</code>, the actual method called was the overridden subclass method.</p>
<h3 id="static-polymorphism-method-overloading"><a class="header" href="#static-polymorphism-method-overloading">Static Polymorphism (Method Overloading)</a></h3>
<p>Note: different to over<em>ridding</em></p>
<ul>
<li>Multiple methods with the same name can be written, as long as they have different parameter lists</li>
<li>The method that is called depends upon the number of and type of the arguments passed</li>
</ul>
<p>Example:</p>
<pre><code class="language-java">public class Addition{
    private int add(int x, int y){return x+y;}
    private float add(float x, float y){return x+y;}
    public static void main(String[] args){
        add(1,2); //calls first method
        add(3.14,2.72); //calls second method
        add(15,1.5); //calls second method
    }
}
</code></pre>
<h2 id="abstraction"><a class="header" href="#abstraction">Abstraction</a></h2>
<p>Abstraction is the process of removing irrelevant details from the user, while exposing the relevant details. For example, you don't need to know how a function works, it's inner workings are <em>abstracted</em> away, leaving only the function's interface and details of what it does.</p>
<p>In the example below, the workings of the sine function are abstracted away, but we still know what it does and how to use it.</p>
<pre><code class="language-java">float sin(float x){
    //dont care really
}
sin(90); // 1.0
</code></pre>
<h2 id="encapsulation"><a class="header" href="#encapsulation">Encapsulation</a></h2>
<p>Encapsulation is wrapping the data and the code that acts on it into a single unit. The process is also known as <strong>data hiding</strong>, because the data is often hidden (declared <code>private</code>) behind the methods that retrieve them (getters/setters).</p>
<h2 id="reference-variables"><a class="header" href="#reference-variables">Reference Variables</a></h2>
<p>There is no such thing as an object variable in Java. Only primitives (<code>int</code>,<code>char</code>,<code>float</code>...), and references. All objects are heap-allocated (<code>new</code>), and a reference to them stored. Methods are all pass by value: either the value of the primitive, or the value of the reference. <em>Java is not pass by reference</em> . Objects are never copied/cloned/duplicated implicitly.</p>
<p>If a reference type is required (ie <code>Integer</code>), but a primitive is given (<code>(int) 1</code>), then the primitive will be <em>autoboxed</em> into it's equivalent object type.</p>
<h2 id="abstract-classes-and-interfaces"><a class="header" href="#abstract-classes-and-interfaces">Abstract Classes and Interfaces</a></h2>
<ul>
<li>Abstract classes are are classes that contain one or more <code>abstract</code> methods.
<ul>
<li>A class must be declared <code>abstract</code></li>
<li>Abstract methods have no body, ie are unimplemented.</li>
<li>The idea of them is to generalise behaviour, and leave it up to subclasses to implement</li>
<li>Abstract classes cannot be instantiated directly, though can still have constructors for subclasses to call</li>
</ul>
</li>
<li>Interfaces are a special kind of class that contain only abstract methods (and fields declared <code>public static final</code>)
<ul>
<li>Used to define behaviour</li>
<li>Technically can contain methods, but they're default implementations
<ul>
<li>This raises all sorts of issues so is best avoided</li>
</ul>
</li>
<li>Don't have to declare methods abstract, it's implicit</li>
</ul>
</li>
</ul>
<p>The diagram shows the inheritance hierarchy of the java collections framework, containing interfaces, abstract classes, and concrete classes.</p>
<p><img src="cs118/./img/abstract.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="exceptions"><a class="header" href="#exceptions">Exceptions</a></h1>
<h2 id="exceptions-1"><a class="header" href="#exceptions-1">Exceptions</a></h2>
<p>Exceptions are events that occur within the normal flow of program execution that disrupt the normal flow of control.</p>
<h3 id="throwing-exceptions"><a class="header" href="#throwing-exceptions">Throwing Exceptions</a></h3>
<p>Exceptions can occur when raised by other code we call, but an exception can also be raised manually using a <code>throw</code> statement. Any object that inherits, either directly or indirectly, from the <code>Throwable</code> class, can be raised as an exception.</p>
<pre><code class="language-java">//pop from a stack
public E pop(){
    if(this.size == 0)
        throw new EmptyStackException();
    //pop the item
}
</code></pre>
<h3 id="exception-handling"><a class="header" href="#exception-handling">Exception Handling</a></h3>
<ul>
<li>Exceptions can be caught using a <code>try</code>-<code>catch</code> block</li>
<li>If any code within the <code>try</code> block raises an exception, the <code>catch</code> block will be executed
<ul>
<li><code>catch</code> blocks must specify the type of exception to catch</li>
<li>Can have multiple catch blocks for different exceptions
<ul>
<li>Only 1 catch block will be executed</li>
</ul>
</li>
</ul>
</li>
<li>A <code>finally</code> block can be included to add any code to execute after the try-catch, regardless of if an exception is raised or not.</li>
<li>The exception object can be queried through the variable <code>e</code></li>
</ul>
<pre><code class="language-java">try{
    //try to do something
} catch (ExceptionA e){
    //if an exception of type ExceptionA is thrown, this is executed
} catch (ExceptionB e){
    //if an exception of type ExceptionB is thrown, this is executed
} finally{
    //this is always executed
}
</code></pre>
<h3 id="exception-heirachy"><a class="header" href="#exception-heirachy">Exception Heirachy</a></h3>
<ul>
<li>The <code>Throwable</code> class is the parent class of all errors and exceptions in Java</li>
<li>There are two subclasses of <code>Throwable</code>
<ul>
<li><code>Error</code>, which defines hard errors within the JVM that aren't really recoverable</li>
<li><code>Exception</code>, which defines errors that may occur within the code
<ul>
<li>There are two kinds of exception, checked and unchecked</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="cs118/./img/exceptions.png" alt="" /></p>
<h3 id="checked-and-unchecked-exceptions"><a class="header" href="#checked-and-unchecked-exceptions">Checked and Unchecked Exceptions</a></h3>
<ul>
<li>Checked exceptions must be either caught or re-thrown
<ul>
<li><code>IOException</code> is a good example</li>
</ul>
</li>
<li>When a method that may throw a checked exception is required, there are two options
<ul>
<li>Wrap the possibly exception-raising code in a <code>try</code>-<code>catch</code></li>
<li>Use the <code>throws</code> keyword in the method definition to indicate that the method may throw a checked exception</li>
</ul>
</li>
</ul>
<pre><code class="language-java">public static void ReadFile() throws FileNotFoundException{
    File f = new File(&quot;non-existant-file.txt&quot;)
    FileInputStream stream = new FileInputStream(f);
}
// OR
public static void ReadFile(){
    File f = new File(&quot;non-existant-file.txt&quot;)
    try{
        FileInputStream stream = new FileInputStream(f);
    } catch (FileNotFoundException){
        e.printStackTrace();
        return;
    }
}
</code></pre>
<ul>
<li>Unchecked Exceptions all subclass <code>RunTimeException</code>
<ul>
<li>ie <code>NullPointerException</code> and <code>ArrayIndexOutOfBoundsException</code></li>
</ul>
</li>
<li>Can be thrown at any point and will cause program to exit if not caught</li>
</ul>
<h3 id="custom-exceptions"><a class="header" href="#custom-exceptions">Custom Exceptions</a></h3>
<ul>
<li>Custom exception classes can be created</li>
<li>Should subclass <code>Throwable</code>
<ul>
<li>Ideally the most specific subclass possible</li>
<li>Subclassing <code>Exception</code> gives a new checked exception</li>
<li>Subclassing <code>RunTimeException</code> gives a new unchecked exception</li>
</ul>
</li>
<li>All methods such as <code>printStackTrace</code> and <code>getMessage</code> inherited from superclass</li>
<li>Should provide at least one constructor that overrides a superclass constructor</li>
</ul>
<pre><code class="language-java">public class IncorrectFileExtensionException
  extends RuntimeException {
    public IncorrectFileExtensionException(String errorMessage, Throwable err) {
        super(errorMessage, err);
    }
}
</code></pre>
<h2 id="generics"><a class="header" href="#generics">Generics</a></h2>
<p>Generics allow for classes to be parametrised over some type or types, to provide additional compile time static type checking. A simple box class parametrised over some type <code>E</code>, for example:</p>
<pre><code class="language-java">public class Box&lt;E&gt;{
    E item;

    public Box(E item){
        this.item = item;
    }
    public E get(){
        return item;
    }
    public E set(E item){
        this.item = item;
    }
}
</code></pre>
<h3 id="generic-methods"><a class="header" href="#generic-methods">Generic Methods</a></h3>
<p>Methods can be generic too, introducing their own type parameters. The parameters introduced in methods are local to that method, not the whole class. As an example, the static method below compares two <code>Pair&lt;K,V&gt;</code> classes to see if they are equal.</p>
<pre><code class="language-java">public static &lt;K, V&gt; boolean compare(Pair&lt;K, V&gt; p1, Pair&lt;K, V&gt; p2) {
        return p1.getKey().equals(p2.getKey()) &amp;&amp;
               p1.getValue().equals(p2.getValue());
    }
</code></pre>
<h3 id="type-erasure"><a class="header" href="#type-erasure">Type erasure</a></h3>
<p>Type information in generic classes and methods is erased at runtime, with the compiler replacing all instances of the type variable with <code>Object</code>. <code>Object</code> is also what appears in the compiled bytecode. This means that at runtime, any type casting of generic types is unchecked, and can cause runtime exceptions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs126"><a class="header" href="#cs126">CS126</a></h1>
<p>The book <em>Data Structures and Algorithms in Java</em> by Goodrich, Tamassia and Goldwasser is a good resource as it aligns closely with the material. It can be found online fairly easily.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="arrays--linked-lists"><a class="header" href="#arrays--linked-lists">Arrays &amp; Linked Lists</a></h1>
<h2 id="arrays"><a class="header" href="#arrays">Arrays</a></h2>
<p>Arrays are the most common data structure and are very versatile</p>
<ul>
<li>A sequenced collection of variables of the same type (homogenous)</li>
<li>Each cell in the array has an index $0... (n-1)$</li>
<li>Arrays are of fixed length and so have a max capacity</li>
<li>Can store primitives, or references to objects</li>
<li>When inserting an element into the array, all to the right must be shifted up by one</li>
<li>The same applies in reverse for removal to prevent null/0 gaps being left</li>
</ul>
<h2 id="sorting-arrays"><a class="header" href="#sorting-arrays">Sorting Arrays</a></h2>
<ul>
<li>The sorting problem:
<ul>
<li>Consider an array of unordered elements</li>
<li>We want to put them in a defined order</li>
<li>For example <code>[3, 6, 2, 7, 8, 10, 22, 9]</code> needs to become <code>[2, 3, 6, 7, 8, 9, 10, 22]</code></li>
</ul>
</li>
<li>One possible solution: insertion sort:
<ul>
<li>Go over the entire array, inserting each element at it's proper location by shifting elements along</li>
</ul>
</li>
</ul>
<pre><code class="language-java">public static void insertionSort(int[] data){
    int n = data.length;
    for(int k = 1; k &lt; n; k++){             //start with second element
        int cur = data[k];                  //insert data[k]
        int j = k;                          //get correct index j for cur
        while(j &lt; 0 &amp;&amp; data[j-1] &gt; cur){    //data[j-1] must go after cur
            data[j] = data[j-1];            // slide data[j-1] to the right
            j--;                            //consider previous j for cur
        }
        data[j] = cur; //cur is in the right place
    }
}
</code></pre>
<ul>
<li>Insertion sort sucks</li>
<li>Has worst case quadratic complexity, as k comparisons are required for k iterations.</li>
<li>When the list is in reverse order (worst case), $\frac{n(n-1)}{2}$ comparisons are made</li>
<li>Can do much better with alternative algorithms</li>
</ul>
<h2 id="singly-linked-lists"><a class="header" href="#singly-linked-lists">Singly Linked Lists</a></h2>
<ul>
<li>Linked lists is a concrete data structure consisting of a chain of nodes which point to each other</li>
<li>Each node stores the element, and the location of the next node</li>
<li>The data structure stores the head element and traverses the list by following the chain</li>
<li>Operations on the head of the list (ie, prepending) are efficient, as the head node can be accessed via its pointer</li>
<li>Operations on the tail require first traversing the entire list, so are slow</li>
<li>Useful when data needs to always be accessed sequentially</li>
<li>Generally, linked lists suck for literally every other reason</li>
</ul>
<h2 id="doubly-linked-lists"><a class="header" href="#doubly-linked-lists">Doubly Linked Lists</a></h2>
<ul>
<li>In a doubly linked list, each node stores a pointer to the node in front of and behind it</li>
<li>This allows the list to be traversed in both directions, and for nodes to be easily inserted mid-sequence</li>
<li>Sometimes, special header and trailer &quot;sentinel&quot; nodes are added to maintain a reference to the head an tail of the list
<ul>
<li>Also removes edge cases when inserting/deleting nodes as there is always nodes before/after head and tail</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="analysis-of-algorithms"><a class="header" href="#analysis-of-algorithms">Analysis of Algorithms</a></h1>
<p><em>This topic is key to literally every other one, and also seems to make up 90% of the exam questions (despite there being only 1 lecture on it) so it's very important.</em></p>
<ul>
<li>Need some way to characterise how good a data structure or algorithm is</li>
<li>Most algorithms take input and generate output</li>
<li>The run time of an algorithm typically grows with input size</li>
<li>Average case is often difficult to determine
<ul>
<li>Focus on the worst case</li>
</ul>
</li>
<li>Runtime analysis and benchmarks can be used to determine the performance of an algorithm, but this is often not possible
<ul>
<li>Results will also vary from machine to machine</li>
</ul>
</li>
<li>Theoretical analysis is preferred as it gives a more high-level analysis
<ul>
<li>Characterises runtime as a function of input size $n$</li>
</ul>
</li>
</ul>
<h2 id="pseudocode"><a class="header" href="#pseudocode">Pseudocode</a></h2>
<ul>
<li>Pseudocode is a high level description of an algorithm</li>
<li>Primitive perations are assumed to take unit time</li>
<li>For example
<ul>
<li>Evaluating an expression</li>
<li>Assigning to a variable</li>
<li>Indexing into an array</li>
<li>Calling a method</li>
</ul>
</li>
</ul>
<p>Looking at an algorithm, can count the number of operations in each step to analyse its runtime</p>
<pre><code class="language-java">public static double arrayMax(double[] data){
    int n = data.length; //2 ops
    double max = data[0]; //2 ops
    for (int j=1; j &lt; n;j++) //2n ops
        if(data[j] &gt; max) //2n-2 ops
            max = data[j]; //0 to 2n-2 ops
    return max; //1 op
}
</code></pre>
<ul>
<li>In the best case, there are $4n+3$ primitive operations</li>
<li>In the worst case, $6n+1$</li>
<li>The runtime $T(n)$ is therefore $a(4n+3) \leq T(n) \leq a(6n+1)$
<ul>
<li>$a$ is the time to execute a primitive operation</li>
</ul>
</li>
</ul>
<h2 id="functions"><a class="header" href="#functions">Functions</a></h2>
<p>There are 7 important functions $f(n) $that appear often when analysing algorithms</p>
<ul>
<li><strong>Constant</strong> - $1$
<ul>
<li>$f(n) = c$</li>
<li>A fixed constant</li>
<li>Could be any number but 1 is the most fundamental constant</li>
<li>Sometimes denoted $f(n) = c \times g(n)$ where $g(n) = 1$</li>
</ul>
</li>
<li><strong>Logarithmic</strong> - $\log n$
<ul>
<li>For some constant $b &gt; 1$, $f(n) = \log_b (n)$</li>
<li>Logarithm is the inverse of the power function
<ul>
<li>$x = \log_b n \Leftrightarrow b^x = n$</li>
</ul>
</li>
<li>Usually, $b=2$ because we are computer scientists and everything is base 2</li>
</ul>
</li>
<li><strong>Linear</strong> - $n$
<ul>
<li>$f(n) = cn$
<ul>
<li>$c$ is a fixed constant</li>
</ul>
</li>
</ul>
</li>
<li><strong>n-log-n</strong> - $n \log n$
<ul>
<li>$f(n) = n \times \log n$</li>
<li>Commonly appears with sorting algorithms</li>
</ul>
</li>
<li><strong>Quadratic</strong> - $n^2$
<ul>
<li>$f(n) = n^2$</li>
<li>Commonly appears where there are nested loops</li>
</ul>
</li>
<li><strong>Cubic</strong> - $n^3$
<ul>
<li>$f(n) = n^3$</li>
<li>Less common, also appears where there are 3 nested loops</li>
<li>Can be generalised to other polynomial functions</li>
</ul>
</li>
<li><strong>Exponential</strong> - $2^n$
<ul>
<li>$f(n) = b^n$
<ul>
<li>$b$ is some arbitrary base, $n$ is the exponent</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The growth rate of these functions is not affected by changing the hardware/software environment. Growth rate is also not affected by lower-order terms.</p>
<ul>
<li>Insertion sort takes time $\frac 1 2 n^2$
<ul>
<li>Characterised as taking $n^2$ time</li>
</ul>
</li>
<li>Merge sort takes $2n \log n$
<ul>
<li>Characterised as $n \log n$</li>
</ul>
</li>
<li>The <code>arrayMax</code> example from earlier took $a(4n+3) \leq T(n) \leq a(6n+1)$ time
<ul>
<li>Characterised as $n$</li>
</ul>
</li>
<li>A polynomial $f(n)$ of degree $d$, is of order $n^d$</li>
</ul>
<h2 id="big-o-notation"><a class="header" href="#big-o-notation">Big-O Notation</a></h2>
<ul>
<li>Big-O notation is used to formalise the growth rate of functions, and hence describe the runtime of algorithms.</li>
<li>Gives an upper bound on the growth rate of a function as $n \to \infty$</li>
<li>The statement &quot;$f(n)$ is $O(g(n))$&quot; means that the growth rate of $f(n)$ is no more than the growth rate of $g(n)$</li>
<li>If $f(n)$ is a polynomial of degree $d$, then $f(n)$ is $O(n^d)$
<ul>
<li>Drop lower order terms</li>
<li>Drop constant factors</li>
</ul>
</li>
<li>Always use the smallest possible class of functions
<ul>
<li>$2n$ is $O(n)$, not $O(n^2)$</li>
</ul>
</li>
<li>Always use the simplest expression
<ul>
<li>$3n+5$ is $O(n)$, not $O(3n)$</li>
</ul>
</li>
</ul>
<p>Formally, given functions $f(n)$ and $g(n)$, we say that $f(n)$ is $O(g(n))$ if there is a positive constant $c$ and a positive integer constant $n_0$, such that</p>
<p>$$f(n) \leq c g(n): \text{for}: n \geq n_0$$</p>
<p>where $c &gt; 0$, and $n_0 \geq 1$</p>
<h3 id="examples"><a class="header" href="#examples">Examples</a></h3>
<p>$2n + 10$ is $O(n)$:</p>
<p>$$f(n) = 2n+10,\quad g(n) = n$$
$$2n+10 \leq cn$$
$$(c-2)n \geq 10$$
$$n \geq \frac{10}{c-2}$$
$$c=3,\quad n_0 = 10$$</p>
<p>The function $n^2$ is not $O(n)$
$$f(n) = n^2,\quad g(n) = n$$
$$n^2 \leq cn$$
$$n \leq c$$
The inequality does not hold, since $c$ must be constant.</p>
<p>Big-O of $7n-2$:
$$f(n) = 7n-10,\quad g(n) = n$$
$$7n-2 \leq cn$$
$$(c-7)n \geq 2$$
$$n \geq \frac{2}{c-7}$$
$$c = 7, \quad c_0 = 1$$</p>
<p>Big-O of $3n^3 + 20n^2 + 5$:
$$f(n) = 3n^3 + 20n^2 + 5 ,\quad g(n) = n^3$$
$$3n^3 + 20n^2 + 5 \leq cn^3 ; \text{for}; n \geq n_0$$
$$c=4, \quad n_0 = 21$$</p>
<p>$3\log n + 5$ is $O(\log n)$
$$f(n) = 3 \log n +5 ,\quad g(n) = \log n$$
$$3 \log n + 5 \leq c\log n ; \text{for}; n \geq n_0$$
$$\log n \geq \frac{5}{c-3}$$
$$c=8, n_0 = 2$$</p>
<h2 id="asymptotic-analysis"><a class="header" href="#asymptotic-analysis">Asymptotic Analysis</a></h2>
<ul>
<li>The asymptotic analysis of an algorithm determines the running time big-O notation</li>
<li>To perform asymptotic analysis:
<ul>
<li>Find the worst-case number of primitive operations in the function</li>
<li>Express the function with big-O notation</li>
</ul>
</li>
<li>Since constant factors and lower-order terms are dropped, can disregard them when counting primitive operations</li>
</ul>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<p>The $i$th prefix average of an array $X$ is the average of the first $i+1$ elements of $X$. Two algorithms shown below are used to calculate the prefix average of an array.</p>
<h4 id="quadratic-time"><a class="header" href="#quadratic-time">Quadratic time</a></h4>
<pre><code class="language-java">//returns an array where a[i] is the average of x[0]...x[i]
public static double[] prefixAverage(double[] x){
    int n = x.length;
    double[] a = new double[n];
    for(int j = 0; j &lt; n; j++){
        double total = 0;
        for(int i = 0; i &lt;= j; i++)
            total += x[i];
        a[j] = total / (j+1);
    }
    return a;
}
</code></pre>
<p>The runtime of this function is $O(1 + 2 + ... + n) + O(n)$. The sum of the first $n$ integers is $\frac{n^2 + n}{2}$, so this algorithm runs in quadratic $O(n^2)$ time. This can easily be seen by the nested loops in the function too.</p>
<h4 id="linear-time"><a class="header" href="#linear-time">Linear time</a></h4>
<pre><code class="language-java">//returns an array where a[i] is the average of x[0]...x[i]
public static double[] prefixAverage(double[] x){
    int n = x.length;
    double[] a = new double[n];
    double total = 0;
    for(int i = 0; i &lt;= n; i++){
        total += x[i];
        a[i] = total / (i+1);
    }
    return a;
}
</code></pre>
<p>This algorithm uses a running average to compute the same array in linear time, by calculating a running sum.</p>
<h2 id="big-omega-and-big-theta"><a class="header" href="#big-omega-and-big-theta">Big-Omega and Big-Theta</a></h2>
<p>Big-Omega is used to describe the best case runtime for an algorithm. Formally, $f(n)$ is $\Omega(g(n))$ if there is a constant $c&gt;0$ and an integer constant $n_0 geq 1$ such that
$$f(n) \geq c \cdot g(n) ;\text{for}; n \geq n_0$$</p>
<p>Big-Theta describes the average case of the runtime. $f(n)$ is $\Theta(g(n))$ if there are constants $c' &gt;0$ and $c'' &gt; 0$, and an integer constant $n_0 \geq 1$ such that
$$c'g(n) \leq f(n) \leq c''g(n) ;\text{for}; n \geq n_0$$</p>
<p>The three notations compare as follows:</p>
<ul>
<li>Big-O
<ul>
<li>$f(n)$ is $O(g(n))$ if $f(n)$ is asymptotically <em>less than or equal to</em> $g(n)$</li>
</ul>
</li>
<li>Big-$\Omega$
<ul>
<li>$f(n)$ is $\Omega(g(n))$ if $f(n)$ is asymptotically <em>greater than or equal to</em> $g(n)$</li>
</ul>
</li>
<li>Big-$\Theta$
<ul>
<li>$f(n)$ is $O(g(n))$ if $f(n)$ is asymptotically <em>equal to</em> $g(n)$</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recursive-algorithms"><a class="header" href="#recursive-algorithms">Recursive Algorithms</a></h1>
<p>Recursion allows a problem to be broken down into sub-problems, defining a problem in terms of itself. Recursive methods work by calling themselves. As an example, take the factorial function:</p>
<p>$$
n! =
\begin{cases}
1 &amp; \text{if } n =0 \
n \times (n-1)!&amp;  \text{otherwise}
\end{cases}
$$</p>
<p>In java, this can be written:</p>
<pre><code class="language-java">public static int factorial(int n){
    if(n == 0) return 1;
    return n * factorial(n-1);
}
</code></pre>
<p>Recursive algorithms have:</p>
<ul>
<li>A base case
<ul>
<li>This is the case where the method <em>doesn't</em> call itself, and the stack begins to unwind</li>
<li>Every possible chain of recursive calls <em>must</em> reach a base case
<ul>
<li>If not the method will recurse infinitely and cause an error</li>
</ul>
</li>
</ul>
</li>
<li>A recursive case
<ul>
<li>Calls the current method again</li>
<li>Should always eventually end up on a base case</li>
</ul>
</li>
</ul>
<h2 id="binary-search"><a class="header" href="#binary-search">Binary Search</a></h2>
<p>Binary search is a recursively defined searching algorithm, which works by splitting an array in half at each step. Note that for binary search, the array must already be ordered.</p>
<p>Three cases:</p>
<ul>
<li>If the target equals <code>data[midpoint]</code> then the target has been found
<ul>
<li>This is the base case</li>
</ul>
</li>
<li>If the target is less than <code>data[midpoint]</code> then we binary search everything to the left of the midpoint</li>
<li>If the target is greater than <code>data[midpoint]</code> then we binary search everything to the right of the midpoint</li>
</ul>
<p><img src="cs126/./img/binsearch.png" alt="" /></p>
<pre><code class="language-java">public static boolean binarySearch(int[] data, int target, int left, int right){
    if (left &gt; right)
        return false;
    int mid = (left + right) / 2;
    if(target == data[mid])
        return true;
    else if (target &lt; data[mid])
        return binarySearch(data,target,low,mid-1);
    else
        return binarySearch(data,target,mid+1,high);

}
</code></pre>
<p>Binary search has $O(\log ,n)$, as the size of the data being processed halves at each recursive call. After the $i^{th}$ call, the size of the data is at most $n/2^i$.</p>
<h2 id="linear-recursion"><a class="header" href="#linear-recursion">Linear Recursion</a></h2>
<ul>
<li>The method only makes one recursive call</li>
<li>There may be multiple possible recursive calls, but only one should ever be made (ie binary search)</li>
<li>For example, a method used in computing powers by repeated squaring:</li>
</ul>
<p>$$
pow(x,n) =
\begin{cases}
1 &amp; \text{if } n =0 \
x \left(pow(x,\frac{n-1}{2})\right)^2 &amp;  n \text{ is odd} \
\left(pow(x,\frac{n}{2})\right)^2 &amp;  n \text{ is even}
\end{cases}
$$</p>
<pre><code class="language-java">public static int pow(int x, int n){
    if (n == 0) return 1;
    if (n % 2 == 0){
        y = pow(x,n/2);
        return x * y * y;
    }
    y = pow(x,(n-1)/2);
    return y * y;
}
</code></pre>
<p>Note how despite multiple cases, <code>pow</code> only ever calls itself once.</p>
<h2 id="binary-recursion"><a class="header" href="#binary-recursion">Binary Recursion</a></h2>
<p>Binary recursive methods call themselves <em>twice</em> recursively. Fibonacci numbers are defined using binary recursion:</p>
<ul>
<li>$F_0$ = 0</li>
<li>$F_1 = 1$</li>
<li>$F_i = F_{i-1} + F_{i-2}$</li>
</ul>
<pre><code class="language-java">public static int fib(int n){
    if (n == 0) return 0;
    if (n == 1) return 1;
    return fib(n-1) + fib(n-2);
}
</code></pre>
<p>This method calls itself twice, which isn't very efficient. It can end up having to compute the same result many many times. A better alternative is shown below, which uses linear recursion, and is therefore much much more efficient.</p>
<pre><code class="language-java">public static Pair&lt;Integer,Integer&gt; linearFib(int n){
    if(k = 1) return new Pair(n,0);
    Pair result = linearFib(n-1);
    return new Pair(result.snd+1, result.fst);
}
</code></pre>
<h2 id="multiple-recursion"><a class="header" href="#multiple-recursion">Multiple Recursion</a></h2>
<p>Multiple recursive algorithms call themselves recursively more than twice. These are generally very inefficient and should be avoided.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stacks--queues"><a class="header" href="#stacks--queues">Stacks &amp; Queues</a></h1>
<h2 id="abstract-data-types-adts"><a class="header" href="#abstract-data-types-adts">Abstract Data Types (ADTs)</a></h2>
<ul>
<li>An ADT is an abstraction of a data structure</li>
<li>Specifies the operations performed on the data</li>
<li>Focus is on what the operation does, not how it does it</li>
<li>Expressed in java with an interface</li>
</ul>
<h2 id="stacks"><a class="header" href="#stacks">Stacks</a></h2>
<ul>
<li>A stack is a last in, first out data structure (LIFO)</li>
<li>Items can be pushed to or popped from the top</li>
<li>Example uses include:
<ul>
<li>Undo sequence in a text editor</li>
<li>Chain of method calls in the JVM (method stack)</li>
<li>As auxillary storage in multiple algorithms</li>
</ul>
</li>
</ul>
<h3 id="the-stack-adt"><a class="header" href="#the-stack-adt">The Stack ADT</a></h3>
<p>The main operations are <code>push()</code> and <code>pop()</code>, but others are included for usefulness</p>
<pre><code class="language-java">public interface Stack&lt;E&gt;{
    int size();
    boolean isEmpty();
    E peek(); //returns the top element without popping it
    void push(E elem); //adds elem to the top of the stack
    E pop(); //removes the top stack item and returns it
}
</code></pre>
<h3 id="example-implementation"><a class="header" href="#example-implementation">Example Implementation</a></h3>
<p>The implementation below uses an array to implement the interface above. Only the important methods are included, the rest are omitted for brevity.</p>
<pre><code class="language-java">public class ArrayStack&lt;E&gt; implements Stack&lt;E&gt;{
    private E[] elems;
    private int top = -1;

    public ArrayStack(int capacity){
        elems = (E[]) new Object[capacity];
    }

    public E pop(){
        if (isEmpty()) return null;
        E t = elems[top];
        top = top-1;
        return t;
    }
    public E push(){
        if (top == elems.length-1) throw new FullStackException; //cant push to full stack
        top++;
        return elems[top];
    }
}
</code></pre>
<ul>
<li>Advantages
<ul>
<li>Performant, uses an array so directly indexes each element</li>
<li>$O(n)$ space and each operation runs in $O(1)$ time</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Limited by array max size</li>
<li>Trying to push to full stack throws an exception</li>
</ul>
</li>
</ul>
<h2 id="queues"><a class="header" href="#queues">Queues</a></h2>
<ul>
<li>Queues are a first in, first out (FIFO) data structure</li>
<li>Insertions are to the rear and removals are from the front
<ul>
<li>In contrast to stacks which are LIFO</li>
</ul>
</li>
<li>Example uses:
<ul>
<li>Waiting list</li>
<li>Control access to shared resources (printer queue)</li>
<li>Round Robin Scheduling
<ul>
<li>A CPU has limited resources for running processes simultaneously</li>
<li>Allows for sharing of resources</li>
<li>Programs wait in the queue to take turns to execute</li>
<li>When done, move to the back of the queue again</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="the-queue-adt"><a class="header" href="#the-queue-adt">The Queue ADT</a></h3>
<pre><code class="language-java">public interface Queue&lt;E&gt;{
    int size();
    boolean isEmpty();
    E peek();
    void enqueue(E elem); //add to rear of queue
    E dequeue(); // pop from front of queue
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lists"><a class="header" href="#lists">Lists</a></h1>
<p>The list ADT provides general support for adding and removing elements at arbitrary positions</p>
<h2 id="the-list-adt"><a class="header" href="#the-list-adt">The List ADT</a></h2>
<pre><code class="language-java">public interface List&lt;E&gt;{
    int size();
    boolean isEmpty();
    E get(int i); //get the item from the index i
    E set(int i, E e); //set the index i to the element e, returning what used to be at that index
    E add(int i, E e); //insert an element in the list at index i
    void remove(int i); //remove the element from index i
}
</code></pre>
<h2 id="array-based-implementation-arraylist"><a class="header" href="#array-based-implementation-arraylist">Array Based Implementation (<code>ArrayList</code>)</a></h2>
<p>Array lists are growable implementations of the List ADT that use arrays as the backing data structure. The idea is that as more elements are added, the array resizes itself to be bigger, as needed. Using an array makes implementing <code>get()</code> and <code>set()</code> easy, as they can both just be thin wrappers around <code>array[]</code> syntax.</p>
<ul>
<li>When inserting, room must be made for new elements by shifting other elements forward
<ul>
<li>Worst case (inserting to the head) $O(n)$ runtime</li>
</ul>
</li>
<li>When removing, need to shift elements backward to fill the hole
<ul>
<li>Same worst case as insertion, $O(n)$</li>
</ul>
</li>
</ul>
<p>When the array is full, we need to replace it with a larger one and copy over all the elements. When growing the array list, there are two possible strategies:</p>
<ul>
<li>Incremental
<ul>
<li>Increase the size by a constant $c$</li>
</ul>
</li>
<li>Doubling
<ul>
<li>Double the size each time</li>
</ul>
</li>
</ul>
<p>These two can be compared by analysing the <em>amortised</em> runtime of the push operation, ie the average time required $T(n) / n$ for a $n$ pushes taking a total time $T(n)$.</p>
<p>With incremental growth, over $n$ push operations, the array is replaced $k= n/c$ times, where $c$ is the constant amount the array size is increased by. The total time $T(n)$ of $n$ push operations is proportional to:
$$c + 2c + 3c + ... + kc = c (1 + 2 + .. + k) = \frac{ck(k+1)}{2}$$</p>
<p>Since $c$ is a constant, $T(n)$ is $\Omega(k^2)$, meaning the amortised time of a push operation is $\Omega(n)$.</p>
<p>With doubling growth, the array is replaced $k = \log_2 n$ times. The total time $T(n)$ of $n$ pushes is proportional to:</p>
<p>$$1 + 2 + 4 + 8 + ... + 2^k = 2^{k+1} -1 = 2n-1$$</p>
<p>Thus, $T(n)$ is $O(n)$, meaning the <em>amortised</em> time $T(n)/n$ is $O(1)$</p>
<h2 id="positional-lists"><a class="header" href="#positional-lists">Positional Lists</a></h2>
<ul>
<li>Positional lists are a general abstraction of a sequence of elements without indices</li>
<li>A position acts as a token or marker within the broader positional list</li>
<li>A position <code>p</code> is unaffected by changes elsewhere in a list
<ul>
<li>It only becomes invalid if explicitly deleted</li>
</ul>
</li>
<li>A position instance is an object (ie there is some <code>Position</code> class)
<ul>
<li>ie <code>p.getElement()</code> returns the element stored at position <code>p</code></li>
</ul>
</li>
<li>A very natural way to implement a positional list is with a doubly linked list, where each node represents a position.
<ul>
<li>Where a pointer to a node exists, access to the previous and next node is fast ($O(1)$)</li>
</ul>
</li>
</ul>
<h3 id="adt"><a class="header" href="#adt">ADT</a></h3>
<pre><code class="language-java">
public interface PositionalList&lt;E&gt;{
    int size();
    boolean isEmpty();
    Position&lt;E&gt; first(); //return postition of first element
    Position&lt;E&gt; last();  //return position of last element
    Position&lt;E&gt; before(Position&lt;E&gt; p); //return position of element before position p
    Position&lt;E&gt; after(Posittion&lt;E&gt; p); //return position of element after position p
    void addFirst(E e); //add a new element to the front of the list
    void addLast(E e); // add a new element to the back of the list
    void addBefore(Position&lt;E&gt; p, E e); // add a new element just before position p
    void addAfter(Position&lt;E&gt; p, E e); // add a new element just after position p
    void set(Position&lt;E&gt; p, E e); // replaces the element at position p with element e
    E remove(p); //removes and returns the element at position p, invalidating the position
}
</code></pre>
<h2 id="iterators"><a class="header" href="#iterators">Iterators</a></h2>
<p>Iterators are a software design pattern that abstract the process of scanning through a sequence one element at a time. A collection is <code>Iterable</code> if it has an <code>iterator()</code> method, which returns an instance of a class which implements the <code>Iterator</code> interface. Each call to <code>iterator()</code> returns a new object. The iterable interface is shown below.</p>
<pre><code class="language-java">public interface Iterator&lt;E&gt;{
    boolean hasNext(); //returns true if there is at least one additional element in the sequence
    E next(); //returns the next element in the sequence, advances the iterator by 1 position.
}
// example usage
public static void iteratorOver(Iterable&lt;E&gt; collection){
    Iterator&lt;E&gt; iter = collection.iterator();
    while(iter.hasNext()){
      E var = iter.next();
      System.out.println(var);
    }
}
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="maps"><a class="header" href="#maps">Maps</a></h1>
<ul>
<li>Maps are a searchable collection of key-value entries</li>
<li>Lookup the value using the key</li>
<li>Keys are unique</li>
</ul>
<h2 id="the-map-adt"><a class="header" href="#the-map-adt">The Map ADT</a></h2>
<pre><code class="language-java">public interface Map&lt;K,V&gt;{
    int size();
    boolean isEmpty();
    V get(K key); //return the value associated with key in the map, or null if it doesn't exist
    void put(K key, V value); //associate the value with the key in the map
    void remove(K key); //remove the key and it's value from the map
    Collection&lt;E&gt; entrySet(); //return an iterable collection of the values in the map
    Collection&lt;E&gt; keySet(); //return an iterable collection of the keys in the map
    Iterator&lt;E&gt; values(); //return an iterator over the map's values
}
</code></pre>
<h2 id="list-based-map"><a class="header" href="#list-based-map">List-Based Map</a></h2>
<p>A basic map can be implemented using an unsorted list.</p>
<ul>
<li><code>get(k)</code>
<ul>
<li>Does a simple linear search of the list looking for the key,value pair</li>
<li>Returns null if search reaches end of list and is unsuccessful</li>
</ul>
</li>
<li><code>put(k,v)</code>
<ul>
<li>Does linear search of the list to see if key already exists
<ul>
<li>If so, replace value</li>
</ul>
</li>
<li>If not, just add new entry to end</li>
</ul>
</li>
<li><code>remove(k)</code>
<ul>
<li>Does a linear search of the list to find the entry and removes it</li>
</ul>
</li>
<li>All operations take $O(n)$ time so this is not very efficient</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hash-tables"><a class="header" href="#hash-tables">Hash Tables</a></h1>
<ul>
<li>Recall the map ADT</li>
<li>Intuitively, a map <code>M</code> supports the abstraction of using keys as indices such as <code>M[k]</code></li>
<li>A map with n keys that are known to be integers in a fixed range is just an array</li>
<li>A hash function can map general keys (ie not integers) to corresponding indices in a table/array</li>
</ul>
<h2 id="hash-functions"><a class="header" href="#hash-functions">Hash Functions</a></h2>
<p>A hash function $h$ maps keys of a given type to integers in a fixed interval $[0,N-1]$.</p>
<ul>
<li>
<p>A very simple hash function is the mod function: $h(x) = x \mod N$</p>
<ul>
<li>Works for integer keys</li>
<li>The integer $h(x)$ is the hash value of the key $x$</li>
</ul>
</li>
<li>
<p>The goal of a hash function is to store an entry $(k,v)$ at index $i = h(k)$</p>
</li>
<li>
<p>Function usually has two components:</p>
<ul>
<li>Hash code $h_1$
<ul>
<li>keys -&gt; integers</li>
</ul>
</li>
<li>Compression function $h_2$
<ul>
<li>integers -&gt; integers in range $[0,N-1]$</li>
</ul>
</li>
<li>Hash code applied first, then compression - $h(x) = h_2(h_1(x))$
Some example hash functions:</li>
</ul>
</li>
<li>
<p>Memory address</p>
<ul>
<li>Use the memory address of the object as it's hash code</li>
</ul>
</li>
<li>
<p>Integer cast</p>
<ul>
<li>Interpret the bits of the key as an integer</li>
<li>Only suitable with $\leq$ 64 bits</li>
</ul>
</li>
<li>
<p>Component sum</p>
<ul>
<li>Partition they key into bitwise components of fixed length and sum the components</li>
</ul>
</li>
<li>
<p>Polynomial accumulation</p>
<ul>
<li>Partition the bits of the key into a sequence of components of fixed length $a_0$, $a_1$, ... , $a_{n-1}$</li>
<li>Evaluate the polynomial $P(z) = a_0 + a_1 z + a_2 z^2 + ... + a_{n-1}z^{n-1}$ for some fixed value $z$</li>
<li>Especially suitable for strings</li>
<li>Polynomial can be evaluated in $O(n)$ time as $p_i(z) = a_{n-i-1} + z,p_{i-1}(z)$</li>
</ul>
</li>
</ul>
<p>Some example compression functions:</p>
<ul>
<li>Division
<ul>
<li>$h_2(y) = y \mod N$</li>
<li>The size $N$ is usually chosen to be a prime to increase performance</li>
</ul>
</li>
<li>Multiply, Add, and Divide (MAD)
<ul>
<li>$h_2(y) = (ay + b) mod N$</li>
<li>$a$ and $b$ are nonnegative integers such that $a \mod N \neq 0$</li>
</ul>
</li>
</ul>
<h2 id="collision-handling"><a class="header" href="#collision-handling">Collision Handling</a></h2>
<p>Collisions occur when different keys hash to the same cell. There are several strategies for resolving collisions.</p>
<h3 id="separate-chaining"><a class="header" href="#separate-chaining">Separate Chaining</a></h3>
<p>With separate chaining, each cell in the map points to another map containing all the entries for that cell.</p>
<h3 id="linear-probing"><a class="header" href="#linear-probing">Linear Probing</a></h3>
<ul>
<li>Open addressing
<ul>
<li>The colliding item is placed in a different cell of the table</li>
</ul>
</li>
<li>Linear probing handles collisions by placing the colliding item at the next available table cell</li>
<li>Each table cell inspected is referred to as a &quot;probe&quot;</li>
<li>Colliding items can lump together, causing future collisions to cause a longer sequence of probes</li>
</ul>
<p>Consider a hash table $A$ that uses linear probing.</p>
<ul>
<li><code>get(k)</code>
<ul>
<li>Start at cell $h(k)$</li>
<li>Prove consecutive locations until either
<ul>
<li>Key is found</li>
<li>Empty cell is found</li>
<li>All cells have been unsuccessfully probed</li>
</ul>
</li>
</ul>
</li>
<li>To handle insertions and deletions, need to introduce a special marker object <code>defunct</code> which replaces deleted elements</li>
<li><code>remove(k)</code>
<ul>
<li>Search for an entry with key <code>k</code></li>
<li>If an entry <code>(k, v)</code> is found, replace it with <code>defunct</code> and return <code>v</code></li>
<li>Else, return <code>null</code></li>
</ul>
</li>
</ul>
<h3 id="double-hashing"><a class="header" href="#double-hashing">Double Hashing</a></h3>
<ul>
<li>Double hashing uses two hash functions <code>h()</code> and <code>f()</code></li>
<li>If cell <code>h(k)</code> already occupied, tries sequentially the cell $(h(k) + i\cdot f(k)) \mod N$ for $i=1,2,3...$</li>
<li><code>f(k)</code> cannot return zero</li>
<li>Table size $N$ must be a prime to allow probing of all cells</li>
<li>Common choice of second hash func is $f(k) = q - k \mod q$ where q is a prime</li>
<li>if $f(k) = 1$ then we have linear probing</li>
</ul>
<h2 id="performance"><a class="header" href="#performance">Performance</a></h2>
<ul>
<li>In the worst case, operations on hash tables take $O(n)$ time when the table is full and all keys collide into a single cell</li>
<li>The load factor $\alpha = n /N$ affects the performance of a hash table
<ul>
<li>$n$ = number of entries</li>
<li>$N$ = number of cells</li>
</ul>
</li>
<li>When $\alpha$ is large, collision is likely</li>
<li>Assuming hash values are true random numbers, the &quot;expected number&quot; of probes for an insertion with open addressing is $\frac{1}{1-\alpha}$</li>
<li>However, in practice, hashing is very fast and operations have $O(1)$ performance, provided $\alpha$ is not close to 1</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sets"><a class="header" href="#sets">Sets</a></h1>
<p>A set is an unordered collection of unique elements, typically with support for efficient membership tests</p>
<ul>
<li>Like keys of a map, but with no associated value</li>
</ul>
<h2 id="set-adt"><a class="header" href="#set-adt">Set ADT</a></h2>
<p>Sets also provide for traditional mathematical set operations: Union, Intersection, and Subtraction/Difference.</p>
<pre><code class="language-java">public interface Set&lt;E&gt;{
    void add(E e); //add element e to set if not already present
    void remove(E e); //remove element e from set if present
    boolean contains(E e); //test if element e is in set
    Iterator&lt;E&gt; iterator(); //returns an iterator over the elements
    //updates the set to include all elements of set T
    // union
    void addAll(Set&lt;E&gt; T);
    //updates the set to include only the elements of the set that are also in T
    //intersection
    void retainAll(Set&lt;E&gt; T);
    //updates the set to remove any elements that are also in T
    //difference
    void removeAll(Set&lt;E&gt; T);
}
</code></pre>
<h2 id="generic-merging"><a class="header" href="#generic-merging">Generic Merging</a></h2>
<p>Generic merge is a generalised merge of two sorted lists <code>A</code> and <code>B</code> to implement set operations. Uses a template method <code>merge</code> and 3 auxillary methods that describe what happens in each case:</p>
<ul>
<li><code>aIsLess</code>
<ul>
<li>Called when the element of <code>A</code> is less than the element of <code>B</code></li>
</ul>
</li>
<li><code>bIsLess</code>
<ul>
<li>Called when the element of <code>B</code> is less than the element of <code>A</code></li>
</ul>
</li>
<li><code>bothEqual</code>
<ul>
<li>Called when the element of <code>A</code> is equal to the element of <code>B</code></li>
</ul>
</li>
</ul>
<pre><code class="language-java">public static Set&lt;E&gt; merge(Set&lt;E&gt; A, Set&lt;E&gt; B){
    Set&lt;E&gt; S = new Set&lt;&gt;();
    while (!A.isEmpty() &amp;&amp; !B.isEmpty()){
        a = A.firstElement();
        b = B.firstElement();
        if(a &lt; b){
            aIsLess(a,S);
            A.remove(a);
        }
        else if (b &lt; a){
            bIsLess(b,S);
            B.remove(b);
        }
        else{ //b == a
            bothEqual(a,b,S);
            A.remove(a);
            B.remove(b);
        }
        while(!A.isEmpty()){
            aIsLess(a,S);
            A.remove(a);
        }
        while(!B.isEmpty()){
            bIsLess(b,S);
            B.remove(b);
        }
    }
    return S;
}
</code></pre>
<ul>
<li>Any set operation can be implemented using generic merge</li>
<li>Union
<ul>
<li><code>aIsLess</code> adds a into S</li>
<li><code>bIsLess</code> adds b into S</li>
<li><code>bothEqual</code> adds a (or b) into S</li>
</ul>
</li>
<li>Intersection
<ul>
<li><code>aIsLess</code> and <code>bIsLess</code> do nothing</li>
<li><code>bothEqual</code> adds a (or b) into S</li>
</ul>
</li>
<li>Difference
<ul>
<li><code>aIsLess</code> adds a into S</li>
<li><code>bIsLess</code> and <code>bothEqual</code> do nothing</li>
</ul>
</li>
<li>Runs in linear time, $O(N_A + N_B)$, provided the auxillary methods are $O(1)$</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="trees"><a class="header" href="#trees">Trees</a></h1>
<ul>
<li>A tree is an abstract model of a heirarchical structure</li>
<li>A tree consists of nodes with a parent-child relationship
<ul>
<li>A parent has one or more children</li>
<li>Each child has only one parent</li>
</ul>
</li>
<li>The root is the top node in the tree, the only node without a parent</li>
<li>An internal node has at least one child</li>
<li>An external node (or leaf) is a mode with no children</li>
<li>Nodes have ancestors (ie, the parent node of a parent)</li>
<li>The depth of a node is its number of ancestors</li>
<li>The height of a tree is its maximum depth</li>
</ul>
<h2 id="tree-adt"><a class="header" href="#tree-adt">Tree ADT</a></h2>
<p>Tree ADTs are defined using a similar concept to positional lists, as they don't have a natural ordering/indexing in the same way arrays do.</p>
<pre><code class="language-java">public interface Tree&lt;E&gt;{
    int size();
    boolean isEmpty();
    Node&lt;E&gt; root(); //returns root node
    Node&lt;E&gt; parent(Node&lt;E&gt; n); //returns parent of Node n
    Iterable&lt;Node&lt;E&gt;&gt; children(Node&lt;E&gt; n); //collection of all the children of Node n
    int numChildren(Node&lt;E&gt; n);
    Iterator&lt;E&gt; iterator(); //an iterator over the trees elements
    Iterator&lt;Node&lt;E&gt;&gt; nodes(); //collection of all the nodes
    boolean isInternal(Node&lt;E&gt; n); //does the node have at least one child
    boolean isExternal(Node&lt;E&gt; n); //does the node have no children
    boolean isRoot(Node&lt;E&gt; n); //is the node the root

}
</code></pre>
<h2 id="tree-traversal"><a class="header" href="#tree-traversal">Tree Traversal</a></h2>
<p>Trees can be traversed in 3 different orders. As trees are recursive data structures, all 3 traversals are defined recursively. The tree below is used as an example in all 3 cases.</p>
<p><img src="cs126/./img/tree-traverse.png" alt="" /></p>
<h3 id="pre-order"><a class="header" href="#pre-order">Pre-order</a></h3>
<ul>
<li>Visit the root</li>
<li>Pre order traverse the left subtree</li>
<li>Pre order traverse the right subtree</li>
</ul>
<p>Pre-order traversal of the tree gives: F B A D C E G I H</p>
<h3 id="in-order"><a class="header" href="#in-order">In-order</a></h3>
<ul>
<li>In order traverse the left subtree</li>
<li>Visit the root</li>
<li>In order traverse the right subtree</li>
</ul>
<p>In-order traversal of the tree gives: A B C D E F G H I</p>
<h3 id="post-order"><a class="header" href="#post-order">Post-order</a></h3>
<ul>
<li>Post order traverse the left subtree</li>
<li>Post order traverse the right subtree</li>
<li>Visit the root</li>
</ul>
<p>Post-order traversal of the tree gives: A C E D B H I G F</p>
<h2 id="binary-trees"><a class="header" href="#binary-trees">Binary Trees</a></h2>
<p>A binary tree is a special case of a tree:</p>
<ul>
<li>Each node has at most two children (either 0, 1 or 2)</li>
<li>The children of the node are an ordered pair (the left node is less than the right node)</li>
</ul>
<p>A binary tree will always fulfil the following properties:</p>
<ul>
<li>$e = i+1$</li>
<li>$n = 2e-1$</li>
<li>$h \leq i$</li>
<li>$h \leq (n-1)/2$</li>
<li>$e \leq 2^h$</li>
<li>$h \geq \log_2 e$</li>
<li>$h \geq \log_2 (n+1) -1$</li>
</ul>
<p>Where:</p>
<ul>
<li>$n$ is the number of nodes in the tree</li>
<li>$e$ is the number of external nodes</li>
<li>$i$ is the number of internal nodes</li>
<li>$h$ is the height/max depth of the tree</li>
</ul>
<h3 id="binary-tree-adt"><a class="header" href="#binary-tree-adt">Binary Tree ADT</a></h3>
<p>The binary tree ADT is an extension of the normal tree ADT with extra accessor methods.</p>
<pre><code class="language-java">public interface BinaryTree&lt;E&gt; extends Tree&lt;E&gt;{
    Node&lt;E&gt; left(Node&lt;E&gt; n); //returns the left child of n
    Node&lt;E&gt; right(Node&lt;E&gt; n); //returns the right child of n
    Node&lt;E&gt; sibling(Node&lt;E&gt; n); //returns the sibling of n
}
</code></pre>
<h3 id="arithmetic-expression-trees"><a class="header" href="#arithmetic-expression-trees">Arithmetic Expression Trees</a></h3>
<p>Binary trees can be used to represent arithmetic expressions, with internal nodes as operators and external nodes as operands. The tree below shows the expression $(2 \times (a - 1)) + (3 \times b)$. Traversing the tree in-order will can be used to print the expression infix, and post-order evaluating each node with it's children as the operand will return the value of the expression.</p>
<p><img src="cs126/./img/expr-tree.png" alt="" /></p>
<h3 id="implementations"><a class="header" href="#implementations">Implementations</a></h3>
<ul>
<li>Binary trees can be represented in a linked structure, similar to a linked list</li>
<li>Node objects are positions in a tree, the same as positions in a positional list</li>
<li>Each node is represented by an object that stores
<ul>
<li>The element</li>
<li>A pointer to the parent node</li>
<li>A pointer to the left child node</li>
<li>A pointer to the right child node</li>
</ul>
</li>
<li>Alternatively, the tree can be stored in an array <code>A</code></li>
<li><code>A[root]</code> is 0</li>
<li>If p is the left child of q, <code>A[p] = 2 * A[q] + 1</code></li>
<li>If p is the right child of q, <code>A[p] = 2 * A[q] + 2</code></li>
<li>In the worst, case the array will have size $2^n -1$</li>
</ul>
<h2 id="binary-search-trees"><a class="header" href="#binary-search-trees">Binary Search Trees</a></h2>
<ul>
<li>Binary trees can be used to implement a sorted map</li>
<li>Items are stored in order by their keys</li>
<li>For a node $p$ with key $K_p$, every key in the left subtree is less than $K_p$, and every node in the right subtree is greater than $K_p$</li>
<li>This allows for support of nearest-neighbour queries, so can fetch the key above or below another key</li>
<li>Binary search can perform nearest neighbour queries on an ordered map to find a key in $O(\log n)$ time</li>
<li>A search table is an ordered map implemented using a sorted sequence
<ul>
<li>Searches take $O(\log n) time$</li>
<li>Insertion and removal take $O(n)$ time</li>
<li>Only effective for maps of small size</li>
</ul>
</li>
</ul>
<h3 id="methods"><a class="header" href="#methods">Methods</a></h3>
<p>Binary trees are recursively defined, so all the methods operating on them are easily defined recursively also.</p>
<ul>
<li>Search</li>
<li>To search for a key $K$
<ul>
<li>Compare it with the key at $K_{root}$</li>
<li>If $K_{root} = K$, the value has been found</li>
<li>If $K_{root} &lt; K$, search the right subtree</li>
<li>If $K_{root} &gt; K$, search the left subtree</li>
</ul>
</li>
<li>Insertion
<ul>
<li>Search for the key being inserted $K$</li>
<li>Insert $K$ at the leaf reached by the search</li>
</ul>
</li>
<li>Deletion
<ul>
<li>Find the internal node that is follows the key being inserted in an in order traversal (the in order successor)</li>
<li>Copy key into the in order successor node</li>
<li>Remove the node copied out of</li>
</ul>
</li>
</ul>
<h3 id="performance-1"><a class="header" href="#performance-1">Performance</a></h3>
<ul>
<li>Consider a binary search tree with $n$ items and height $h$</li>
<li>The space used is $O(n)$</li>
<li>The methods get, put, remove take $O(h)$ time
<ul>
<li>The height h is $O(\log n)$ in the best case, when the tree is perfectly balanced</li>
<li>In the worst case, when the tree is basically just a linked list, this decays to $O(n)$</li>
</ul>
</li>
</ul>
<h2 id="avl-trees"><a class="header" href="#avl-trees">AVL Trees</a></h2>
<ul>
<li>AVL trees are balanced binary trees
<ul>
<li>For every internal node $v$ of the tree, the heights of the subtrees of $v$ can differ by at most 1</li>
</ul>
</li>
<li>The height of an AVL tree storing $n$ keys is $O(\log n)$</li>
<li>Balance is maintained by <em>rotating</em> nodes every time a new one is inserted/removed</li>
</ul>
<h3 id="performance-2"><a class="header" href="#performance-2">Performance</a></h3>
<ul>
<li>The runtime of a single rotation is $O(1)$</li>
<li>The tree is assured to always have $h= \log n$, so the runtime of all methods is $O(\log n)$</li>
<li>This makes AVL trees an efficient implementation of binary trees, as their performance does not decay as the tree becomes unbalanced</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="priority-queues"><a class="header" href="#priority-queues">Priority Queues</a></h1>
<p>A priority queue is an implementation of a queue where each item stored has a <strong>priority</strong>. The items with the highest priority are moved to the front of the queue to leave first. A priority queue takes a key along with a value, where the key is used as the priority of the item.</p>
<h2 id="priority-queue-adt"><a class="header" href="#priority-queue-adt">Priority Queue ADT</a></h2>
<pre><code class="language-java">public interface PriorityQueue&lt;K,V&gt;{
    int size();
    boolean isEmpty();
    void insert(K key, V value); //inserts a value into the queue with key as its priority
    V removeMin(); //removes the entry with the lowest key (at the front of the queue)
    V min(); //returns but not removes the smallest key entry (peek)
}
</code></pre>
<h3 id="entry-objects"><a class="header" href="#entry-objects">Entry Objects</a></h3>
<ul>
<li>To store a key-value pair, a tuple/pair-like object is needed</li>
<li>An <code>Entry&lt;K,V&gt;</code> object is used to store each queue item
<ul>
<li><code>Key</code> is what is used to defined the priority of the item in the queue</li>
<li><code>Value</code> is the queue item</li>
</ul>
</li>
<li>This pattern is similar to what is used in maps</li>
</ul>
<pre><code class="language-java">public class Entry&lt;K,V&gt;{
    private K key;
    private V value;

    public Entry(K key, V value){
        this.key = key;
        this.value = value;
    }

    public K getKey(){
        return key;
    }

    public V getValue(){
        return value;
    }

}
</code></pre>
<h2 id="total-order-relations"><a class="header" href="#total-order-relations">Total Order Relations</a></h2>
<ul>
<li>Keys may be arbitrary values, so they must have some order defined on them
<ul>
<li>Two entries may also have the same key</li>
</ul>
</li>
<li>A total order relation is a mathematical concept which formalises ordering on a set of objects where any 2 are comparable.</li>
<li>A total ordering satisfies the following properties $\forall a,b,c \in X$
<ul>
<li>$a \leq b$ or $b \leq a$
<ul>
<li>Comparability property</li>
</ul>
</li>
<li>If $a \leq b$ $b \leq c$, then $a \leq c$
<ul>
<li>Transitive property</li>
</ul>
</li>
<li>If $a \leq b$ and $b \leq a$, then $a = b$
<ul>
<li>Antisymmetric property</li>
</ul>
</li>
<li>$a \leq a$
<ul>
<li>Reflexive property</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="comparators"><a class="header" href="#comparators">Comparators</a></h2>
<ul>
<li>A comparator encapsulates the action of comparing two objects with a total order declared on them</li>
<li>A priority queue uses a comparator object given to it to compare two keys to decide their priority</li>
</ul>
<pre><code class="language-java">public class Comparator&lt;E&gt;{
    public int compare(E a, E b){
        if(a &lt; b)
            return -1;
        if(a &gt; b)
            return 1;
        return 0;
    }
}
</code></pre>
<h2 id="implementations-1"><a class="header" href="#implementations-1">Implementations</a></h2>
<h3 id="unsorted-list-based-implementation"><a class="header" href="#unsorted-list-based-implementation">Unsorted List-Based Implementation</a></h3>
<p>A simple implementation of a priority queue can use an unsorted list</p>
<ul>
<li><code>insert()</code> just appends the <code>Entry(key,value)</code> to the list
<ul>
<li>$O(1)$ time</li>
</ul>
</li>
<li><code>removeMin()</code> and <code>min()</code> linear search the list to find the smallest key (one with highest priority) to return
<ul>
<li>Linear search takes $O(n)$ time</li>
</ul>
</li>
</ul>
<h3 id="sorted-list-based-implementation"><a class="header" href="#sorted-list-based-implementation">Sorted List-Based Implementation</a></h3>
<p>To improve the speed of removing items, a sorted list can instead be used. These two implementations have a tradeoff between which operations are faster, so the best one for the application is usually chosen.</p>
<ul>
<li><code>insert()</code> finds the correct place to insert the <code>Entry(key,value)</code> in the list to maintain the ordering
<ul>
<li>Has to find place to insert, takes $O(n)$ time</li>
</ul>
</li>
<li>As the list is maintained in order, the entry with the lowest key is always at the front, meaning <code>removeMin()</code> and <code>min()</code> just pop from the front
<ul>
<li>Takes $O(1)$ time</li>
</ul>
</li>
</ul>
<h2 id="sorting-using-a-priority-queue"><a class="header" href="#sorting-using-a-priority-queue">Sorting Using a Priority Queue</a></h2>
<p>The idea of using a priority queue for sorting is that all the elements are inserted into the queue, then removed one at a time such that they are in order</p>
<ul>
<li>Selection sort uses an unsorted queue
<ul>
<li>Inserting $n$ items in each $O(1)$ time takes $O(n)$ time</li>
<li>Removing the elements in order
<ul>
<li>$O(n) + O(n-1) + O(n-2) + ... + O(1)$</li>
</ul>
</li>
<li>Overall $O(n^2)$ time</li>
</ul>
</li>
<li>Insertion sort uses a sorted queue
<ul>
<li>Runtimes are the opposite to unsorted</li>
<li>Adding $n$ elements takes $O(1) + O(2) + O(3) + ... + O(n)$ time</li>
<li>Removing $n$ elements in each $O(1)$ time takes $O(n)$ time</li>
<li>Overall runtime of $O(n^2)$ again</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="heaps"><a class="header" href="#heaps">Heaps</a></h1>
<ul>
<li>A heap is a tree-based data structure where the tree is a <em>complete binary tree</em></li>
<li>Two kinds of heaps, min-heaps and max-heaps</li>
<li>For a min-heap, the <em>heap order</em> specifies that for every internal node $v$ other than the root, $v \geq parent(v)$
<ul>
<li>In other words, the root of the tree/subtree must be the smallest node</li>
<li>This property is inverted for max heaps</li>
</ul>
</li>
<li>Complete binary tree means that every level of the tree, except possibly the last, is filled, and all nodes are as far left as possible.
<ul>
<li>More formally, for a heap of height $h$, for $i = 0,1,...,h-1$ there are $2^i$ nodes of depth $i$</li>
<li>At depth $h-1$, the internal nodes are to the left of the external nodes</li>
<li>The last node of a heap is the rightmost node of maximum depth</li>
</ul>
</li>
<li>Unlike binary search trees, heaps can contain duplicates</li>
<li>Heaps are also unordered data structures</li>
<li>Heaps can be used to implement priority queues
<ul>
<li>An <code>Entry(Key,Value)</code> is stored at each node</li>
</ul>
</li>
</ul>
<p><img src="cs126/./img/heaps.png" alt="" /></p>
<h2 id="insertion"><a class="header" href="#insertion">Insertion</a></h2>
<ul>
<li>To insert a node <code>z</code> into a heap, you insert the node after the last node, making <code>z</code> the new last node
<ul>
<li>The last node of a heap is the rightmost node of max depth</li>
</ul>
</li>
<li>The heap property is then restored using the upheap algorithm</li>
<li>The just inserted node is filtered up the heap to restore the ordering</li>
<li>Moving up the branches starting from the <code>z</code>
<ul>
<li>While <code>parent(z) &gt; (z)</code>
<ul>
<li>Swap <code>z</code> and <code>parent(z)</code></li>
</ul>
</li>
</ul>
</li>
<li>Since a heap has height $\log, n$, this runs in $O(\log, n)$ time</li>
</ul>
<h2 id="removal"><a class="header" href="#removal">Removal</a></h2>
<ul>
<li>To remove a node <code>z</code> from the heap, replace the root node with the last node <code>w</code></li>
<li>Remove the last node <code>w</code></li>
<li>Restore the heap order using downheap</li>
<li>Filter the replacement node back down the tree
<ul>
<li>While <code>w</code> is greater than either of its children
<ul>
<li>Swap <code>w</code> with the smallest of its children</li>
</ul>
</li>
</ul>
</li>
<li>Also runs in $O(\log, n)$ time</li>
</ul>
<h2 id="heap-sort"><a class="header" href="#heap-sort">Heap Sort</a></h2>
<p>For a sequence <code>S</code> of <code>n</code> elements with a total order relation on them, they can be ordered using a heap.</p>
<ul>
<li>Insert all the elements into the heap</li>
<li>Remove them all from the heap again, they should come out in order</li>
<li>$n$ calls of insert take $O(n \log, n)$ time</li>
<li>$n$ calls to remove take $O(n \log, n)$ time</li>
<li>Overall runtime is $O(n \log, n)$</li>
<li>Much faster than quadratic sorting algorithms such as insertion and selection sort</li>
</ul>
<h2 id="array-based-implementation"><a class="header" href="#array-based-implementation">Array-based Implementation</a></h2>
<p>For a heap with <code>n</code> elements, the element at position <code>p</code> is stored at cell <code>f(p)</code> such that</p>
<ul>
<li>If <code>p</code> is the root, <code>f(p) = 0</code></li>
<li>If <code>p</code> is the left child <code>q</code>, <code>f(p) = 2*f(q)+1</code></li>
<li>If <code>p</code> is the right child <code>q</code>, <code>f(p) = 2*f(q)+2</code></li>
</ul>
<p>Insert corresponds to inserting at the first free cell, and remove corresponds to removing from cell 0</p>
<ul>
<li>A heap with <code>n</code> keys has length $O(n)$</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="skip-lists"><a class="header" href="#skip-lists">Skip Lists</a></h1>
<ul>
<li>When implementing sets, the idea is to be able to test for membership and update elements efficiently</li>
<li>A sorted array or list is easy to search, but difficult to maintain in order</li>
<li>Skip lists consists of multiple lists/sets
<ul>
<li>The skip list $S = \left{S_0, S_1, S_2,...,S_h \right}$</li>
<li>$S_0$ contains all the elements, plus $\pm \infty$</li>
<li>$S_i$ is a random subset of $S_{i-1}$, for $i = 1,2,...,h-1$
<ul>
<li>Each element of $S_{i-1}$ appears in $S_i$ with probability 0.5</li>
</ul>
</li>
<li>$S_h$ contains only $\pm \infty$</li>
</ul>
</li>
</ul>
<p><img src="cs126/./img/skip-list.png" alt="" /></p>
<h2 id="search"><a class="header" href="#search">Search</a></h2>
<p>To search for an element $x$ in the list:</p>
<ul>
<li>Start in the first position of the top list</li>
<li>At the current position $p$, compare $x$ with the next element in the current list $y$
<ul>
<li>If $x=y$, return $y$</li>
<li>If $x&gt;y$, move to the next element in the list
<ul>
<li>&quot;Scan forward&quot;</li>
</ul>
</li>
<li>If $x&lt;y$, drop down to the element below
<ul>
<li>&quot;Drop down&quot;</li>
</ul>
</li>
</ul>
</li>
<li>If the end of the list ($+\infty$) is reached, the element does not exist</li>
</ul>
<p><img src="cs126/./img/skip-search.png" alt="" /></p>
<h2 id="insertion-1"><a class="header" href="#insertion-1">Insertion</a></h2>
<p>To insert an element $k$ into the list:</p>
<ul>
<li>Repeatedly toss a fair coin until tails comes up
<ul>
<li>$i$ is the number of times the coin came up heads</li>
</ul>
</li>
<li>If $i \geq h$, add to the skip list new lists $S_{h+1},...,S_{i+1}$
<ul>
<li>Each containing only the two end keys $\pm \infty$</li>
</ul>
</li>
<li>Search for $k$ and find the positions $p_0, p_1, ... ,P_i$ of the items with the largest element $&gt; k$ in each list $S_0, S_1, ..., S_i$
<ul>
<li>Same as the search algorithm</li>
</ul>
</li>
<li>For $j = 0..i$, insert k into list $S_j$ after position $p_j$</li>
</ul>
<p><img src="cs126/./img/skip-insert.png" alt="" /></p>
<h2 id="deletion"><a class="header" href="#deletion">Deletion</a></h2>
<p>To remove an entry $x$ from a skip list:</p>
<ul>
<li>Search for $x$ in the skip list and find the positions of the items $p_0, p_1, ..., p_i$ containing $x$</li>
<li>Remove those positions from the lists $S_0,S_1,...,S_i$</li>
<li>Remove a list if neccessary</li>
</ul>
<p><img src="cs126/./img/skip-remove.png" alt="" /></p>
<h2 id="implementation"><a class="header" href="#implementation">Implementation</a></h2>
<p>A skip list can be implemented using quad-nodes, where each node stores</p>
<ul>
<li>It's item/element </li>
<li>A pointer to the node above</li>
<li>A pointer to the node below</li>
<li>A pointer to the next node</li>
<li>A pointer to the previous node</li>
</ul>
<h2 id="performance-3"><a class="header" href="#performance-3">Performance</a></h2>
<ul>
<li>The space used by a skip list depends on the random number on each invocation of the insertion algorithm
<ul>
<li>On average, the expected space usage of a skip list with $n$ items is $O(n)$</li>
</ul>
</li>
<li>The run time of the insertion is affected by the height $h$ of the skip list
<ul>
<li>A skip list with $n$ items has average height $O(\log n)$</li>
</ul>
</li>
<li>The search time in a skip list is proportional to the number of steps taken</li>
<li>The drop-down steps are bounded by the height of the list</li>
<li>The scan-forward steps are bounded by the length of the list
<ul>
<li>Both are $O(\log n)$</li>
</ul>
</li>
<li>Insertion and deletion are also both $O(\log n)$</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="graphs"><a class="header" href="#graphs">Graphs</a></h1>
<p>A graph is a collection of edges and vertices, a pair $(V,,E)$, where</p>
<ul>
<li>$V$ is a set of nodes, called <em>vertices</em></li>
<li>$E$ is a collection of pairs of vertices, called <em>edges</em></li>
<li>Vertices and edges are positions and store elements</li>
</ul>
<p>Examples of graphs include routes between locations, users of a social network and their friendships, and the internet.</p>
<p>There are a number of different types of edge in a graph, depending upon what the edge represents:</p>
<ul>
<li><em>Directed</em> edge
<ul>
<li>Ordered pair of vertices $(u,v)$</li>
<li>First vertex $u$ is the origin</li>
<li>Second vertex $v$ is the destination</li>
<li>For example, a journey between two points</li>
</ul>
</li>
<li><em>Undirected</em> edge
<ul>
<li>Unordered pair of vertices $(u,v)$</li>
</ul>
</li>
<li>In a directed graph, all edges are directed</li>
<li>In an undirected graph, all edged are undirected</li>
</ul>
<h2 id="graph-terminology"><a class="header" href="#graph-terminology">Graph Terminology</a></h2>
<p><img src="cs126/./img/graph.png" alt="" /></p>
<ul>
<li>Adjacent vertices
<ul>
<li>Two vertices $U$ and $V$ are adjacent (ie connected by an edge)</li>
</ul>
</li>
<li>Edges incident on a vertex
<ul>
<li>The edges connect to a vertex</li>
<li>$a$, $d$, and $b$ are incident on $V$</li>
</ul>
</li>
<li>End vertices or endpoints of an edge
<ul>
<li>The vertices connected to an edge</li>
<li>$U$ and $V$ are endpoints of $a$</li>
</ul>
</li>
<li>The degree of a vertex
<ul>
<li>The number of edges connected to it</li>
<li>$X$ has degree 5</li>
</ul>
</li>
<li>Parallel edges
<ul>
<li>Edges that make the same connection</li>
<li>$h$ and $i$ are parallel</li>
</ul>
</li>
<li>Self-loop
<ul>
<li>An edge that has the same vertex at both ends</li>
<li>$j$ is a self-loop</li>
</ul>
</li>
<li>Path
<ul>
<li>A sequence of alternating vertices and edges</li>
<li>Begins and ends with a vertex</li>
<li>Each edge is preceded and followed by its endpoints</li>
<li>$P_1 = (V,b,X,h,Z)$ is a simple path</li>
</ul>
</li>
<li>Cycle
<ul>
<li>A circular sequence of alternating vertices and edges
<ul>
<li>A circular path</li>
</ul>
</li>
<li>A simple cycle is one where all edges and vertices are distinct</li>
<li>A non-simple cycle contains an edge or vertex more than once</li>
<li>A graph without cycles (acyclic) is a tree</li>
</ul>
</li>
<li>Length
<ul>
<li>The number of edges in a path</li>
<li>The number of edges in a cycle</li>
</ul>
</li>
</ul>
<h3 id="graph-properties"><a class="header" href="#graph-properties">Graph Properties</a></h3>
<p>Notation:</p>
<ul>
<li>$n$ is the number of vertices</li>
<li>$m$ is the number of edges</li>
<li>$\deg(v)$ is the degree of vertex $v$</li>
</ul>
<p>The sum of the degrees of the vertices of a graph is always an even number. Each edge is counted twice, as it connects to two vertices, so $\sum_v \deg(v) = 2m$. For example, the graph shown has $n = 4$ and $m = 6$. $\deg(v) = 3 \Rightarrow \sum_v \deg(v) = 2m = 12$</p>
<p><img src="cs126/./img/graph-2.png" alt="" /></p>
<p>In an undirected graph with no self loops and no multiple edges, $m \leq n \frac{n-1}{2}$. Each vertex has degree at most $(n-1)$ and $\sum_v \deg(v) = 2m$. For the graph shown, $m = 6 \leq n \frac{n-1}{2} = 6 $</p>
<h2 id="the-graph-adt"><a class="header" href="#the-graph-adt">The Graph ADT</a></h2>
<p>A graph is a collection of vertices and edges, which are modelled as a combination of 3 data types: <code>Vertex</code>, <code>Edge</code> and <code>Graph</code>.</p>
<ul>
<li>A <code>Vertex</code> is just a box object storing an element provided by the user</li>
<li>An <code>Edge</code> also stores an associated value which can be retrieved</li>
</ul>
<pre><code class="language-java">public interface Graph{
    int numVertices();

    Collection vertices(); //returns all the graph's vertices

    int numEdges();

    Collection&lt;Edge&gt; edges(); //returns all the graph's edges

    Edge getEdge(u,v); //returns the edge between u and v, if on exists
    // for an undirected graph getEdge(u,v) == getEdge(v,u)

    Pair&lt;Vertex, Vertex&gt; endVertices(e); //returns the endpoint vertices of edge e

    Vertex oppsite(v,e); //returns the vertex adjacent to v along edge e

    int outDegree(v); //returns the number of edges going out of v

    int inDegree(v); //returns the number of edges coming into v
    //for an undirected graph, inDegree(v) == outDegree(v)

    Collection&lt;Vertex&gt; outgoingEdges(v); //returns all edges that point out of vertex v

    Collection&lt;Vertex&gt; incomingEdges(v); //returns all edges that point into vertex v
    //for an undirected graph, incomingEdges(v) == outgoingEdges(v)

    Vertex insertVertex(x); //creates and returns a new vertex storing element x

    Edge insertEdge(u,v,x); //creates and returns a new edge from vertices u to v, storing element x in the edge

    void removeVertex(v); //removes vertex v and all incident edges from the graph

    void removeEdge(e); //removes edge e from the graph
}
</code></pre>
<h3 id="representations"><a class="header" href="#representations">Representations</a></h3>
<p>There are many different ways to represent a graph in memory.</p>
<h4 id="edge-list"><a class="header" href="#edge-list">Edge List</a></h4>
<p>An edge list is just a list of edges, where each edge knows which two vertices it points to.</p>
<ul>
<li>The <code>Edge</code> object stores
<ul>
<li>It's element</li>
<li>It's origin <code>Vertex</code></li>
<li>It's destination <code>Vertex</code></li>
</ul>
</li>
<li>The edge list stores a sequence of <code>Edge</code> objects</li>
</ul>
<h4 id="adjacency-list"><a class="header" href="#adjacency-list">Adjacency List</a></h4>
<p>In an adjacency list, each vertex stores an array of the vertices adjacent to it.</p>
<ul>
<li>The <code>Vertex</code> object stores
<ul>
<li>It's element</li>
<li>A collection/array of all it's incident edges</li>
</ul>
</li>
<li>The adjacency list stores all <code>Vertex</code> Objects</li>
</ul>
<h4 id="adjacency-matrix"><a class="header" href="#adjacency-matrix">Adjacency Matrix</a></h4>
<p>An adjacency matrix is an $n \times n$ matrix, where $n$ is the number of vertices in the graph. It acts as a lookup table, where each cell corresponds to an edge between two vertices.</p>
<ul>
<li>If there is an edge between two vertices $u$ and $v$, the matrix cell $(u ,, v)$ will contain the edge.</li>
<li>Undirected graphs are symmetrical along the leading diagonal</li>
</ul>
<h2 id="subgraphs"><a class="header" href="#subgraphs">Subgraphs</a></h2>
<ul>
<li>A subgraph $S$ of a graph $G$ is a graph such that:
<ul>
<li>The vertices of $S$ are a subset of the vertices of $G$</li>
<li>The edges of $S$ are a subset of the edges of $G$</li>
</ul>
</li>
<li>A spanning subgraph of $G$ is a subgraph that contains <em>all</em> the vertices of $G$</li>
<li>A graph is <em>connected</em> if there is a path between every pair of vertices</li>
<li>A tree is an undirected graph $T$ such that
<ul>
<li>$T$ is connected</li>
<li>$T$ has no cycles</li>
</ul>
</li>
<li>A forest is an undirected graph without cycles</li>
<li>The connected components of a forest are trees</li>
</ul>
<p><img src="cs126/./img/forest.png" alt="" /></p>
<ul>
<li>A spanning tree of a connected graph is a spanning subgraph that has all vertices covered with a minimum possible number of edges
<ul>
<li>A spanning tree is not unique unless the graph is a tree
<ul>
<li>Multiple spanning trees exist</li>
</ul>
</li>
<li>Spanning trees have applications in the design of communication networks</li>
<li>A spanning forest of a graph is a spanning subgraph that is a forest</li>
</ul>
</li>
</ul>
<h2 id="depth-first-search"><a class="header" href="#depth-first-search">Depth First Search</a></h2>
<p>DFS is a general technique for traversing a graph. A DFS traversal of a graph $G$ will:</p>
<ul>
<li>Visit all vertices and edges of $G$</li>
<li>Determine whether $G$ is connected</li>
<li>Compute the spanning components of $G$</li>
<li>Compute the spanning forest of $G$</li>
</ul>
<p>DFS on a graph with $n$ vertices and $m$ edges takes $O(n+m)$ time. The algorithm is:</p>
<ul>
<li>For a graph $G$ and a vertex $u$ of $G$</li>
<li>Mark vertex $u$ as visited</li>
<li>For each of $u$'s outgoing edges $e = (u,v)$
<ul>
<li>If $v$ has not been visited then
<ul>
<li>Record $e$ as the discovery edge for vertex $v$</li>
<li>Recursively call DFS with on $v$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>DFS(G,V)</code> visits all vertices and edges in the connected component of <code>v</code>, and the discovery edges labelled by <code>DFS(G,V)</code> form a spanning tree of the connected component of <code>v</code>.</p>
<p>DFS can also be extended to path finding, to find a path between two given vertices $u$ and $v$. A stack is used to keep track of the path, and the final state of the stack is the path between the two vertices. As soon as the destination vertex $v$ is encountered, the contents of the stack is returned.</p>
<p>DFS can be used for cycle detection too. A stack is used to keep track of the path between the start vertex and the current vertex. As soon as a back edge $(v,w)$ (an edge we have already been down in the opposite direction) is encountered, we return the cycle as the portion of the stack from the top to the vertex $w$.</p>
<p>To perform DFS on every connected component of a graph, we can loop over every vertex, doing a new DFS from each unvisited one. This will detect all vertices in graphs with multiple connected components.</p>
<h2 id="breadth-first-search"><a class="header" href="#breadth-first-search">Breadth First Search</a></h2>
<p>BFS is another algorithm for graph traversal, similar to DFS. It also requires $O(n + m)$ time. The difference between the two is that BFS uses a stack while DFS uses a queue. The algorithm is as follows:</p>
<ul>
<li>Mark all vertices and edges as unexplored</li>
<li>Create a new queue</li>
<li>Add the starting vertex $s$ to the queue</li>
<li>Mark $s$ as visited</li>
<li>While the queue is not empty
<ul>
<li>Pop a vertex $v$ from the queue</li>
<li>For all neighbouts $w$ of $v$
<ul>
<li>If $w$ is not visited
<ul>
<li>Push $w$ into the queue</li>
<li>Mark $w$ as visited</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>For a connected component $G_s$ of graph $G$ containing $s$:</p>
<ul>
<li>BFS visits all vertices and edges of $G_s$</li>
<li>The discovery edges labelled by <code>BFS(G,s)</code> form a spanning tree of $G_s$</li>
<li>The path of the spanning tree formed by the BFS is the shortest path between the two vertices</li>
</ul>
<p>BFS can be specialised to solve the following problems in $O(n+m)$ time:</p>
<ul>
<li>Compute the connected components of a graph</li>
<li>Compute a spanning forest of a graph</li>
<li>Find a simple cycle in G</li>
<li>Find the shortest path between two vertices
<ul>
<li><strong>DFS cannot do this, this property is unique to BFS</strong></li>
</ul>
</li>
</ul>
<h2 id="directed-graphs"><a class="header" href="#directed-graphs">Directed Graphs</a></h2>
<p>A <em>digraph</em> (short for directed graph) is a graph whose edges are all directed.</p>
<ul>
<li>Each edge goes in only one direction</li>
<li>Edge $(a,b)$ goes from a to b but <strong>not</strong> from b to a</li>
<li>If the graph is simple and has $n$ vertices and $m$ edges, $m \leq n \frac{n-1}{2}$</li>
<li>DFS and BFS can be specialised to traversing directed edges
<ul>
<li>A directed DFS starting at a vertex $s$ determines the vertices <em>reachable</em> from $s$</li>
<li>One vertex is reachable from another if there is a directed path to it</li>
</ul>
</li>
</ul>
<h3 id="strong-connectivity"><a class="header" href="#strong-connectivity">Strong Connectivity</a></h3>
<p>A digraph is said to be <em>strongly connected</em> if each vertex can reach all other vertices. This property can be identified in $O(n+m)$ time with the following algorithm:</p>
<ul>
<li>Pick a vertex $v$ in the graph $G$</li>
<li>Perform a DFS starting from $v$
<ul>
<li>If theres a vertex not visited, return false</li>
</ul>
</li>
<li>Let $G'$ be $G$ with all the edge directions reversed</li>
<li>Perform a DFS starting from $v$ in $G'$
<ul>
<li>If theres a vertex not visited, return false</li>
<li>Else, return True</li>
</ul>
</li>
</ul>
<h3 id="transitive-closure"><a class="header" href="#transitive-closure">Transitive Closure</a></h3>
<p>Given a digraph $G$, the transitive closure of $G$ is the digraph $G*$ such that:</p>
<ul>
<li>$G*$ has the same vertices as $G$</li>
<li>If $G$ has a directed path from $u$ to $v$, then G* also has a directed *edge* from $u$ to $v$</li>
<li>In $G*$, every pair of vertices with a path between them in $G$ is now adjacent</li>
<li>The transitive closure provides reachability information about a digraph</li>
</ul>
<p><img src="cs126/./img/G-star.png" alt="" /></p>
<p>The transitive closure can be computed by doing a DFS starting at each vertex. However, this takes $O(n(n+m))$ time. Alternatively, there is the Floyd-Warshall algorithm:</p>
<ul>
<li>For the graph $G$, number the vertices $1,2,...,n$</li>
<li>Compute the graphs $G_0, ..., G_n$
<ul>
<li>$G_0 = G$</li>
<li>$G_k$ has directed edge $(v_i,v_j)$ if $G$ has a directed path from $v_i$ to $v_j$ with intermediate vertices ${v_1, ..., v_k}$</li>
</ul>
</li>
<li>Digraph $G_k$ is computed from $G_{k-1}$</li>
<li>$G_n = G*$</li>
<li>Add $(v_i,v_j)$ if edges $(v_i,v_k)$ and $(v_k,v_j)$ appear in $G_{k-1}$</li>
</ul>
<p>In pseudocode:</p>
<pre><code class="language-java">for k=1 to n
    Gk = Gk_1
      for i=1 to n (i != k)
          for j=1 to n (j != i, j!=k)
              if G_(k-1).areAdjacent(vi,vk) &amp;&amp; G_(k-1).areAdjacent(vk,vj)
                  if !G_(k-1).areAdjacent(vi,vj)
                      G_k.insertDirectedEdge(vi,vj,k)
  return G_n
</code></pre>
<p>This algorithm takes $O(n^3)$ time. Basically, at each iteration a new vertex is introduced, and each vertex is checked to see if a path exists through the newly added vertex. If it does, a directed edge is inserted to transitively close the graph.</p>
<h3 id="topological-ordering"><a class="header" href="#topological-ordering">Topological Ordering</a></h3>
<ul>
<li>A Directed Acyclic Graph (DAG) is digraph that has no directed cycles</li>
<li>A topological ordering of a digraph is a numbering $v_1, v_2, ..., v_n$ of the vertices such that for every edge $(v_i, v_j)$, $i &lt; j$
<ul>
<li>The vertex it points to is always greater than it</li>
</ul>
</li>
<li>A digraph can have a topological ordering <strong>if and only if it is a DAG</strong></li>
</ul>
<p>A topological ordering can be calculated using a DFS:</p>
<pre><code class="language-java">public static void topDFS(Graph G, Vertex v){
    v.visited = true
    for(Edge e: v.edges){
        w = opposite(v,e)
        if(w.visited = false)
            topDFS(G,w)
        else{
            v.label = n
            n = n-1
        }
    }
}
</code></pre>
<p>The first node encountered in the DFS is assigned $n$, the one after that $n-1$, and so on until all nodes are labelled.</p>
<p><img src="cs126/./img/top-order.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs132"><a class="header" href="#cs132">CS132</a></h1>
<p>Note that specifics details of architectures such as the 68k, its specific instruction sets, or the PATP are not examinable. They are included just to serve as examples.</p>
<p>The 68008 datasheet can be found <a href="cs132/pdf.datasheetcatalog.com/datasheet/motorola/MC68008.pdf">here</a>, as a useful resource.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="digital-logic"><a class="header" href="#digital-logic">Digital Logic</a></h1>
<p>Digital logic is about reasoning with systems with two states: on and off (0 and 1 (binary)).</p>
<h2 id="basic-logic-functions"><a class="header" href="#basic-logic-functions">Basic Logic Functions</a></h2>
<p>Some basic logic functions, along with their truth tables.</p>
<h4 id="not"><a class="header" href="#not">NOT</a></h4>
<p>$$f = \bar A$$</p>
<table><thead><tr><th>A</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>1</td></tr>
<tr><td>1</td><td>0</td></tr>
</tbody></table>
<h4 id="and"><a class="header" href="#and">AND</a></h4>
<p>$$f = A \cdot B$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<h4 id="or"><a class="header" href="#or">OR</a></h4>
<p>$$f = A + B$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<h4 id="xor"><a class="header" href="#xor">XOR</a></h4>
<p>$$f = A \oplus B$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>0</td></tr>
</tbody></table>
<h4 id="nand"><a class="header" href="#nand">NAND</a></h4>
<p>$$f = \overline{A \cdot B}$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>0</td></tr>
</tbody></table>
<h4 id="nor"><a class="header" href="#nor">NOR</a></h4>
<p>$$f = \overline{A + B}$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td></tr>
</tbody></table>
<h4 id="x-nor"><a class="header" href="#x-nor">X-NOR</a></h4>
<p>$$f = \overline{A \oplus B}$$</p>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<h3 id="logic-gates"><a class="header" href="#logic-gates">Logic Gates</a></h3>
<p>Logic gates represent logic functions in a circuit. Each logic gate below represents one of the functions shown above.</p>
<p><img src="cs132/./img/gates.png" alt="" /></p>
<h3 id="logic-circuits"><a class="header" href="#logic-circuits">Logic Circuits</a></h3>
<p>Logic circuits can be built from logic gates, where outputs are logical functions of their inputs. Simple functions can be used to build up more complex ones. For example, the circuit below implements the XOR function.</p>
<p>$$f = \bar A \cdot B + A \cdot \bar B$$</p>
<p><img src="cs132/./img/xor-circuit.png" alt="" /></p>
<p>Another example, using only NAND gates to build XOR. NAND (or NOR) gates can be used to construct <em>any</em> logic function.</p>
<p><img src="cs132/./img/xor-nand.png" alt="" /></p>
<p>Truth tables can be constructed for logic circuits by considering intermediate signals. The circuit below has 3 inputs and considers 3 intermediate signals to construct a truth table.</p>
<p><img src="cs132/./img/circuit1.png" alt="" /></p>
<p>$$
P = A \cdot B \
Q = A \cdot B\
R = A \cdot C \
$$</p>
<p>$$f = P + Q + R = A \cdot B + A \cdot B +  A \cdot C$$</p>
<table><thead><tr><th>A</th><th>B</th><th>C</th><th>P</th><th>Q</th><th>R</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<p>Truth tables of circuits are important as they enumerate all possible outputs, and help to reason about logic circuits and functions.</p>
<h2 id="boolean-algebra"><a class="header" href="#boolean-algebra">Boolean Algebra</a></h2>
<ul>
<li>Logic expressions, like normal algebraic ones, can be simplified to reduce complexity
<ul>
<li>This reduces the number of gates required for their implementation</li>
<li>The less gates, the more efficient the circuit is
<ul>
<li>More gates is also more expensive</li>
</ul>
</li>
</ul>
</li>
<li>Sometimes, only specific gates are available too and equivalent expressions must be found that use only the available gates</li>
<li>Two main ways to simplify expressions
<ul>
<li>Boolean algebra</li>
<li>Karnaugh maps</li>
</ul>
</li>
<li>The truth table for the expression before and after simplifying <em>must</em> be identical, or you've made a mistake</li>
</ul>
<h3 id="expressions-from-truth-tables"><a class="header" href="#expressions-from-truth-tables">Expressions from Truth Tables</a></h3>
<p>A sum of products form of a function can be obtained from it's truth table directly.</p>
<table><thead><tr><th>A</th><th>B</th><th>C</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<p>Taking only the rows that have an output of 1:</p>
<ul>
<li>The first row of the table: $\bar A \cdot \bar B \cdot \bar C$</li>
<li>The second row: $\bar A \cdot \bar B \cdot C$</li>
<li>Fifth: $ A \cdot \bar B \cdot \bar C$</li>
<li>Seventh: $ A \cdot B \cdot \bar C$</li>
<li>Eight: $A \cdot B \cdot C$</li>
</ul>
<p>Summing the products yields:</p>
<p>$$f = (\bar A \cdot \bar B \cdot \bar C) + (\bar A \cdot \bar B \cdot C) + (A \cdot \bar B \cdot \bar C) + (A \cdot B \cdot \bar C) + (A \cdot B \cdot C)$$</p>
<h3 id="boolean-algebra-laws"><a class="header" href="#boolean-algebra-laws">Boolean Algebra Laws</a></h3>
<p>There are several laws of boolean algebra which can be used to simplify logic expressions:</p>
<table><thead><tr><th>Name</th><th>AND form</th><th>OR form</th></tr></thead><tbody>
<tr><td>Identity Law</td><td>$1A = A$</td><td>$0 + A = A$</td></tr>
<tr><td>Null Law</td><td>$0A = A$</td><td>$1 + A = 1$</td></tr>
<tr><td>Idempotent Law</td><td>$AA = A$</td><td>$A + A = A$</td></tr>
<tr><td>Inverse Law</td><td>$A\bar A = 0$</td><td>$A + \bar A = 1$</td></tr>
<tr><td>Commutative Law</td><td>$AB = BA$</td><td>$A + B = B + A$</td></tr>
<tr><td>Associative Law</td><td>$(AB)C = A(BC) = ABC$</td><td>$(A + B) + C = A + (B+C) = A + B + C$</td></tr>
<tr><td>Distributive Law</td><td>$A + BC = (A+B)(A+C)$</td><td>$A(B+C) = AB + AC$</td></tr>
<tr><td>Absorption Law</td><td>$A(A+B) = A$</td><td>$A + AB = A$</td></tr>
<tr><td>De Morgan's Law</td><td>$\overline{A\cdot B} = \bar A + \bar B$</td><td>$\overline{A + B} = \bar A \cdot \bar B$</td></tr>
</tbody></table>
<ul>
<li>Can go from AND to OR form (and vice versa) by swapping AND for OR, and 0 for 1</li>
</ul>
<p>Most are fairly intuitive, but some less so. The important ones to remember are:</p>
<ul>
<li>$A + BC = (A+B)(A+C)$</li>
<li>$A(B+C) = AB + AC$</li>
<li>$A(A+B) = A$</li>
<li>$A + AB = A$</li>
</ul>
<h3 id="de-morgans-laws"><a class="header" href="#de-morgans-laws">De Morgan's Laws</a></h3>
<p>De Morgan's Laws are very important and useful ones, as they allow to easily go from AND to OR. In simple terms:</p>
<ul>
<li>Break the negation bar</li>
<li>Swap the operator</li>
</ul>
<h3 id="example-1"><a class="header" href="#example-1">Example 1</a></h3>
<p>When doing questions, all working steps should be annotated.</p>
<p>$$f = (\overline{X + Y}) \cdot (\overline{\bar X + Y}) $$
$$f = (\bar X \cdot \bar Y) \cdot (\overline{\bar X + Y}) \quad \text{De Morgan OR form}$$
$$f = (\bar X \cdot \bar Y) \cdot (X \cdot \bar Y) \quad \text{De Morgan AND form}$$
$$f = \bar X \cdot \bar Y \cdot X \cdot \bar Y \quad \text{Remove brackets (associative law)}$$
$$f = \bar X \cdot X \cdot \bar Y \cdot \bar Y \quad \text{Re-order (commutative law)}$$
$$f = 0 \cdot \bar Y \quad \text{Inverse and idempotent laws}$$
$$f = 0 \quad \text{Null law}$$</p>
<h3 id="example-2"><a class="header" href="#example-2">Example 2</a></h3>
<p>$$f = X + \bar Y + \bar X \cdot Y + (X + \bar Y) \cdot \bar X \cdot Y $$
$$f = X + \bar Y + \bar X \cdot Y + X \cdot \cdot \bar X \cdot Y + \bar Y \cdot \bar X \cdot Y \quad \text{Distributive law}$$
$$f = X + \bar Y + \bar X \cdot Y + 0 + 0 \quad \text{Inverse AND law}$$
$$f = X + (\bar Y + \bar X)(\bar Y + Y) \quad \text{Distributive law}$$
$$f = X + (\bar Y + \bar X)\cdot 1 \quad \text{Inverse law}$$
$$f = X + \bar Y + \bar X \quad \text{Removing 1 and brackets (identity and associative laws)}$$
$$f = \bar Y + 1 \quad \text{Inverse OR law}$$
$$f= 1 \quad \text{Null law}$$</p>
<h2 id="karnaugh-maps"><a class="header" href="#karnaugh-maps">Karnaugh Maps</a></h2>
<ul>
<li>Karnaugh Maps (k-maps) are sort of like a 2D- truth table</li>
<li>Expressions can be seen from the location of 1s in the map</li>
</ul>
<table><thead><tr><th>A</th><th>B</th><th>f</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>a</td></tr>
<tr><td>0</td><td>1</td><td>b</td></tr>
<tr><td>1</td><td>0</td><td>d</td></tr>
<tr><td>1</td><td>1</td><td>c</td></tr>
</tbody></table>
<p><img src="cs132/./img/k-2.png" alt="" /></p>
<ul>
<li>Functions of 3 variables can used a 4x2 or 2x4 map (4 variables use a 4x4 map)</li>
</ul>
<p><img src="cs132/./img/k-3.png" alt="" /></p>
<ul>
<li>Adjacent squares in a k-map differ by exactly 1 variable
<ul>
<li>This makes the map <em>gray coded</em></li>
</ul>
</li>
<li><strong>Adjacency also wraps around</strong></li>
</ul>
<p>The function $f = AB\bar C D + A \bar B \bar C D + \bar A \bar B C D + \bar A B C D$ is shown in the map below.</p>
<p><img src="cs132/./img/k-4.png" alt="" /></p>
<h3 id="grouping"><a class="header" href="#grouping">Grouping</a></h3>
<ul>
<li>Karnaugh maps contain groups, which are rectangular clusters of 1s -</li>
<li>To simplify a logic expression from a k-map, identify groups from it, making them as large and as few as possible</li>
<li>The number of elements in the group <strong>must be a power of 2</strong></li>
<li>Each group can be described by a singular expression</li>
<li>The variables in the group are the ones that are constant within the group (ie, define that group)</li>
</ul>
<p><img src="cs132/./img/kmap-groups.png" alt="" /></p>
<p>Sometimes, groups overlap which allow for more than one expression</p>
<p><img src="cs132/./img/kmap-groups-2.png" alt="" /></p>
<p>The function for the map is therefore either $f =\bar A \bar B \bar C + \bar A B D + B C D $ or $ f = \bar A \bar B \bar C + \bar A \bar C D + B C D $ (both are equivalent)</p>
<p>Sometimes it is not possible to minimise an expression. the map below shows an XOR function $f = (A \oplus B) \oplus (C \oplus D)$</p>
<p><img src="cs132/./img/kmap-xor.png" alt="" /></p>
<h3 id="dont-care-conditions"><a class="header" href="#dont-care-conditions">Don't Care Conditions</a></h3>
<p>Sometimes, a certain combination of inputs can't happen, or we dont care about the output if it does. An <code>X</code> is used to denote these conditions, which can be assumed as either 1 or 0, whichever is more convenient.</p>
<p><img src="cs132/./img/dont-care.png" alt="" /></p>
<h2 id="combinatorial-logic-circuits"><a class="header" href="#combinatorial-logic-circuits">Combinatorial Logic Circuits</a></h2>
<p>Some useful circuits can be constructed using logic gates, examples of which are shown below. Combinatorial logic circuits operate as fast as the gates operate, which is theoretically zero time (realistically, there is a nanosecond-level tiny propagation delay).</p>
<h3 id="1-bit-half-adder"><a class="header" href="#1-bit-half-adder">1-Bit Half Adder</a></h3>
<ul>
<li>Performs the addition of 2 bits, outputting the result and a carry bit.</li>
</ul>
<p><img src="cs132/./img/half-adder.png" alt="" /></p>
<table><thead><tr><th>A</th><th>B</th><th>Sum</th><th>Carry</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td><td>1</td></tr>
</tbody></table>
<h3 id="1-bit-full-adder"><a class="header" href="#1-bit-full-adder">1-Bit Full Adder</a></h3>
<ul>
<li>Adds 2 bits plus carry bit, outputting the result and a carry bit.</li>
</ul>
<p><img src="cs132/./img/full-adder.png" alt="" /></p>
<table><thead><tr><th>Carry in</th><th>A</th><th>B</th><th>Sum</th><th>Carry out</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>0</td><td>0</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>0</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr>
</tbody></table>
<h3 id="n-bit-full-adder"><a class="header" href="#n-bit-full-adder">N-Bit Full Adder</a></h3>
<ul>
<li>Combination of a number of full adders</li>
<li>The carry out from the previous adder feeds into the carry in of the next</li>
</ul>
<p><img src="cs132/./img/nbit-adder.png" alt="" /></p>
<h3 id="n-bit-addersubtractor"><a class="header" href="#n-bit-addersubtractor">N-Bit Adder/Subtractor</a></h3>
<ul>
<li>To convert an adder to an adder/subtractor, we need a control input $Z$ such that:
<ul>
<li>$Z = 0 \Rightarrow S = A + B$</li>
<li>$Z = 1 \Rightarrow S = A - B$</li>
</ul>
</li>
<li>$-B$ is calculated using two's complement
<ul>
<li>Invert the N bit binary number B by doing $Z \oplus B$</li>
<li>Add 1 (make the starting carry in a 1)</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/subber.png" alt="" /></p>
<h3 id="encoders--decoders"><a class="header" href="#encoders--decoders">Encoders &amp; Decoders</a></h3>
<ul>
<li>A decoder has binary input pins, and one output pin per possible input state</li>
<li>eg 2 inputs has 4 unique states so has 4 outputs
<ul>
<li>3 inputs has 8 outputs</li>
</ul>
</li>
<li>Often used for addressing memory</li>
<li>The decoder shown below is <em>active low</em>
<ul>
<li>Active low means that 0 = active, and 1 = inactive
<ul>
<li>Converse to what would usually be expected</li>
</ul>
</li>
<li>Active low pins sometimes labelled with a bar, ie $\overline{\text{enable}}$</li>
</ul>
</li>
<li>It is important to be aware of this, as ins and outs must comform to the same standard</li>
</ul>
<p><img src="cs132/./img/decoder.png" alt="" /></p>
<table><thead><tr><th>$X_0$</th><th>$X_1$</th><th>$Y_0$</th><th>$Y_1$</th><th>$Y_2$</th><th>$Y_3$</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>1</td><td>1</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>0</td></tr>
</tbody></table>
<ul>
<li>Encoders are the opposite of decoders, encoding a set of inputs into outputs</li>
<li>Multiple input pins, only one should be active at a time</li>
<li>Active low encoder shown below</li>
</ul>
<p><img src="cs132/./img/encoder.png" alt="" /></p>
<table><thead><tr><th>$Y_0$</th><th>$Y_1$</th><th>$Y_2$</th><th>$Y_3$</th><th>$X_0$</th><th>$X_1$</th></tr></thead><tbody>
<tr><td>0</td><td>1</td><td>1</td><td>1</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>0</td><td>1</td><td>1</td></tr>
</tbody></table>
<h3 id="multiplexers--de-multiplexers"><a class="header" href="#multiplexers--de-multiplexers">Multiplexers &amp; De-Multiplexers</a></h3>
<p>Multiplexers have multiple inputs, and then selector inputs which choose which of the inputs to put on the output.</p>
<p><img src="cs132/./img/mux.png" alt="" /></p>
<table><thead><tr><th>$S_0$</th><th>$S_1$</th><th>Y</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>$X_0$</td></tr>
<tr><td>0</td><td>1</td><td>$X_1$</td></tr>
<tr><td>1</td><td>0</td><td>$X_2$</td></tr>
<tr><td>1</td><td>1</td><td>$X_3$</td></tr>
</tbody></table>
<p>$$Y = X_0 \bar S_0 \bar S_1 + X_1 \bar S_0 S_1 + X_2 S_0 \bar S_1 + X_3 S_0 S_1$$</p>
<p>De-Multiplexers are the reverse of multiplexers, taking one input and selector inputs choosing which output it appears on. The one shown below is active low</p>
<p><img src="cs132/./img/dmux.png" alt="" /></p>
<table><thead><tr><th>$S_0$</th><th>$S_1$</th><th>$Y_0$</th><th>$Y_1$</th><th>$Y_2$</th><th>$Y_3$</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>A</td><td>1</td><td>1</td><td>1</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>A</td><td>1</td><td>1</td></tr>
<tr><td>1</td><td>0</td><td>1</td><td>1</td><td>A</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td><td>A</td></tr>
</tbody></table>
<p>$$Y_0 = A + \bar S_0 S_1 + S_0 \bar S_1 + S_0 S_1 = A + S_1 + S_0$$</p>
<p>Multiplexers and De-Multiplexers are useful in many applications:</p>
<ul>
<li>Source selection control</li>
<li>Share one communication line between multiple senders/receivers</li>
<li>Parallel to serial conversion
<ul>
<li>Parallel input on X, clock signal on S, serial output on Y</li>
</ul>
</li>
</ul>
<h2 id="sequential-logic-circuits"><a class="header" href="#sequential-logic-circuits">Sequential Logic Circuits</a></h2>
<p>A logic circuit whose outputs are logical functions of its inputs <em>and</em> it's current state</p>
<h3 id="flip-flops"><a class="header" href="#flip-flops">Flip-Flops</a></h3>
<p>Flip-flops are the basic elements of sequential logic circuits. They consist of two nand gates whose outputs are fed back to the inputs to create a bi-stable circuit, meaning it's output is only stable in two states.</p>
<p><img src="cs132/./img/flipflop.png" alt="" /></p>
<ul>
<li>$\bar S$ and $\bar R$ are active low <strong>set</strong> and <strong>reset</strong> inputs</li>
<li>$Q$ is set high when $\bar S = 0$ and $\bar R = 1$</li>
<li>$Q$ is reset (to zero) when $\bar R = 0$ and $\bar S = 1$</li>
<li>If $\bar S = \bar R = 1$ then $Q$ does not change</li>
<li>If both $\bar S$ and $\bar R$ are zero, this is a hazard condition and the output is invalid</li>
</ul>
<table><thead><tr><th>$\bar S$</th><th>$\bar R$</th><th>Q</th><th>P</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>X</td><td>X</td></tr>
<tr><td>0</td><td>1</td><td>1</td><td>0</td></tr>
<tr><td>1</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>X</td><td>X</td></tr>
</tbody></table>
<p>The timing diagram shows the operation of the flip flop</p>
<p><img src="cs132/./img/timing.png" alt="" /></p>
<h3 id="d-type-latch"><a class="header" href="#d-type-latch">D-Type Latch</a></h3>
<p>A D-type latch is a modified flip-flop circuit that is essentially a 1-bit memory cell.</p>
<p><img src="cs132/./img/dlatch.png" alt="" /></p>
<ul>
<li>Output can only change when the enable line is high</li>
<li>$D=Q$ when enabled, otherwise $Q$ does not change ($Q=Q$)</li>
<li>When enabled, data on $D$ goes to $Q$</li>
</ul>
<table><thead><tr><th>Enable</th><th>$D$</th><th>$Q$</th><th>$\bar Q$</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>$Q$</td><td>$\bar Q$</td></tr>
<tr><td>0</td><td>1</td><td>$Q$</td><td>$\bar Q$</td></tr>
<tr><td>1</td><td>0</td><td>0</td><td>1</td></tr>
<tr><td>1</td><td>1</td><td>1</td><td>0</td></tr>
</tbody></table>
<h3 id="clocked-flip-flop"><a class="header" href="#clocked-flip-flop">Clocked Flip-Flop</a></h3>
<p>There are other types of clocked flip-flop whose output only changes on the rising edge of the clock input.</p>
<ul>
<li>$\uparrow$ means rising edge responding</li>
</ul>
<p><img src="cs132/./img/clocked-flops.png" alt="" /></p>
<h3 id="n-bit-register"><a class="header" href="#n-bit-register">N-bit Register</a></h3>
<ul>
<li>A multi-bit memory circuit built up from d-type latches</li>
<li>The number on $A_{N-1}, A_{N-2},..., A_1, A_0$ is stored in the registers when the clock rises</li>
<li>The stored number appears on the outputs $Q$</li>
<li>$Q$ cannot change unless the circuit is clocked</li>
<li>Parallel input, parallel output</li>
</ul>
<p><img src="cs132/./img/nbit-reg.png" alt="" /></p>
<h3 id="n-bit-shift-register"><a class="header" href="#n-bit-shift-register">N-bit Shift Register</a></h3>
<ul>
<li>A register that stores and shifts bits taking one bit input at a time</li>
<li>Serial input, parallel output</li>
<li>When a clock transition occurs, each bit in the register will be shifted one place</li>
<li>Useful for serial to parallel conversion</li>
</ul>
<p><img src="cs132/./img/shift-reg.png" alt="" /></p>
<h3 id="n-bit-counter"><a class="header" href="#n-bit-counter">N-bit Counter</a></h3>
<ul>
<li>The circles on the clock inputs are inverted on all but the first</li>
<li>Each flip-flop is triggerd on a high -&gt; low transition of the previous flip-flop</li>
<li>Creates a counter circuit</li>
</ul>
<p><img src="cs132/./img/counter.png" alt="" /></p>
<p>Output is 0000, 1000, 0100, 1100, 0010, etc...</p>
<ul>
<li>The first bit swaps every clock</li>
<li>2nd bit swaps every other clock</li>
<li>3rd bit swaps every fourth clock</li>
<li>etc...</li>
</ul>
<h2 id="three-state-logic"><a class="header" href="#three-state-logic">Three State Logic</a></h2>
<ul>
<li>Three state logic introduces a third state to logic - <strong>unconnected</strong></li>
<li>A three-state buffer has an enable pin, which when set high, disconnects the output from the input</li>
<li>Used to prevent connecting outputs to outputs, as this can cause issues (short circuits)</li>
</ul>
<p><img src="cs132/./img/3state.png" alt="" /></p>
<p>This can be used to allow different sources of data onto a common bus. Consider a 4-bit bus, where 2 4-bit inputs are connected using 3-state buffers. Only one of the buffers should be enabled at any one time.</p>
<p><img src="cs132/./img/3state-bus.png" alt="" /></p>
<ul>
<li>When $\overline{E1} = 0$, A will be placed on the bus</li>
<li>When $\overline{E2} = 0$, B will be placed on the bus</li>
</ul>
<h2 id="physical-implementations"><a class="header" href="#physical-implementations">Physical Implementations</a></h2>
<p>Logic gates are physical things with physical properties, and these have to be considered when designing with them. Typical voltage values for TTL (Transistor-Transistor Logic):</p>
<ul>
<li>5v - max voltage</li>
<li>2.8v - minimum voltage for a logical 1</li>
<li>2.8-0.8v - &quot;forbidden region&quot;, ie voltages in this region are undefined</li>
<li>0.8-0v - voltage range for a logical 0</li>
</ul>
<h3 id="propagation-delay"><a class="header" href="#propagation-delay">Propagation Delay</a></h3>
<ul>
<li>Logic gates have a propagation delay, the amount of time it takes for the output to reflect the input
<ul>
<li>Typically a few nanoseconds or less</li>
</ul>
</li>
<li>This limits the speed at which logic circuits can operate</li>
<li>Delay can be reduced by increasing density of gates on an IC</li>
</ul>
<h3 id="integrated-circuits"><a class="header" href="#integrated-circuits">Integrated Circuits</a></h3>
<ul>
<li>Elementary logic gates can be obtained in small ICs</li>
<li>Programmable deviced allow large circuits to be created inside a single chip
<ul>
<li><strong>PAL</strong> - Programmable Array Logic
<ul>
<li>One-time programmamble</li>
</ul>
</li>
<li><strong>PLA</strong> - Programmable Logic Array
<ul>
<li>Contains an array of AND and OR gates to implement any logic functions</li>
</ul>
</li>
<li><strong>FPGA</strong> - Field Programmable Gate Array
<ul>
<li>Contains millions of configurable gates</li>
<li>More modern</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="pla-example"><a class="header" href="#pla-example">PLA example</a></h4>
<p>A PLA allows for the implementation of any sum-of-products function, as it has an array of AND gates, then OR gates, with fuses that can be broken to implement a specific function.</p>
<p><img src="cs132/./img/PLA.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="assembly"><a class="header" href="#assembly">Assembly</a></h1>
<h2 id="microprocessor-fundamentals"><a class="header" href="#microprocessor-fundamentals">Microprocessor Fundamentals</a></h2>
<h3 id="the-cpu"><a class="header" href="#the-cpu">The CPU</a></h3>
<ul>
<li>The CPU controls and performs the execution of instructions</li>
<li>Does this by continuously doing fetch-decode-execute cycle</li>
<li>Very complex, but two key components
<ul>
<li>Control Unit (CU)
<ul>
<li>Decodes the instructions and handles logistics</li>
</ul>
</li>
<li>Arithmetic Logic Unit (ALU)
<ul>
<li>Does maths</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="fetch-decode-execute"><a class="header" href="#fetch-decode-execute">Fetch-Decode-Execute</a></h3>
<ul>
<li>Three steps to every cycle
<ul>
<li>Fetch instructions from memory</li>
<li>Decode into operations to be performed</li>
<li>Execute to change state of CPU</li>
</ul>
</li>
<li>Takes place over several clock cycles</li>
</ul>
<p>The components of the CPU that are involved in the cycle:</p>
<ul>
<li>ALU</li>
<li>CU</li>
<li>Program Counter (PC)
<ul>
<li>Tracks the memory address of the next instruction to be executed</li>
</ul>
</li>
<li>Instruction Register (IR)
<ul>
<li>Contains the most recent instruction fetched</li>
</ul>
</li>
<li>Memory Address Register (MAR)
<ul>
<li>Contains address of the memory location to be read/written</li>
</ul>
</li>
<li>Memory Data/Buffer Register (MDR/MBR)
<ul>
<li>Contains data fetched from memory or to be written to memory</li>
</ul>
</li>
</ul>
<p>The steps of the cycle:</p>
<ul>
<li>Fetch
<ul>
<li>Instruction fetched from memory location held by PC</li>
<li>Fetched instruction stored in IR</li>
<li>PC incremented to point to next instruction</li>
</ul>
</li>
<li>Decode
<ul>
<li>Retrieved instruction decoded</li>
<li>Establish opcode type</li>
</ul>
</li>
<li>Execute
<ul>
<li>CU signals the necessary CPU components</li>
<li>May result in changes to data registers, ALU, I/O, etc</li>
</ul>
</li>
</ul>
<h2 id="the-68008"><a class="header" href="#the-68008">The 68008</a></h2>
<p>The 68008 is an example of a CPU. The &quot;programmer's model&quot; is an abstraction that represents the internals of the architecture. The internal registers as shown below are part of the programmer's model.</p>
<p><img src="cs132/./img/68k.png" alt="" /></p>
<ul>
<li>Internal registers are 32 bits wide</li>
<li>Internal data buses are 16 bit wide</li>
<li>8 bit external data bus</li>
<li>20 bit external address bus</li>
<li>D0-D7 are 32 bit registers used to store frequently used values
<ul>
<li>Can be long (32 bits), word (16 bits), or byte (8 bits)</li>
</ul>
</li>
<li>Status register (CCR) consists of 2 8-bit registers
<ul>
<li>Various status bits are set or reset depending upon conditions arising from execution</li>
</ul>
</li>
<li>A0-A6 are pointer registers</li>
<li>A7 is system stack pointer to hold subroutine return addresses</li>
<li>Operations on addresses do not alter status register/ CCR
<ul>
<li>Only ALU can incur changes in status</li>
</ul>
</li>
<li>The stack pointer is a pointer to the next free location in the system stack
<ul>
<li>Provides temporary storage of state, return address, registers, etc during subroutine calls and interrupts</li>
</ul>
</li>
</ul>
<p>The diagram shows the internal architecture of the CPU, and how the internal registers are connected via the buses. Note how and which direction data moves in, as indicated by the arrows on the busses.</p>
<p><img src="cs132/./img/68k-2.png" alt="" /></p>
<h2 id="register-transfer-language"><a class="header" href="#register-transfer-language">Register Transfer Language</a></h2>
<p>The fetch-decode-execute cycle is best described using Register Transfer Language (RLT), a notation used to show how data moves around the internals of a processor and between registers.</p>
<ul>
<li>For example <code>[MAR] &lt;- [PC]</code> denotes the transfer of the contents of the program counter to the memory address register</li>
<li>Computer's main memory is called Main Store (MS), and the contents of memory location <code>N</code> is denoted <code>[MS(N)]</code></li>
<li>RLT does not account for the pipelining of instructions</li>
<li>Fetching an instruction in RTL:</li>
</ul>
<table><thead><tr><th>RLT</th><th>Meaning</th></tr></thead><tbody>
<tr><td><code>[MAR] &lt;- [PC]</code></td><td>Move contents of PC to MAR</td></tr>
<tr><td><code>[PC] &lt;- [PC] + 1</code></td><td>Increment PC</td></tr>
<tr><td><code>[MBR] &lt;- [MS([MAR])]</code></td><td>Read address from MAR into MBR.</td></tr>
<tr><td><code>[IR] &lt;- [MBR]</code> -</td><td>Load instruction into I</td></tr>
<tr><td><code>CU &lt;- [IR(opcode)]</code></td><td>Decode the instruction</td></tr>
</tbody></table>
<h2 id="assembly-language"><a class="header" href="#assembly-language">Assembly Language</a></h2>
<ul>
<li>Assembly is the lowest possible form of code</li>
<li>High level code (for example C) is compiled to assembly code</li>
<li>Assembly is then assembled into machine code (binary)</li>
<li>Assembly instructions map 1:1 to processor operations</li>
<li>Uses mnemonics for instructions, ie <code>MOV</code> or <code>ADD</code></li>
<li>Languages vary, but format tends to be similar: <code>LABEL: OPCODE OPERAND(S) | COMMENT</code></li>
</ul>
<p>An example program is shown below</p>
<pre><code>    ORG  $4B0      | this program starts at hex 4B0
    move.b #5, D0  | load D0 with number 5
    add.b  #$A, D0 | add 10 (0x0A) to D0
    move.b D0, ANS | move contents of D0 to ANS
ANS: DS.B 1        | leave 1 byte of memory empty and name it ANS
</code></pre>
<ul>
<li><code>#</code> indicates a literal</li>
<li><code>$</code> means hexadecimal</li>
<li><code>%</code> means binary</li>
<li>A number without a prefix is a memory address</li>
<li><code>ANS</code> is a symbolic name</li>
<li><code>ORG</code> (Origin) indicates where to load the program in memory</li>
<li><code>DS</code> (Define Storage) tells the assembler where to put data</li>
</ul>
<h3 id="the-68008-instruction-set"><a class="header" href="#the-68008-instruction-set">The 68008 Instruction Set</a></h3>
<ul>
<li>Instructions are commands that tell the processor what to do</li>
<li>5 main kinds of instructions
<ul>
<li>Logical
<ul>
<li>Bitwise operations</li>
<li><code>AND</code>, <code>LSL</code> (Logical Shift Left)</li>
</ul>
</li>
<li>Branch
<ul>
<li>Cause the processor to jump execution to a labelled address</li>
<li>Condition is specified by testing state of CCR set by previous instruction</li>
<li><code>BRA</code> - branch unconditionally</li>
<li><code>BEQ</code> - branch if equal</li>
</ul>
</li>
<li>System Control</li>
</ul>
</li>
<li>Instructions are also specified with their data type, <code>.b</code> for byte, <code>.w</code> for word, <code>.l</code> for long
<ul>
<li><code>move.w</code> moves 2 bytes</li>
</ul>
</li>
</ul>
<h4 id="data-movement"><a class="header" href="#data-movement">Data Movement</a></h4>
<ul>
<li>Similar to RTL</li>
</ul>
<pre><code>move.b D0,   D1 | [D1(0:7)] &lt;- [D0(0:7)]
move.w D0,   D1 | [D1(0:15)] &lt;- [D0(0:15)]
swap   D2       | swap lower and upper words
move.l $F20, D3  | [D3(24:31)] ← [MS($F20)]
                | [D3(16:23)] ← [MS($F21)]
                | [D3( 8:15)] ← [MS($F22)]
                | [D3( 0:7)] ← [MS($F23)]
                | copied 8 bytes at a time in big endian order
</code></pre>
<h4 id="arithmetic"><a class="header" href="#arithmetic">Arithmetic</a></h4>
<ul>
<li>Maths performed on the ALU</li>
<li>The 68008, like many older processors, has no FPU, so only integer operations are supported</li>
</ul>
<pre><code>add.l   Di, Dj  | [Dj] ← [Di] + [Dj]
addx.w  Di, Dj  | also add in x bit from CCR
sub.b   Di, Dj  | [Dj] ← [Dj] - [Di]
subx.b  Di, Dj  | also subtract x bit from CCR
mulu.w  Di, Dj  | [Dj(0:31)] ← [Di(0:15)] * [Dj(0:15)]
                | unsigned multiplication
muls.w  Di, Dj  | signed multiplication
</code></pre>
<h4 id="logical"><a class="header" href="#logical">Logical</a></h4>
<ul>
<li>Perform bitwise operations on data</li>
<li>Also done by ALU</li>
<li>AND, OR, etc but also shifts and rotates</li>
<li>Logical shift (<code>LSL</code>/<code>LSR</code>) adds a 0 when shifting
<ul>
<li>Bit shifted out goes into C and X</li>
</ul>
</li>
<li>Arithmetic shift preserves sign bit (<code>ASL</code>/<code>ASR</code>)</li>
<li>Normal rotate (<code>ROL</code>/<code>ROR</code>) moves the top of the bit to the bottom bit and also puts the top bit into C and X</li>
<li>Rotate through X (<code>ROXL</code>/<code>ROXR</code>) rotates the value through the X register</li>
</ul>
<pre><code>AND.B #$7F, D0 | [D0] &lt;- [D0] . [0x7F]
OR.B  D1,  D0 | [D0] &lt;- [D0] + [D1]
LSL D0,    2  | [D0] &lt;- [D0] &lt;&lt; [2]
</code></pre>
<h4 id="branch"><a class="header" href="#branch">Branch</a></h4>
<ul>
<li>Cause the processor to move execution to a new pointer (jump/GOTO)</li>
<li>Instruction tests the state of the CCR bits against certain condition</li>
<li>Bits set by previous instructions</li>
</ul>
<pre><code>BRA | branch unconditionally
BCC | branch on carry clear
BEQ | branch on equal
</code></pre>
<h4 id="system-control"><a class="header" href="#system-control">System Control</a></h4>
<ul>
<li>Certain instructions used to issue other commands to the microprocessor</li>
</ul>
<h2 id="subroutines-and-stacks"><a class="header" href="#subroutines-and-stacks">Subroutines and Stacks</a></h2>
<ul>
<li>Subroutines are useful for frequently used sections of code for obvious reasons</li>
<li>Can jump and return from subroutines in assembly
<ul>
<li><code>JSR &lt;label&gt;</code> - Jump to Subroutine</li>
<li><code>RTS</code> - Return from Subroutine</li>
</ul>
</li>
<li>When returning, need to know where to return to</li>
<li>The stack is used as a LIFO data structure to store return addresses</li>
<li><code>JSR</code> pushes the contents of the PC on the stack</li>
<li><code>RTS</code> pops the return address from the stack to the PC</li>
<li>Can nest subroutine calls and stack will keep track</li>
</ul>
<h2 id="addressing-modes"><a class="header" href="#addressing-modes">Addressing Modes</a></h2>
<ul>
<li>Addressing modes are how we tell the computer where to find the data it needs</li>
<li>5 Kinds in the 68006, and many other processors have equivalents
<ul>
<li>Direct</li>
<li>Immediate</li>
<li>Absolute</li>
<li>Address Register Indirect
<ul>
<li>5 variations</li>
</ul>
</li>
<li>Relative</li>
</ul>
</li>
</ul>
<h3 id="direct-addressing"><a class="header" href="#direct-addressing">Direct Addressing</a></h3>
<ul>
<li>Probably the simplest</li>
<li>The address of an operand is specified by either a data or address register</li>
</ul>
<pre><code>move D3, D2 | [D2] &lt;- [D3]
move D3, A2 | [A2] &lt;- [D3]
</code></pre>
<h3 id="immediate-addressing"><a class="header" href="#immediate-addressing">Immediate Addressing</a></h3>
<ul>
<li>The operand forms part of the instruction (is a literal) and remains a constant</li>
<li>Note the prefix <code>#</code> specifying a literal and the prefix specifying the base of the number</li>
</ul>
<pre><code>move.b #$42, D5 | [D5] &lt;- $42
</code></pre>
<h3 id="absolute-addressing"><a class="header" href="#absolute-addressing">Absolute Addressing</a></h3>
<ul>
<li>Operand specifies the location in memory</li>
<li>Does not allow for position-independent code: will always access the exact address given</li>
</ul>
<pre><code>move.l D2, $7FFF0 | [MS(7FFF0)] &lt;- [D2]
</code></pre>
<h3 id="address-register-indirect-addressing"><a class="header" href="#address-register-indirect-addressing">Address Register Indirect Addressing</a></h3>
<ul>
<li>Uses offsets/increments/indexing to address memory based upon the address registers</li>
<li>Bad, rarely used</li>
<li><strong>Not examinable</strong></li>
</ul>
<h3 id="relative-addressing"><a class="header" href="#relative-addressing">Relative Addressing</a></h3>
<ul>
<li>Specifies an offset relative to the program counter</li>
<li>Can be used to write position independent code</li>
</ul>
<pre><code>move 16(PC), D3 | [D3] &lt;- [MS(PC + 16)]
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="memory-systems"><a class="header" href="#memory-systems">Memory Systems</a></h1>
<h2 id="the-memory-hierarchy"><a class="header" href="#the-memory-hierarchy">The Memory Hierarchy</a></h2>
<ul>
<li>Memory systems must facilitate the reading and writing of data</li>
<li>Many factors influence the choice of memory technology
<ul>
<li>Frequency of access</li>
<li>Access time</li>
<li>Capacity</li>
<li>Cost</li>
</ul>
</li>
<li>Memory wants to be low cost, high capacity, and also fast</li>
<li>As a tradeoff, we organise memory into a hierarchy
<ul>
<li>Allows for some high speed, some high capacity</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/hierarchy.png" alt="" /></p>
<ul>
<li>Data has to be dragged up the hierarchy</li>
<li>Memory access is somewhat predictable</li>
<li>Temporal locality - when a location accessed, likely the same location will be accessed again in the near future</li>
<li>Spatial locality - when a location accessed, likely that nearby locations will be referenced in the near future
<ul>
<li>90% of memory access is within 2Kb of program counter</li>
</ul>
</li>
</ul>
<h3 id="semiconductor-memory-types"><a class="header" href="#semiconductor-memory-types">Semiconductor Memory Types</a></h3>
<table><thead><tr><th>Memory Type</th><th>Category</th><th>Erasure</th><th>Write Mechanism</th><th>Volatility</th></tr></thead><tbody>
<tr><td>Random Access Memory (RAM)</td><td>Read-Write</td><td>Electronically, at byte-level</td><td>Electronically written</td><td>Volatile</td></tr>
<tr><td>Read Only Memory (ROM)</td><td>Read only</td><td>Not possible</td><td>Mask Written</td><td>Non-volatile</td></tr>
<tr><td>Programmable ROM (PROM)</td><td>Read only</td><td>Not possible</td><td>Electronically written</td><td>Non-volatile</td></tr>
<tr><td>Erasable PROM (EPROM)</td><td>Read (mostly)</td><td>UV light at chip level</td><td>Electronically written</td><td>Non-volatile</td></tr>
<tr><td>Electrically Erasable PROM (EEPROM)</td><td>Read (mostly)</td><td>Electronically, at byte-level</td><td>Electronically written</td><td>Non-volatile</td></tr>
<tr><td>Flash Memory</td><td>Read (mostly)</td><td>Electronically, at byte-level</td><td>Electronically written</td><td>Non-volatile</td></tr>
</tbody></table>
<ul>
<li>Particularly interested in random access</li>
<li>RAM is most common - implements main store
<ul>
<li>nb that all types shown here allow random access, name is slightly misleading</li>
</ul>
</li>
<li>RAM is also volatile, meaning it is erased when de powered</li>
</ul>
<h2 id="cache"><a class="header" href="#cache">Cache</a></h2>
<ul>
<li>If 90% of memory access is within 2Kb, store those 2Kb somewhere fast</li>
<li>Cache is small, fast memory right next to CPU</li>
<li>10-200 times faster</li>
<li>If data requested is found in cache, this is a &quot;cache hit&quot; and provides a big speed improvement</li>
<li><strong>We want things to be in cache</strong></li>
<li>Cache speed/size is often a bigger bottleneck to performance than clock speed</li>
</ul>
<h3 id="moores-law"><a class="header" href="#moores-law">Moore's Law</a></h3>
<ul>
<li>As said by the co-founder of intel, Gordon Moore, the number of transistors on a chip will double roughly every 18 months
<ul>
<li>Less true in recent years</li>
</ul>
</li>
<li>Cost of computer logic and circuitry has fallen dramatically in the last 30 years</li>
<li>ICs become more densely paced</li>
<li>CPU clock speed is also increasing at a similar rate</li>
<li>Memory access speed is improving much more slowly however</li>
</ul>
<h3 id="cache-concepts"><a class="header" href="#cache-concepts">Cache Concepts</a></h3>
<ul>
<li>Caching read-only data is relatively straightforward
<ul>
<li>Don't need to consider the possibility data will change</li>
<li>Copies everywhere in the memory hierarchy remain consistent</li>
</ul>
</li>
<li>When caching mutable data, copies can become different between cache/memory</li>
<li>Two strategies for maintaining parity
<ul>
<li><strong>Write through</strong> - updates cache and then writes through to update lower levels of hierarchy</li>
<li><strong>Write back</strong> - only update cache, then when memory is replaced copy blocks back from cache</li>
</ul>
</li>
</ul>
<h3 id="cache-performance"><a class="header" href="#cache-performance">Cache Performance</a></h3>
<p>Cache performance is generally measured by its <em>hit rate</em>. If the processor requests some block of memory and it is already in cache, this is a hit. The hit rate is calculated as</p>
<p>$$h = \frac{\text{total number of cache hits}}{\text{total number of memory accesses}}$$</p>
<p>Cache misses can be categorised:</p>
<ul>
<li><strong>Compulsory</strong> - misses that would occur regardless of cache size, eg the first time a block is accessed, it will not be in cache</li>
<li><strong>Capacity</strong> - misses that occur because cache is not large enough to contain all blocks needed during program execution</li>
<li><strong>Conflict</strong> - misses that occur as a result of the placement strategy for blocks not being fully associative, meaning a block may have to be discarded and retrieved</li>
<li><strong>Coherency</strong> - misses that occur due to cache flushes in multiprocessor systems</li>
</ul>
<p>Measuring performance solely based upon cache misses is not accurate as it does not take into factor the cost of a cache miss. Average memory access time is measured as hit time + (miss rate $\times$ miss penalty).</p>
<h3 id="cache-levels"><a class="header" href="#cache-levels">Cache Levels</a></h3>
<p>Cache has multiple levels to provide a tradeoff between speed and size.</p>
<ul>
<li>Level 1 cache is the fastest as it is the closest to the cpu, but is typically smallest
<ul>
<li>Sometimes has separate instructions/data cache</li>
</ul>
</li>
<li>Level 2 cache is further but larger</li>
<li>Level 3 cache is slowest (but still very fast) but much larger (a few megabytes)</li>
<li>Some CPUs even have a level 4 cache</li>
</ul>
<p>Different levels of cache exist as part of the memory hierarchy.</p>
<h2 id="semiconductors"><a class="header" href="#semiconductors">Semiconductors</a></h2>
<ul>
<li>RAM memory used to implement main store</li>
<li>Static RAM (SRAM) uses a flip-flop as the storage element for each bit
<ul>
<li>Uses a configuration of flip-flops and logic gates</li>
<li>Hold data as long as power is supplied</li>
<li>Provide faster read/write than DRAM</li>
<li>Typically used for cache</li>
<li>More expensive</li>
</ul>
</li>
<li>Dynamic RAM (DRAM) uses a capacitor, and the presence to denote a bit
<ul>
<li>Typically simpler design</li>
<li>Can be packed much tighter</li>
<li>Cheaper to produce</li>
<li>Capacitor charge decays so needs refreshing by periodically supplying charge</li>
</ul>
</li>
<li>The interface to main memory is a critical performance bottleneck</li>
</ul>
<h2 id="memory-organisation"><a class="header" href="#memory-organisation">Memory Organisation</a></h2>
<p>The basic element of memory is a one-bit cell with two states, capable of being read and written. Cells are built up into larger banks with combinatorial logic circuits to select which cell to read/write. The diagram shows an example of a 16x8 memory IC (16 words of 8 bytes).</p>
<p><img src="cs132/./img/memory.png" alt="" /></p>
<p>For a 16x8 memory cell:</p>
<ul>
<li>4 address inputs
<ul>
<li>$\log_2 ,16$</li>
</ul>
</li>
<li>8 data lines
<ul>
<li>word size</li>
</ul>
</li>
</ul>
<p>Consider alternatively a 1Kbit device with 1024 cells</p>
<ul>
<li>Organised as a 128x8 array
<ul>
<li>7 address pins</li>
<li>8 data pins</li>
</ul>
</li>
<li>Or, could organise as 1024x1 array
<ul>
<li>10 address pins</li>
<li>1 data pins</li>
</ul>
</li>
<li>Less pins but very poorly organised</li>
<li>Best to keep memory cells square to make efficient use of space</li>
</ul>
<h2 id="error-correction"><a class="header" href="#error-correction">Error Correction</a></h2>
<p>Errors often occur within computer systems in the transmission of data dude to noise and interference. This is bad. Digital logic already gives a high degree of immunity to noise, but when noise is at a high enough level, this collapses.</p>
<p>Two common ways in which errors can occur:</p>
<ul>
<li>Isolated errors
<ul>
<li>Occur at random due to noise</li>
<li>Usually singular incidences</li>
</ul>
</li>
<li>Burst errors
<ul>
<li>Errors usually occur in bursts</li>
<li>A short period of time over which multiple errors occur</li>
<li>For example, a 1ms dropout of a connection can error many bits</li>
</ul>
</li>
</ul>
<h3 id="majority-voting"><a class="header" href="#majority-voting">Majority Voting</a></h3>
<ul>
<li>A simple solution to correcting errors</li>
<li>Just send every bit multiple times (usually 3)
<ul>
<li>The one that occurs the most is taken to be the true value</li>
</ul>
</li>
<li>Slow &amp; expensive</li>
</ul>
<h3 id="parity"><a class="header" href="#parity">Parity</a></h3>
<ul>
<li>Parity adds an extra <em>parity bit</em> to each byte</li>
<li>Two types of parity system
<ul>
<li>Even parity
<ul>
<li>The value of the extra bit is chosen to make the total number of 1s an even number</li>
</ul>
</li>
<li>Odd parity
<ul>
<li>The value of the extra bit is chosen to make the total number of 1s an odd number</li>
</ul>
</li>
</ul>
</li>
<li>7 bit ascii for <code>A</code> is <code>0100 0001</code>
<ul>
<li>With even parity - <code>0100 0001</code></li>
<li>Odd parity - <code>1100 0001</code></li>
</ul>
</li>
<li>Can be easily computed in software</li>
<li>Can also be computed in hardware using a combination of XOR gates
<ul>
<li>Usually faster than in software</li>
</ul>
</li>
<li>Allows for easy error detection without the need to significantly change the model for communication</li>
<li>Parity bit is computed and added before data is sent, parity is checked when data is received</li>
<li>Note that if there is more than one error, the parity bit will be correct still and the error won't be detected
<ul>
<li>Inadequate for detecting bursts of error</li>
</ul>
</li>
</ul>
<h3 id="error-correcting-codes"><a class="header" href="#error-correcting-codes">Error Correcting Codes</a></h3>
<ul>
<li>ECCs or checksums are values computed from the entire data</li>
<li>If any of the data changes, the checksum will also change</li>
<li>The checksum is calculated and broadcast with the data so it can be checked on reception</li>
<li>Can use row/column parity to compute an checksum
<ul>
<li>Calculate parity of each row and of each column</li>
<li>Diagram shows how parity bits detect an error in the word &quot;Message&quot;</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/parity.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="io"><a class="header" href="#io">I/O</a></h1>
<h2 id="memory-mapped-io"><a class="header" href="#memory-mapped-io">Memory Mapped I/O</a></h2>
<ul>
<li>With memory mapped I/O, the address bus is used to address both memory and I/O devices</li>
<li>Memory on I/O devices is mapped to values in the main address space</li>
<li>When a CPU accesses a memory address, the address may be in physical memory (RAM), or the memory of some I/O device</li>
<li>Advantages
<ul>
<li>Very simple</li>
<li>CPU requires less internal logic</li>
<li>Can use general purpose memory instructions for I/O</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Have to give up some memory
<ul>
<li>Less of a concern on 64-bit processors</li>
<li>Still relevant in smaller 16 bit CPUs</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="polled-io"><a class="header" href="#polled-io">Polled I/O</a></h2>
<ul>
<li>Polling is a technique for synchronising communication between devices.</li>
<li>Most I/O devices are much slower than the CPU</li>
<li>Busy-wait polling involves constantly checking the state of the device
<ul>
<li>Usually the device replies with nothing</li>
<li>Can interleave polls with something else useful</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/polling.png" alt="" /></p>
<ul>
<li>Advantages
<ul>
<li>Still relatively simple</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Wastes CPU time and power</li>
<li>Interleaving can lead to delayed responses from CPU</li>
</ul>
</li>
</ul>
<p>Synchronisation methods also need some way to transfer the data, so are sometimes used in conjunction with memory-mapped I/O. Methods for synchronising devices and methods for reading/writing data are not directly comparable.</p>
<h2 id="handshaking"><a class="header" href="#handshaking">Handshaking</a></h2>
<p>Another form of synchronisation</p>
<p><img src="cs132/./img/handshake.png" alt="" /></p>
<ul>
<li>Computer responds to the printer being ready by placing data on the data bus and signalling <code>DATA_VALID</code>
<ul>
<li>Can do this either in hardware or in software</li>
</ul>
</li>
<li>Timing diagram shows data exchange</li>
<li>During periods where both signals are at a logical 0, data is exchanged</li>
</ul>
<p><img src="cs132/./img/timing_handshake.png" alt="" /></p>
<h3 id="handshaking-hardware"><a class="header" href="#handshaking-hardware">Handshaking Hardware</a></h3>
<p>Handshaking is usually done using an external chip, such as the 6522 VIA (Versatile Interface Adapter)</p>
<p><img src="cs132/./img/6522.png" alt="" /></p>
<p>Setting bit values in the PCR (Peripheral Control Register) on the VIA allows to control the function.</p>
<ul>
<li>Use PORT B as output</li>
<li>CB1 control line as <code>PRINTER_READY</code></li>
<li>CB2 control line as <code>DATA_VALID</code></li>
<li>For CB1 and CB2 control, 8 bit register is set to 1000xxxx
<ul>
<li>Last 4 bits not used, don't care</li>
</ul>
</li>
</ul>
<h2 id="interrupts"><a class="header" href="#interrupts">Interrupts</a></h2>
<ul>
<li>Asynchronous I/O</li>
<li>Two kinds of interrupts (in 6502 processor)
<ul>
<li>Interrupt Request (IRQ)
<ul>
<li>Code can disable response</li>
<li>Sent with a priority</li>
<li>If priority lower than that of current task, will be ignored</li>
<li>Can become non-maskable if ignored for long enough</li>
</ul>
</li>
<li>Non-Maskable Interrupt (NMI)
<ul>
<li>Cannot be disabled, must be serviced</li>
</ul>
</li>
</ul>
</li>
<li>An interrupt forces the CPU to jump to an Interrupt Service Routine (ISR)
<ul>
<li>Switches context, uses stack to store state of registers</li>
</ul>
</li>
<li>ISRs can be nested</li>
<li>Interrupts usually generated by some external device
<ul>
<li>Hard drive can generate an interrupt when data is ready</li>
<li>A timer can generate an interrupt repeatedly at a fixed interval</li>
<li>A printer can generate an interrupt when ready to receive data</li>
</ul>
</li>
<li>Advantages
<ul>
<li>Fast response</li>
<li>No wasted CPU time</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>All data transfer still CPU controlled</li>
<li>More complex hardware/software</li>
</ul>
</li>
</ul>
<h2 id="direct-memory-access-dma"><a class="header" href="#direct-memory-access-dma">Direct Memory Access (DMA)</a></h2>
<ul>
<li>The CPU is a bottleneck for I/O</li>
<li>All techniques shown so far are limited by this bottleneck</li>
<li>DMA is used where large amounts of data must be transferred quickly</li>
<li>Control of system busses surrendered from CPU to a DMA Controller (DMAC)
<ul>
<li>DMAC is a dedicated device optimised for data transfer</li>
</ul>
</li>
<li>Can be up to 10x faster than CPU-driven I/O</li>
</ul>
<h3 id="dma-operation"><a class="header" href="#dma-operation">DMA Operation</a></h3>
<p><img src="cs132/./img/DMA.png" alt="" /></p>
<ul>
<li>DMA transfer is requested by I/O</li>
<li>DMAC passes request to CPU</li>
<li>CPU initialises DMAC
<ul>
<li>Input or Output?</li>
<li>Start address is put into DMAC address register</li>
<li>Number of words is put into DMAC count register</li>
<li>CPU enables DMAC</li>
</ul>
</li>
<li>DMAC requests use of system busses</li>
<li>CPU responds with DMAC ack when ready to surrender busses</li>
<li>DMAC can operate in different modes
<ul>
<li>Cycle stealing
<ul>
<li>Uses system busses when they're not being used by CPU</li>
</ul>
</li>
<li>Burst mode
<ul>
<li>Requires busses for extended period of time, locks the CPU out for a fixed time, until transfer complete, or until CPU receives interrupt from device of higher priority</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="dma-organisation"><a class="header" href="#dma-organisation">DMA Organisation</a></h3>
<p>There are multiple ways a DMA can be incorporated into a system:</p>
<ul>
<li>Single bus, detached DMA
<ul>
<li>All modules (DMA, I/O devices, memory, CPU) share system bus</li>
<li>DMA uses programmed I/O to exchanged data between memory and I/O device</li>
<li>Straightforward, as DMA can just mimic processor</li>
<li>Inefficient</li>
</ul>
</li>
<li>Separate I/O bus
<ul>
<li>Only one interface to DMA module</li>
<li>The bus the DMA shares with processor and memory is only used to transfer data to and from memory</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/dma-organisation.png" alt="" /></p>
<h2 id="summary"><a class="header" href="#summary">Summary</a></h2>
<ul>
<li>**Memory-mapped **deviced are accessed in the same way as RAM, at fixed address locations</li>
<li><strong>Polled I/O</strong> is for scheduling input and output, where the CPU repeatedly checks for data</li>
<li>I/O devices are slow, so <strong>handshaking techniques</strong> coordinate CPU and device for transfer of data</li>
<li><strong>Interrupts</strong> avoid polled I/O by diverting the CPU to a special I/O routine when necessary</li>
<li>A <strong>DMA</strong> controller can be used instead of the CPU to transfer data into and out of memory, faster than the CPU but at additional hardware cost</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="microprocessor-architecture"><a class="header" href="#microprocessor-architecture">Microprocessor Architecture</a></h1>
<ul>
<li>Computer architecture concerns the structure and properties of a computer system, from the perspective of a software engineer</li>
<li>Computer organisation concerns the structure and properties of a computer system, from the perspective of a hardware engineer</li>
</ul>
<h2 id="the-patp"><a class="header" href="#the-patp">The PATP</a></h2>
<p>The Pedagogically Advanced Teaching Processor is a very simple microprocessor. The specifics of it are <strong>not examinable</strong>, but it is used to build an understanding of microprocessor architecture.</p>
<h3 id="programmers-model"><a class="header" href="#programmers-model">Programmer's model</a></h3>
<p>The PATP has 8 instructions. Each instruction is 1 8-bit word, with the first 3 bits as the opcode and last 5 as the operand, if applicable.</p>
<table><thead><tr><th>Opcode</th><th>Mnemonic</th><th>Macro Operation</th><th>Description</th></tr></thead><tbody>
<tr><td>000</td><td><code>CLEAR</code></td><td><code>[D0] &lt;- 0</code></td><td>Set D0 to 0 (and set <code>Z</code>)</td></tr>
<tr><td>001</td><td><code>INC</code></td><td><code>[D0] &lt;- [D0] + 1</code></td><td>Increment the value in D0 (and set <code>Z</code> if result is 0)</td></tr>
<tr><td>010</td><td><code>ADD #v</code></td><td><code>[D0] &lt;- [D0] + v</code></td><td>Add the literal v to D0 (and set <code>Z</code> if result is 0)</td></tr>
<tr><td>011</td><td><code>DEC</code></td><td><code>[D0] &lt;- [D0] - 1</code></td><td>Decrement the value in D0 (and set <code>Z</code> if result is 0)</td></tr>
<tr><td>100</td><td><code>JMP loc</code></td><td><code>[PC] &lt;- loc</code></td><td>Jump unconditionally to address location <code>loc</code></td></tr>
<tr><td>101</td><td><code>BNZ loc</code></td><td>If <code>Z</code> is not 0 then <code>[PC] &lt;- loc</code></td><td>Jump to address location <code>loc</code> if <code>Z</code> is not set</td></tr>
<tr><td>110</td><td><code>LOAD loc</code></td><td><code>[DO] &lt;- [MS(loc)]</code></td><td>Load the 8 bit value from address location <code>loc</code> to D0</td></tr>
<tr><td>111</td><td><code>STORE loc</code></td><td><code>[MS(loc)] &lt;- [D0]</code></td><td>Write the 8 bit value from D0 to address location <code>loc</code></td></tr>
</tbody></table>
<p>This is not many instructions, but it is technically Turing-complete. The other specs of the PATP are:</p>
<ul>
<li>An address space of 32 bytes (the maximum address is <code>11111</code>)</li>
<li>A single 8-bit data register/accumulator <code>D0</code></li>
<li>A CCR with only 1 bit (<code>Z</code>, set when an arithmetic operation has a result of zero)</li>
<li>A 5-bit program counter (only 5 bits needed to address whole memory)</li>
</ul>
<h3 id="internal-organisation"><a class="header" href="#internal-organisation">Internal Organisation</a></h3>
<p>There are several building blocks that make up the internals of the PATP:</p>
<p><img src="cs132/./img/D0.png" alt="" /></p>
<ul>
<li>The data register <code>D0</code>
<ul>
<li>An 8 bit register constructed from D-type flip-flops</li>
<li>Has parallel input and output</li>
<li>Clocked</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/ALU.png" alt="" /></p>
<ul>
<li>The ALU
<ul>
<li>Built around an <a href="cs132/./logic.html#n-bit-addersubtractor">8-bit adder/subtractor</a></li>
<li>Has two 8-bit inputs <code>P</code> and <code>Q</code></li>
<li>Capable of
<ul>
<li>Increment (+1)</li>
<li>Decrement (-1)</li>
<li>Addition (+n)</li>
</ul>
</li>
<li>Two function select inputs <code>F1</code> and <code>F2</code> which choose the operation to perform
<ul>
<li>00: Zero output</li>
<li>01: Q + 1</li>
<li>10: Q + P</li>
<li>11: Q - 1</li>
</ul>
</li>
<li>An output <code>F(P, Q)</code> which outputs the result of the operation</li>
<li>A <code>Z</code> output for the CCR</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/bus.png" alt="" /></p>
<ul>
<li>The main system bus
<ul>
<li>Uses 3-state buffers to enable communication</li>
</ul>
</li>
</ul>
<p><img src="cs132/./img/CU.png" alt="" /></p>
<ul>
<li>The control unit
<ul>
<li>Controls:
<ul>
<li>The busses (enables)</li>
<li>When registers are clocked</li>
<li>ALU operation</li>
<li>Memory acccess</li>
</ul>
</li>
<li>Responsible for decoding instructions and issuing micro-instructions</li>
<li>Inputs
<ul>
<li>Opcode</li>
<li>Clock</li>
<li><code>Z</code> register</li>
</ul>
</li>
<li>Outputs
<ul>
<li>Enables
<ul>
<li>Main store</li>
<li>Instruction register <code>IR</code></li>
<li>Program counter</li>
<li>Data register <code>D0</code></li>
<li>ALU register</li>
</ul>
</li>
<li>Clocks
<ul>
<li>Memory address register <code>MAR</code></li>
<li>Instruction register <code>IR</code></li>
<li>Program counter</li>
<li>Data register <code>D0</code></li>
<li>ALU register</li>
</ul>
</li>
<li><code>F1</code> and <code>F2</code> on the ALU</li>
<li><code>R</code>/<code>W</code> to control bit for main store</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>All the components come together like so:</p>
<p><img src="cs132/./img/PATP.png" alt="" /></p>
<h2 id="micro-and-macro-instructions"><a class="header" href="#micro-and-macro-instructions">Micro and Macro Instructions</a></h2>
<p>There are several steps internally that are required to execute a single instruction. For example, to execute an <code>INC</code> operation:</p>
<ul>
<li>D0 need to be put on the system bus
<ul>
<li>CU enables the three-state buffer for D0</li>
<li><code>[ALU(Q)] &lt;- D0</code></li>
</ul>
</li>
<li>The correct ALU function must be selected
<ul>
<li><code>F1 = 0</code>, <code>F2 = 1</code></li>
<li>Signals asserted by CU</li>
<li><code>[ALU(F)] &lt;- 01</code></li>
</ul>
</li>
<li>The output from the ALU must be read into the ALU register
<ul>
<li>ALUreg clocked by CU</li>
<li><code>[ALUreg] &lt;- [ALU]</code></li>
</ul>
</li>
<li>D0 reads in the ALU output from the ALU register
<ul>
<li>CU enables the three-state buffer for ALUreg</li>
<li>D0 is clocked by CU</li>
</ul>
</li>
</ul>
<p>Macro instructions are the assembly instructions issued to the processor (to the CU, specifically), but micro instructions provide a low level overview of how data is moved around between internals of the CPU and what signals are asserted internally. The PATP can execute all instructions in 2 cycles. The table below gives an overview of the micro operations required for each macro instruction, along with the macro operations for fetching from main store.</p>
<p><img src="cs132/./img/mu-ops.png" alt="" /></p>
<h3 id="control-signals"><a class="header" href="#control-signals">Control Signals</a></h3>
<p>The control unit asserts control signals at each step of execution, and the assertion of these control signals determine how data moves internally. For the PATP:</p>
<ul>
<li>Enable signals are level-triggered</li>
<li>Clock signals are falling edge-triggered</li>
<li>An output can be enabled onto the main bus and then clocked elsewhere in a single time step</li>
<li>ALU timings assume that, if values are enabled at P and Q at the start of a cycle, then the ALU register can be clocked on the falling edge of that cycle</li>
<li>MS timings assume that if MAR is loaded during one cycle, then R, W and EMS can be used in the next cycle</li>
</ul>
<p>The diagram below shows the timing for a fetch taking 4 cycles, and which components are signalled when. Notice which things happen in the same cycle, and which must happen sequentially.</p>
<table><thead><tr><th>cycle</th><th>Micro-Op</th><th>Control Signals</th></tr></thead><tbody>
<tr><td>1</td><td><code>[MAR] &lt;- [PC]</code></td><td>Enable PC, Clock MAR</td></tr>
<tr><td>2</td><td><code>[IR] &lt;- [MS(MAR)]</code></td><td>Set read for MAR, Enable MS, Clock IR</td></tr>
<tr><td>3</td><td><code>[ALU(Q)] &lt;- [PC]</code></td><td>Enable PC</td></tr>
<tr><td>3</td><td><code>[ALU(F) &lt;- 01]</code></td><td>F1 = 0, F2 = 1</td></tr>
<tr><td>3</td><td><code>[ALUreg] &lt;- [ALU]</code></td><td>Clock ALUreg</td></tr>
<tr><td>4</td><td><code>[PC] &lt;- [ALUreg]</code></td><td>Enable ALUreg, Clock PC</td></tr>
</tbody></table>
<p><img src="cs132/./img/timings.png" alt="" /></p>
<h2 id="control-unit-design"><a class="header" href="#control-unit-design">Control Unit Design</a></h2>
<p>The task of the control unit is to coordinate the actions of the CPU, namely the Fetch-Decode-Execute cycle. It generates the fetch control sequence, takes opcode input, and generates the right control sequence based on this. It can be designed to do this in one of two ways:</p>
<ul>
<li>Hardwired design (sometimes called &quot;random logic&quot;)
<ul>
<li>The CU is a combinatorial logic circuit, transforming input directly to output</li>
</ul>
</li>
<li>Microprogrammed
<ul>
<li>Each opcode is turned into a sequence of microinstructions, which form a microprogram</li>
<li>Microprograms stored in ROM called microprogram memory</li>
</ul>
</li>
</ul>
<h3 id="hardwired"><a class="header" href="#hardwired">Hardwired</a></h3>
<ul>
<li>A sequencer is used to sequence the clock cycles
<ul>
<li>Has clock input and n outputs <code>T1 ... Tn</code></li>
<li>First clock pulse is output from <code>T1</code></li>
<li>Second is output from <code>T2</code></li>
<li>Clock pulse n output from <code>Tn</code></li>
<li>Pulse n+1 output from <code>T1</code></li>
</ul>
</li>
<li>This aligns the operation of the circuit with the control steps</li>
<li>Advantages
<ul>
<li>Fast</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Complex, difficult to design and test</li>
<li>Inflexible, cant change design to add new instructions</li>
<li>Takes a long time to design</li>
</ul>
</li>
<li>This technique is most commonly used in RISC processors and has been since the 80s</li>
</ul>
<p><img src="cs132/./img/hardwired-CU.png" alt="" /></p>
<ul>
<li>The control signal generator maps each instruction to outputs</li>
<li>The sequencer sequences the outputs appropriately</li>
<li>The flip-flop is used to regulate control rounds</li>
</ul>
<h3 id="microprogrammed"><a class="header" href="#microprogrammed">Microprogrammed</a></h3>
<ul>
<li>The microprogram memory stores the required control actions for each opcode</li>
<li>The CU basically acts as a mini CPU within the CPU
<ul>
<li><strong>Microaddress</strong> is a location within microprogram memory</li>
<li><strong>MicroPC</strong> is the CU's internal program counter</li>
<li><strong>MicroIR</strong> is the CU's internal microinstruction register</li>
</ul>
</li>
<li>The microPC can be used in different ways depending upon implementation
<ul>
<li>Holds the next microaddress</li>
<li>Holds the microaddress of microroutine for next opcode</li>
</ul>
</li>
<li>When powered initially holds microaddress 0
<ul>
<li>The fetch microprogram</li>
</ul>
</li>
<li>Each microinstruction sets the CU outputs to the values dictated the instruction
<ul>
<li>As the microprogram executes, the CU generates control signals</li>
</ul>
</li>
<li>After each microinstruction, the microPC is typically incremented, so microinstructions are stepped through in sequence</li>
<li>After a fetch, the microPC is not incremented, but is set to the output from the opcode decoding circuit (labelled OTOA in the diagram)</li>
<li>After a normal opcode microprogram, the microPC is set back to 0 (fetch)</li>
<li>When executing the microprogram for a conditional branch instruction, the microPC value is generated based upon whether the CU's Z input is set</li>
</ul>
<p><img src="cs132/./img/micro-CU.png" alt="" /></p>
<ul>
<li>Advantages
<ul>
<li>Easy to design and implement</li>
<li>Flexible design</li>
<li>Simple hardware compared to alternative</li>
<li>Can be reprogrammed for new instructions</li>
</ul>
</li>
<li>Disadvantages
<ul>
<li>Slower than hardwired</li>
</ul>
</li>
<li>Most commonly used for CISC processors</li>
</ul>
<h2 id="risc-and-cisc"><a class="header" href="#risc-and-cisc">RISC and CISC</a></h2>
<p>In the late 70s-early 80s, it was shown that certain instructions are used far more than others:</p>
<ul>
<li>45% data movement (move, store, load)</li>
<li>29% control flow (branch, call, return)</li>
<li>11% arithmetic (add, sub)</li>
</ul>
<p>The overhead from using a microprogram memory also became more significant as the rest of the processor became faster. This caused a shift towards RISC computing. Right now, ARM is the largest RISC computing platform. Intel serve more for backwards compatibility with a CISC instruction set. In an modern intel processor, simplest instructions are executed by a RISC core, more complex ones are microprogrammed.</p>
<ul>
<li>RISC has simple, standard instructions whereas CISC has lots of more complex instructions
<ul>
<li>x86 is often criticised as bloated</li>
</ul>
</li>
<li>RISC allows for simpler, faster, more streamlined design</li>
<li>RISC instructions aim to be executed in a single cycle</li>
<li>CISC puts the focus on the hardware doing as much as possible, whereas RISC makes the software do the work</li>
</ul>
<h2 id="multicore-systems"><a class="header" href="#multicore-systems">Multicore Systems</a></h2>
<ul>
<li>The performance of a processor can be considered as the rate at which it executes instructions: clock speed x IPC (instructions per clock).</li>
<li>To increase performance, increase clock speed and/or IPC</li>
<li>An alternative way of increasing performance is parallel execution</li>
<li>Multithreading separates the instruction stream into threads that can execute in parallel</li>
<li>A <strong>process</strong> is an instance of a program running on a computer
<ul>
<li>A process has <strong>ownership</strong> of resources: the program's virtual address space, i/o devices, other data that defines the process</li>
<li>The process is <strong>scheduled</strong> by the OS to divide the execution time of the processor between threads</li>
<li>The processor <strong>switches</strong> between processes using the stack</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cs141"><a class="header" href="#cs141">CS141</a></h1>
<h2 id="a-hrefhttpswwwyoutubecomwatchvfyfhn_0qhfqnotaculta"><a class="header" href="#a-hrefhttpswwwyoutubecomwatchvfyfhn_0qhfqnotaculta"><a href="https://www.youtube.com/watch?v=FYFhN_0QhfQ">#notacult</a></a></h2>
<div style="break-before: page; page-break-before: always;"></div><h1 id="types--typeclasses"><a class="header" href="#types--typeclasses">Types &amp; Typeclasses</a></h1>
<p>Haskell is a strongly, statically typed programming language, which helps prevent us from writing bad programs.</p>
<ul>
<li>Java, C, Rust - strongly typed</li>
<li>Python, Ruby - dynamically typed</li>
</ul>
<p>Types have many benefits:</p>
<ul>
<li>Describe the value of an expression</li>
<li>Prevent us from doing silly things
<ul>
<li><code>not 7</code> gives <code>Type Error</code></li>
</ul>
</li>
<li>Good for documentation</li>
<li>Type errors occur at compile time</li>
</ul>
<p>GHC checks types and infers the type of expressions for us. Types are discarded after type checking, and are not available at runtime.</p>
<h2 id="type-notation"><a class="header" href="#type-notation">Type notation</a></h2>
<p>We say an expression has a type by writing <code>expression :: type</code>, read as &quot;expression has type&quot;.</p>
<ul>
<li>If we can assign a type to an expression, it is &quot;well typed&quot;</li>
<li>A type approximates and describes the value of an expression.</li>
</ul>
<pre><code class="language-haskell">42 :: Int
True :: Bool
'c' :: Char
&quot;Cake&quot; :: String
0.5 :: Double
4 + 8 :: Int
2 * 9 + 3 :: Int
True &amp;&amp; False :: Bool
&quot;AB&quot; ++ &quot;CD&quot; :: String
even 9 :: Bool
</code></pre>
<p>Before writing a definition, it is good practice to write its type.</p>
<pre><code class="language-haskell">daysPerWeek :: Int
daysperWeek = 7
</code></pre>
<h3 id="function-types"><a class="header" href="#function-types">Function Types</a></h3>
<p>The types of functions are denoted using arrows <code>-&gt;</code>. The <code>not</code> function is defined as <code>not :: Bool -&gt; Bool</code>, read &quot;not has type bool to bool&quot;. It means if you give me a <code>Bool</code>, I will give you back another <code>Bool</code>.</p>
<p>The definition of the <code>not</code> function is shown below.</p>
<pre><code class="language-haskell">not :: Bool -&gt; Bool
not True = False
not False = True
not True :: Bool
</code></pre>
<p>The last line shows how function application eliminates function types, as by applying a function to a value, one of the types from the function definition is removed as it has already been applied.</p>
<p>The <code>xor</code> function takes two boolean arguments and is defined:</p>
<pre><code class="language-haskell">xor :: Bool -&gt; Bool -&gt; Bool
xor False True = True
xor False False = False
xor True True = False
xor True False = True
</code></pre>
<p>Applying one argument to a function that takes two is called <em>partial function application</em>, as it partially applies arguments to a function to return another function. This is because all functions in haskell are <em>curried</em>, meaning all functions actually only take one argument, and functions taking more than one argument are constructed from applying multiple functions with one argument.</p>
<pre><code class="language-haskell">xor :: Bool -&gt; Bool -&gt; Bool
xor True :: Bool -&gt; Bool -- partially applied function
xor True False :: Bool
</code></pre>
<h2 id="polymorphic-types"><a class="header" href="#polymorphic-types">Polymorphic Types</a></h2>
<p>What is the type of <code>\x -&gt; x</code> ? Could be:</p>
<pre><code class="language-haskell">f :: Int -&gt; Int
f :: Bool -&gt; Bool
f :: Char -&gt; Char
</code></pre>
<p>These are all permissible types. To save redifining a function, we can use type variables. Anything with a single lowercase character is a type variable (<code>a</code> in this case).</p>
<pre><code class="language-haskell">\x -&gt; x :: a -&gt; a
</code></pre>
<p><code>\x -&gt; x</code> is the identity function, as it returns its argument unchanged. We can also have functions with more than one type variable, to specify that arguments have different types:</p>
<pre><code class="language-haskell">const :: a -&gt; b -&gt; a
const x y = x
</code></pre>
<h2 id="tuples"><a class="header" href="#tuples">Tuples</a></h2>
<p>Tuples are a useful data structure</p>
<pre><code class="language-haskell">(4, 7) :: (Int, Int)
(4, 7.0) :: (Int, Double)
('a', 9, &quot;Hello&quot;) :: (Char, Int, String)

--can nest tuples
((4, 'g'), False) :: ((Int, Char), Bool)

--can also contain functions
(\x -&gt; x, 8.15) :: (a-&gt;a, Double)
</code></pre>
<p>Functions on pairs. These are all in the standard library</p>
<pre><code class="language-haskell">fst :: (a,b) -&gt; a
snd :: (a,b) -&gt; b
swap :: (a,b) -&gt; (b,a)

-- these functions can also be defined by pattern matching
fst (x,y) = x
snd (x,y) = y
swap (x,y) = (y,x)
</code></pre>
<h2 id="type-classes"><a class="header" href="#type-classes">Type Classes</a></h2>
<p>Type classes are used for restricting polymorphism and overloading functions.</p>
<ul>
<li>The <code>(+)</code> operator probably has type <code>(+) :: Int -&gt; Int -&gt; Int</code>,
<ul>
<li>This is correct, as this typing is permissible</li>
</ul>
</li>
<li>What about <code>1.2 + 3.4</code>?
<ul>
<li>Will raise an error with this definition of <code>(+)</code></li>
</ul>
</li>
<li>Can polymorphism help?</li>
<li><code>(+) :: a -&gt; a -&gt; a</code>
<ul>
<li>This is stupid</li>
<li>Allows any types</li>
<li>Won't work</li>
</ul>
</li>
<li>A <em>type class constraint</em> is needed</li>
<li>The actual type is <code>(+) :: Num a =&gt; a -&gt; a -&gt; a</code>
<ul>
<li>The <code>Num a =&gt;</code> part is the constraint part</li>
<li>Tells the compiler that <code>a</code> has to belong to the typeclass <code>Num</code></li>
</ul>
</li>
<li>Type class constraints are used to constrain type variables to only types which support the functions or operators specified by the type class</li>
<li>Type class names start with an uppercase character</li>
<li><code>Num</code> is a type class that represents all types which support arithmetic operations</li>
</ul>
<h3 id="defining-type-classes"><a class="header" href="#defining-type-classes">Defining Type Classes</a></h3>
<p>A type class is defined as follows:</p>
<pre><code class="language-haskell">class Num a where
    (+) :: a -&gt; a -&gt; a
    (-) :: a -&gt; a -&gt; a
    abs :: a -&gt; a
</code></pre>
<ul>
<li><code>Num</code> is the name of the type class</li>
<li><code>a</code> is the type variable representing it in the method typings</li>
<li>The type class contains method signatures for all functions that members of the type class must implement</li>
</ul>
<p>The type class contains type definitions, but no implementations for the functions. To implement them, we need to tell the compiler which types implement the type class and <em>how</em> they implement the functions in the type class. The <code>Show</code> typeclass tells the compiler that a type can be converted to a string.</p>
<pre><code class="language-haskell">-- typeclass definition
class Show a where
    show :: a -&gt; String

-- instance of typeclass for bool type
instance Show Bool where
    show True = &quot;True&quot;
    show False = &quot;False&quot;
</code></pre>
<p>The <code>instance</code> definition tells the compiler that <code>Bool</code> is a member of <code>Show</code>, and how it implements the functions that <code>Show</code> defines.</p>
<h3 id="prelude-type-classes"><a class="header" href="#prelude-type-classes">Prelude Type Classes</a></h3>
<ul>
<li><code>Num</code> for numbers</li>
<li><code>Eq</code> for equality operators <code>==</code> <code>/=</code></li>
<li><code>Ord</code> for inequality/comparison operators <code>&gt;</code> <code>&lt;=</code> etc</li>
<li><code>Show</code> for converting things to string</li>
<li>Many More</li>
</ul>
<p>The REPL makes extensive use of <code>Show</code> to print things. There are no show instances for function types, so you get an error if you try to <code>Show</code> functions. Typing <code>:i</code> in the REPL gets info on a type class. <code>:i Num</code> gives:</p>
<pre><code class="language-haskell">class Num a where
  (+) :: a -&gt; a -&gt; a
  (-) :: a -&gt; a -&gt; a
  (*) :: a -&gt; a -&gt; a
  negate :: a -&gt; a
  abs :: a -&gt; a
  signum :: a -&gt; a
  fromInteger :: Integer -&gt; a
  {-# MINIMAL (+), (*), abs, signum, fromInteger, (negate | (-)) #-}
        -- Defined in ‘GHC.Num’
instance Num Word -- Defined in ‘GHC.Num’
instance Num Integer -- Defined in ‘GHC.Num’
instance Num Int -- Defined in ‘GHC.Num’
instance Num Float -- Defined in ‘GHC.Float’
instance Num Double -- Defined in ‘GHC.Float’
</code></pre>
<h3 id="types-of-polymorphism"><a class="header" href="#types-of-polymorphism">Types of Polymorphism</a></h3>
<p>In Java, there are two kinds of polymorphism:</p>
<ul>
<li>Parametric polymorphism
<ul>
<li>(Generics/Templates)</li>
<li>A class is generic over certain types</li>
<li>Can put whatever type you like in there to make a concrete class of that type</li>
</ul>
</li>
<li>Subtype polymorphism
<ul>
<li>Can do <code>class Duck extends Bird</code></li>
<li>Can put <code>Duck</code>s wherever <code>Bird</code>s are expected</li>
</ul>
</li>
</ul>
<p>Haskell has two kinds of polymorphism also:</p>
<ul>
<li>Parametric polymorphism
<ul>
<li>Type variables</li>
<li><code>id :: a -&gt; a</code></li>
<li>Can accept any type where <code>a</code> is</li>
</ul>
</li>
<li>Ad-hoc polymorphism
<ul>
<li>Uses type classes</li>
<li><code>double :: Num a =&gt; a -&gt; a</code></li>
<li><code>double x = x * 2</code></li>
</ul>
</li>
</ul>
<h3 id="further-uses-of-constraints"><a class="header" href="#further-uses-of-constraints">Further Uses of Constraints</a></h3>
<p>An example <code>Show</code> instance for pairs:</p>
<pre><code class="language-haskell">instance (Show a, Show b) =&gt; Show (a,b) Show where
    show (x,y) = &quot;(&quot; ++ show x ++ &quot;, &quot; ++ show y ++ &quot;)&quot;
</code></pre>
<p>The <code>(Show a, Show b) =&gt; </code> defines a constraint on <code>a</code> and <code>b</code> that they must both be instances of show for them to be used with this instance. The instance is actually defined on the type <code>(a,b)</code>.</p>
<p>Can also define that a typeclass has a superclass, meaning that for a type to be an instance of a typeclass, it must be an instance of some other typeclass first. The <code>Ord</code> typeclass has a superclass constraint of the <code>Eq</code> typeclass, meaning something cant be <code>Ord</code> without it first being <code>Eq</code>. This makes sense, as you can't have an ordering without first some notion of equality.</p>
<pre><code class="language-haskell">class Eq a =&gt; Ord a where
    (&lt;) :: a -&gt; a -&gt; Bool
    (&lt;=) :: a -&gt; a -&gt; Bool
</code></pre>
<h3 id="default-implementations"><a class="header" href="#default-implementations">Default Implementations</a></h3>
<p>Type classes can provide default method implementations. For example, <code>(&lt;=)</code> can be defined using the definition of <code>(&lt;)</code>, so a default one can be provided using <code>(==)</code></p>
<pre><code class="language-haskell">class Eq a =&gt; Ord a where
    (&lt;) :: a -&gt; a -&gt; Bool
    (&lt;=) :: a -&gt; a -&gt; Bool
    (&lt;=) x y = x &lt; y || x == y
    -- or defined infix
    x &lt;= y = x &lt; y || x == y
</code></pre>
<h3 id="derivable-type-classes"><a class="header" href="#derivable-type-classes">Derivable Type Classes</a></h3>
<p>Writing type class instances can be tedious. Can use the <code>deriving</code> keyword to automatically generate them, which does the same as manually defining type class instances.</p>
<pre><code class="language-haskell">data Bool = False | True
    deriving Eq
data Module = CS141 | CS118 | CS126
    deriving (Eq, Ord, Show)
</code></pre>
<p>Certain other typeclasses can be dervied too, by enabling language extensions within GHC. The extension <code>XDeriveFunctor</code> allows for types to include a <code>deriving Functor</code> statement.</p>
<h2 id="data-types"><a class="header" href="#data-types">Data Types</a></h2>
<p>How do we make our own data types in haskell? Algebraic data types.</p>
<ul>
<li><code>Bool</code> is a type</li>
<li>There are two values of type <code>Bool</code>
<ul>
<li><code>True</code></li>
<li><code>False</code></li>
</ul>
</li>
</ul>
<pre><code class="language-haskell">data Bool = True | False
</code></pre>
<p>A type definition consists of the type name <code>Bool</code> and it's data constructors, or values <code>True | False</code>. A type definition introduces data constructors into scope, which are just functions.</p>
<pre><code class="language-haskell">True :: Bool
False :: Bool
</code></pre>
<p>We can pattern match on data constructors, and also use them as values. This is true for <em>all</em> types.</p>
<pre><code class="language-haskell">not :: Bool -&gt; Bool
not True = False
not False = True
</code></pre>
<p>More examples:</p>
<pre><code class="language-haskell">data Module = CS141 | CS256 | CS263

data Language = PHP | Java | Haskell | CPP

--for this one, the type name and constructor name are separate names in the namespace
data Unit = Unit

-- this one has no values
data Void
</code></pre>
<h3 id="parametrised-data-constructors"><a class="header" href="#parametrised-data-constructors">Parametrised Data Constructors</a></h3>
<p>Parameters can be added to a data constructor by adding their types after the constructor's name. The example below defines a type to represent shapes. Remember that data constructors are just functions, and can be partially applied just like other functions.</p>
<pre><code class="language-haskell">data Shape = Rect Double Double | Circle Double
Rect :: Double -&gt; Double -&gt; Shape
Circle :: Double -&gt; Shape

-- functions utilising the Shape type

-- constructs a square
square x :: Double -&gt; Shape
square x = Rect x x

-- calculates area of a shape using pattern matching on constructors
area :: Shape -&gt; Double
area (Rect w h) = w * h
area (Circle r) = pi * r * r

isLine :: Shape -&gt; Bool#
isLine (Rect 1 h) = True
isLine (Rect w 1) = True
isLine _ = False

-- examples
area (square 4.0)
=&gt; area (Rect 4.0 4.0)
=&gt; 4.0 * 4.0
=&gt; 16.0

area (Circle 5.0)
=&gt; pi * 5.0 * 5.0
=&gt; pi * 25.0
=&gt; 78.53981...

</code></pre>
<h3 id="parametrised-data-types"><a class="header" href="#parametrised-data-types">Parametrised Data Types</a></h3>
<p>The <code>Maybe</code> type is an example of a data type parametrised over some type variable <code>a</code>. It exists within the standard library, defined as <code>data Maybe a = Nothing | Just a</code>. This type is used to show that either there is no result, or some type <code>a</code>.</p>
<p>A function using the <code>Maybe</code> type to perform devision safely, returning <code>Nothing</code> if the divisor is 0, and the result wrapped in a <code>Just</code> if the division can be done.</p>
<pre><code class="language-haskell">data Maybe a = Nothing | Just a

safediv :: Int -&gt; Int -&gt; Maybe Int
safediv x 0 = Nothing
safediv x y = Just (x `div y)
-- safediv 8 0 =&gt; Nothing
-- safediv 8 4 = Just (8 `div` 4) = Just 2

-- this is included in stdlib for extracting the value using pattern matching
fromMaybe :: a -&gt; Maybe a -&gt; a
fromMaybe x Nothing = x
fromMaybe _ (Just x) = x
</code></pre>
<p>Null references were invented in the 1960s ... the guy who invented them called them his &quot;billion dollar mistake&quot;. The <code>Maybe</code> type is a good alternative, which makes it clear that a value may be absent. Similar concepts exist in other procedural languages (Swift, Rust)</p>
<h3 id="recursive-data-types"><a class="header" href="#recursive-data-types">Recursive Data Types</a></h3>
<p>In Haskell, data types can be defined in terms of themselves. An example definition of the natural numbers is shown below, where a number is either zero, or one plus another number.</p>
<pre><code class="language-haskell">data Nat = Zero | Succ Nat

Zero :: Nat
Succ :: Nat -&gt; Nat

one = Succ Zero
two = Succ one
three = Succ two

add :: Nat -&gt; Nat -&gt; Nat
add Zero     m = m
add (Succ n) m = Succ (add n m)

mul :: Nat -&gt; Nat -&gt; Nat
mul Zero     m = Zero
mul (Succ n) m = add m (mul n m)
</code></pre>
<p>Another example defining binary trees in terms of themselves. A binary tree consists of subtrees (smaller binary trees). This type is parametrised over some type variable <code>a</code> also.</p>
<pre><code class="language-haskell">Data BinTree a = Leaf a | Node (BinTree a) (BinTree a)

--converts a binary tree to a list
flatten :: BinTree a -&gt; [a]
flatten (Leaf x)   = [x]
flatten (Node l r) = flatten l ++ flatten r

-- computes the max depth of the tree
depth :: BinTree a -&gt; Int
depth (Leaf _)   = 1
depth (Node l r) = 1 + max (depth l) (depth r)
</code></pre>
<h2 id="type-aliases"><a class="header" href="#type-aliases">Type Aliases</a></h2>
<p>Types can be aliased. For example, <code>String</code> has been an alias of <code>[Char]</code> all along.</p>
<pre><code class="language-haskell">type String = [Char]
</code></pre>
<p>Another example, defining a <code>Predicate</code> type</p>
<pre><code class="language-haskell">type Predicate a = a -&gt; Bool

isEven :: Predicate Int
isEven n = n `mod` 2 == 0

isEven' :: (Eq a, Integral a) =&gt; Predicate a
isEven' n = n `mod` 2 == 0
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="recursion"><a class="header" href="#recursion">Recursion</a></h1>
<p>Recursion is a way of expressing loops with no mutable state, by defining a function in terms of itself. The classic example, the factorial function. Defined mathematically:</p>
<p>$$
n! =
\begin{cases}
1 &amp; \text{if } n =0 \
n \times (n-1)!&amp;  \text{otherwise}
\end{cases}
$$</p>
<p>In haskell:</p>
<pre><code class="language-haskell">factorial :: Int -&gt; Int
factorial 0 = 1
factorial n = n * factorial (n-1)
</code></pre>
<p>It can be seen how this function reduced when applied to a value:</p>
<pre><code class="language-haskell">factorial 2
=&gt; 2 * factorial (2-1)
=&gt; 2 * factorial 1
=&gt; 2 * 1 * factorial (1-1)
=&gt; 2 * 1 * factorial 0
=&gt; 2 * 1 * 1
=&gt; 2
</code></pre>
<p>Another classic example, the fibonacci function:</p>
<pre><code class="language-haskell">fib :: Int -&gt; Int
fib 0 = 1
fib 1 = 1
fib n = fib (n-1) + fib (n-1)
</code></pre>
<p>In imperative languages, functions push frames onto the call stack every time a function is called. With no mutable state, this is not required so recursion is efficient and can be infinite.</p>
<p>Haskell automatically optimises recursive functions to make execution more efficient:</p>
<pre><code class="language-haskell">fac' :: Int -&gt; Int -&gt; Int
fac' 0 m = m
fac' n m = fac' (n-1) (n*m)
</code></pre>
<p>This version of the function prevents haskell from building up large expressions:</p>
<pre><code class="language-haskell">fac 500
=&gt; fac' 500 1
=&gt; fac' (500-1) (500*1)
=&gt; fac' 499 500
=&gt; fac (499-1) (499 * 500)
=&gt; fac' 498 249500
</code></pre>
<p>Notice the pattern for all recursive functions, where there is a recursive case, defining the function in terms of itself, and a base case. Without a base case, the function would recurse infinitely. The cases are usually defined as pattern matches.</p>
<h2 id="recursion-on-lists"><a class="header" href="#recursion-on-lists">Recursion on Lists</a></h2>
<p>Recursion is the natural way to operate on lists in haskell. Defining the product function, which returns the product of all the items in the list:</p>
<pre><code class="language-haskell">product :: [Int] -&gt; Int
product [] = 1
product (n:ns) = n * product ns
</code></pre>
<p>Here, the base case is the empty list <code>[]</code> and pattern match is used to &quot;de-cons&quot; the head off the list and operate on it <code>(n:ns)</code>. The function reduces as follows:</p>
<pre><code class="language-haskell">product [1,2,3,4]
=&gt; 1 * product [2,3,4]
=&gt; 1 * 2 * product [3,4]
=&gt; 1 * 2 * 3 * product [4]
=&gt; 1 * 2 * 3 * 4 * product []
=&gt; 1 * 2 * 3 * 4 * 1
=&gt; 24
</code></pre>
<h2 id="let-and-where"><a class="header" href="#let-and-where"><code>let</code> and <code>where</code></a></h2>
<p><code>let</code> and <code>where</code> clauses can be used to introduct local bindings within a function, which are useful in defining recursive functions. the <code>splitAt</code> function, which splits a list into two at a certain index.</p>
<pre><code class="language-haskell">splitAt :: Int -&gt; [a] -&gt; ([a],[a])
splitAt 0 xs = ([],xs)
splitAt n [] = ([],[])
splitAt n (x:xs) = (x:ys, zs)
    where (ys,zs) = splitAt (n-1) xs
-- alternatively
splitAt n xs =
  let
    ys = take n xs
    zs = drop n xs
  in (ys,zs)
</code></pre>
<p><code>let</code> and <code>where</code> can also define functions locally, as everything in haskell is a function.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="higher-order-functions"><a class="header" href="#higher-order-functions">Higher Order Functions</a></h1>
<p>Higher order functions are functions which operate on functions.</p>
<h2 id="associativity-of-functions"><a class="header" href="#associativity-of-functions">Associativity of functions</a></h2>
<p>Function expressions associate to the right (one argument is applied at a time)</p>
<pre><code class="language-haskell">xor a b = (a || b ) &amp;&amp; not (a &amp;&amp; b)
-- equivalent to
xor = \a -&gt; \b -&gt; (a || b) &amp;&amp; not (a &amp;&amp; b)
-- equivalent to
xor = \a -&gt; (\b -&gt; (a || b) &amp;&amp; not (a &amp;&amp; b))

</code></pre>
<ul>
<li>All functions in haskell are technically nameless, single-parameter functions</li>
<li>Currying allows for functions which return other functions</li>
<li>Functions are expressions
<ul>
<li>The body of a function is an expression</li>
</ul>
</li>
<li>When a function is applied to an argument it reduces to it's body.</li>
</ul>
<p>Function <em>application</em> associates to the left:</p>
<pre><code class="language-haskell">xor True True
=&gt; (xor True) True
=&gt; ((\a -&gt; (\b -&gt; (a || b) &amp;&amp; not (a &amp;&amp; b))) True) True
=&gt; (\b -&gt; (True || b) &amp;&amp; not (True &amp;&amp; b)) True
=&gt; (True || True) &amp;&amp; not (True &amp;&amp; True)

</code></pre>
<p>Function <em>types</em>, however, associate to the right:</p>
<pre><code class="language-haskell">xor :: Bool -&gt; Bool -&gt; Bool
xor = \a -&gt; \b -&gt; (a || b) &amp;&amp; not (a &amp;&amp; b)
--equivalent to
xor :: Bool -&gt; (Bool -&gt; Bool)
xor = xor = \a -&gt; (\b -&gt; (a || b) &amp;&amp; not (a &amp;&amp; b))
</code></pre>
<p>The table below shows how functions application and types associate:</p>
<table><thead><tr><th>Without Parentheses</th><th>With Parentheses</th></tr></thead><tbody>
<tr><td><code>f x y </code></td><td><code>(f x) y</code></td></tr>
<tr><td><code>\x -&gt; \y -&gt; ...</code></td><td><code>\x -&gt; (\y -&gt; ...)</code></td></tr>
<tr><td><code>Int -&gt; Int -&gt; Int</code></td><td><code>Int -&gt; (Int -&gt; Int)</code></td></tr>
</tbody></table>
<h2 id="functions-as-arguments-map"><a class="header" href="#functions-as-arguments-map">Functions as Arguments (<code>map</code>)</a></h2>
<p>Haskell functions can be taken as arguments to other functions. Functions that take/return functions are called higher order functions. An example, increasing every element of a list by one:</p>
<pre><code class="language-haskell">incByOne :: [Int] -&gt; [Int]
incByOne xs = [x+1 | x &lt;- xs]
-- or using recursion
incByOne [] = []
incByOne (x:xs) = x+1 : incByOne xs
</code></pre>
<p>All this function does is applies the function <code>(+ 1)</code> to every element. This pattern can be generalised using the <code>map function</code>: a function that applies a function given as an argument to every element of a list:</p>
<pre><code class="language-haskell">map :: (a -&gt; b) -&gt; [a] -&gt; [b]
map f []     = []
map f (x:xs) = f x : map f xs
</code></pre>
<p>Note the type signature of the map function is <code>map :: (a -&gt; b) -&gt; [a] -&gt; [b]</code>, meaning the first argument is a function of type <code>(a -&gt; b)</code>. Using this to implement <code>incByOne</code>:</p>
<pre><code class="language-haskell">incByOne = map (+1)
-- tracing it's evaluation:
incByOne [1,2,3]
=&gt; map (+1) [1,2,3]
=&gt; (1+1) : map (+1) [2,3]
=&gt; (1+1) : (1+2) : map (+1) [3]
=&gt; (1+1) : (1+2) : (1+3) : map (+1) []
=&gt; (1+1) : (1+2) : (1+3) : []
=&gt; [2,3,4]
</code></pre>
<p>Effectively, <code>map f [x, y, z]</code> evaluates to <code>[f x, f y, f z]</code></p>
<h2 id="sections"><a class="header" href="#sections">Sections</a></h2>
<p>Sections are partially applied operators. Operators are functions like any other, and as such can be partially applied, passed as arguments, etc. The addition operator is shown as an example, but the same applies to any binary operator.</p>
<pre><code class="language-haskell">(+) :: Num a =&gt; a -&gt; a -&gt; a
(+ 4) :: Num a =&gt; a -&gt; a
(4 +) :: Num a =&gt; a -&gt; a
(+) 4 8 = 4 + 8
(+ 4) 8 = 8 + 4
(4 +) 8 = 4 + 8
</code></pre>
<h2 id="filter"><a class="header" href="#filter"><code>Filter</code></a></h2>
<p>Filter is an example of another higher order function, which given a list, returns a new list which contains only the elements satisfying a given predicate.</p>
<pre><code class="language-haskell">filter :: (a -&gt; Bool) -&gt; [a] -&gt; [a]
filter p [] = []
filter p (x:xs)
    | p x       = x : filter p xs
    | otherwise =     filter p xs
</code></pre>
<p>Some examples:</p>
<pre><code class="language-haskell">-- remove all numbers less than or equal to 42
greaterThan42 :: (Int -&gt; Bool) -&gt; [Int] -&gt; [Int]
greaterThan42 xs = filter (&gt;42) xs
-- only keep uppercase letters
uppers :: (Char -&gt; Bool) -&gt; String -&gt; String
uppers xs = filter isUpper xs
</code></pre>
<h2 id="curried-vs-uncurried"><a class="header" href="#curried-vs-uncurried">Curried vs Uncurried</a></h2>
<p>Tuples can be used to define uncurried functions. A function that takes two arguments can be converted to a function that takes an a tuple of two arguments, and returns a single argument/</p>
<pre><code class="language-haskell">uncurriedAdd :: (Int, Int) -&gt; Int
uncurriedAdd (x, y) = x + y
</code></pre>
<p>There are higher-order functions, <code>curry</code> and <code>uncurry</code>, which will do this for us:</p>
<pre><code class="language-haskell">curry :: ((a,b) -&gt; c) -&gt; a -&gt; b -&gt; c
curry f x y = f (x,y)

uncurry :: (a -&gt; b -&gt; c) -&gt; (a,b) -&gt; c
uncurry f (x,y) = f x y

-- examples
uncurriedAdd :: (Int, Int) -&gt; Int
uncurriedAdd = uncurry (+)

curriedAdd :: Int -&gt; Int -&gt; Int
curriedAdd = curry uncurriedAdd

addPairs :: [Int]
addPairs = map (uncurry (+)) [(1, 2), (3, 4)]
</code></pre>
<h2 id="folds"><a class="header" href="#folds">Folds</a></h2>
<p><code>foldr</code> and <code>foldl</code> &quot;collapse&quot; a list by applying a function <code>f</code> to each element in the list in turn, where the first argument is an accumulated value, and the second is the starting value passed. There are several functions which follow this pattern, all reducing a list to a single value using recursion:</p>
<pre><code class="language-haskell">-- and together all bools in the list
and :: [Bool] -&gt; Bool
and [] = True
and (b:bs) = ((&amp;&amp;) b) (and bs)

-- product of everything in the list
product :: Num a =&gt; [a] -&gt; a
product [] = 1
product (n:ns) = ((*) n) (product ns)

-- length of list
length :: [a] -&gt; Int
length [] = 0
length (x:xs) = ((+) 1) (length xs)
</code></pre>
<p>All of these functions have a similar structure, and can be redefined using <code>foldr</code>:</p>
<pre><code class="language-haskell">foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b
foldr f z []     = z
foldr f z (x:xs) = f x (foldr f z xs)

-- examples
and :: [Bool] -&gt; Bool
and = foldr (&amp;&amp;) True

product :: Num a =&gt; [a] -&gt; a
product = foldr (*) 1

length :: [a] -&gt; Int
length = foldr (\x n -&gt; n + 1) 0
</code></pre>
<p>In essence, <code>foldr f z [1, 2, 3]</code> is equal to <code>f 1 (f 2 (f 3 z))</code>. <code>foldr</code> folds from right (<code>r</code>) to left, starting by applying the function to the last element of the list first. <code>foldl</code>, however, works in the opposite direction:</p>
<pre><code class="language-haskell">foldl :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b
foldl f z [] = z
foldl f z (x:xs) = foldl f (f z x) xs
</code></pre>
<p><code>foldl f z [1, 2, 3]</code> is equal to <code>f (f (f z 1) 2) 3</code>. For some functions (commutative ones), there is no difference, but often the choice of which to use is important.</p>
<h2 id="function-composition"><a class="header" href="#function-composition">Function Composition</a></h2>
<p>In haskell, functions are composed with the <code>(.)</code> operator, a higher order function defined as:</p>
<pre><code class="language-haskell">(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
(.) f g x = f (g x)
</code></pre>
<p>Function composition is used to chain functions, so instead of <code>f (g (h x))</code>, you can write <code>f.g.h x</code>. An example, defining a function <code>count</code> to count the number of occurrences of an element in a list:</p>
<pre><code class="language-haskell">count :: Eq a =&gt; a =&gt; [a] -&gt; Int
count _ [] = 0
count y (x:xs)
    | y == x    = 1 + count y xs
    | otherwise =     count y xs

--alternatively, using a fold
count y = foldr (\x l -&gt; if y==x then 1+l else l) 0

-- the stdlib can do this
count y x = length (filter (==y) xs)
count y = length . filter (==y) -- using composition
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lazy-evaluation"><a class="header" href="#lazy-evaluation">Lazy Evaluation</a></h1>
<h2 id="evaluation-strategies"><a class="header" href="#evaluation-strategies">Evaluation Strategies</a></h2>
<p>How are programs evaluated? There are a number of strategies for evaluating a program. For example, the expression <code>(4+8) * (15 + 16)</code> can be evaluated in different ways:</p>
<pre><code class="language-haskell">(4+8) * (15 + 16)
=&gt; 12 * (15+16)
=&gt; 12 * 31
=&gt; 372

-- or

(4+8) * (15 + 16)
=&gt; (4 + 8) * 31
=&gt; 12 * 31
=&gt; 372
</code></pre>
<p>The final value when reducing an expression (it cannot be reduced further) is the <em>normal form</em>, 372 in this case. No matter how the expression is reduced, the normal form is the same. Haskell's type system prevents us from writing anything that cannot reduce to normal form.</p>
<p>A sub-expression (anything partially reduced that can still be reduced further) is called a <em>redex</em>, short for reducible expression. Evaluation strategies only matter when there are multiple redexes, otherwise there is only one route we can take to evaluate an expression.</p>
<h2 id="strict-evaluation"><a class="header" href="#strict-evaluation">Strict Evaluation</a></h2>
<p><strong>A programming language is strict if the arguments of the function are evaluated <em>before</em> the function is called</strong>.</p>
<p>Evaluating <code>fac 500</code> using a strict method:</p>
<pre><code class="language-haskell">fac :: Int -&gt; Int
fac n = fac' n 1

fac' :: Int -&gt; Int -&gt; Int
fac n m = case n of
  0 -&gt; m
  _ -&gt; fac' (n-1) (n*m)

fac 500      -- a redex, function application
=&gt; fac' 500 1   -- another redex
=&gt; fac' (500-1) (500*1)     -- 3 redexes, two multiplications and function application
=&gt; fac' 499 (500*1)     -- two redexes now as 500-1=499 is now in normal form
=&gt; fac' 499 500         -- now only one redex
=&gt; fac' (499-1) (499*500) -- back to 3 redexes
... -- this goes on for a while
</code></pre>
<p><strong>Call-by-value</strong> means that all function arguments are reduced to their normal forms (values), and then passed as such to the function. The call-by-value strategy is an example of strict evaluation. This is the evaluation strategy used by most programming languages: Java, JS, PHP, C/C++, OCaml, F#, Python, Scala, Swift. Note that some of these are also functional languages.</p>
<p>Haskell, on the other hand, is far superior. It is non-strict: aka <strong>lazy</strong>.</p>
<h2 id="call-by-name"><a class="header" href="#call-by-name">Call-by-name</a></h2>
<p>A non-strict evaluation strategy by which expressions given to functions as arguments are not reduced before the function call is made.<br />
Expressions are only reduced when their value is needed. Same example as before:</p>
<pre><code class="language-haskell">fac 2
=&gt; fac' 2 1  -- still a redex here
=&gt; case 2 of
     0 -&gt; 1
     _ -&gt; fac' (2-1) (2*1)   -- the function call is expanded to its expression
=&gt; fac' (2-1) (2*1) -- left with 3 redexes now
=&gt; case 2-1 of
     0 -&gt; 2*1
     _ -&gt; fac' ((2-1)-1) ((2-1) * (2*1)) -- a lot of redexes, but we don't need to know the value of any except the one in the case expression. this one is evaluated but not the others
=&gt; case 1 of
     0 -&gt; 2*1
     _ -&gt; fac' ((2-1)-1) ((2-1) * (2*1)) -- something actually got evaluated, as we needed it's value. we still have a lot of redexes though
</code></pre>
<p>Note how that the same argument (<code>(2-1)</code>) is there 3 times, but it is only evaluated when it is needed. This means that it is evaluated possibly more than once, as it may be needed more than once at different points. With call-by-value (strict), an expression is only reduced once but will only ever be reduced once, but with call-by-name (lazy), expressions may end up being evaluated more than once.</p>
<h2 id="sharing"><a class="header" href="#sharing">Sharing</a></h2>
<p>Sharing avoids duplicate evaluation. Arguments to functions are turned into local definitions, so that when an expression is evaluated, any expressions that are identical are also evaluated. The same example again, using both call-by-name and sharing:</p>
<pre><code class="language-haskell">fac' :: Int -&gt; Int -&gt; Int
fac' n m = case n of
  0 -&gt; m
  _ -&gt; let x = n-1
           y = n*m
       in fac' x y

-- the compiler has replaced the expression arguments with let-bound definitions

fac 2
=&gt; fac' 2 1
=&gt; case 2 of
     0 -&gt; 1
     _ -&gt; let x0 = 2-1
              y0 = 2*1
          in fac' x0 y0 --expressions bound to variables

=&gt; let x0 = 2-1
       y0 = 2*1 -- two redexes
   in fac' x0 y0
=&gt; let x0 = 2-1
       y0 = 2*1
   in case x0 of
        0 -&gt; y0
        _ -&gt; let x1 = x0-1
                 y1 = x0 * y0
            in fac' x1 y1 -- even more redexes and bindings
    -- x0 can be replaced by 1, which evaluates the expresion in all places where x0 is used
</code></pre>
<p>Can think of let or where bindings as storing expressions in memory in such a way that we can refer to them from elsewhere using their names.</p>
<p>The combination of call-by-name and sharing is known as lazy evaluation, which is the strategy haskell uses. Nothing is evaluated until it is needed, and work is only ever done once. (Strict evaluation is done sometimes if the compiler decides to, so it is technically <em>non-strict</em> instead of lazy.)</p>
<h2 id="evaluation-in-haskell"><a class="header" href="#evaluation-in-haskell">Evaluation in Haskell</a></h2>
<p>An example, using haskell's lazy evaluation strategy:</p>
<pre><code class="language-haskell">length (take 2 (map even [1,2,3,4]))
=&gt; length (take 2 (even 1 : map even [2,3,4])) -- check argument is non-empty list
=&gt; length (even 1 : take (2-1) (map even [2,3,4])) -- even 1 cons'd to take 1 of map
=&gt; 1 + length (take (2-1) (map even [2,3,4])) --know length is at least 1, take out
=&gt; 1 + length(take 1 (map even [2,3,4]))
=&gt; 1 + length (take 1 (even 2 : map even [3,4])) --another map call
=&gt; 1 + (1 + length (take (1-1) (map even [3,4])) -- length again
=&gt; 1 + (1 + length []) --take 0 so empty list
=&gt; 1 + 1 + 0 -- return 0
=&gt; 2 -- done
</code></pre>
<p>Note how half the map wasn't evaluated, because haskell knew we only cared about the first 2 elements. However this trace doesn't show any of the internal bindings haskell makes for sharing expressions. The compiler does this by transforming the expression:</p>
<pre><code class="language-haskell">length (take 2 (map even [1,2,3,4]))
-- becomes
let
  xs = take 2 (map even [1,2,3,4])
in length xs
-- becomes
let
  ys = map even [1,2,3,4]
  xs = take 2 ys
in length xs
-- becomes
let
  ys = map even (1:(2:(3:(4:[]))))
  xs = take 2 ys
in length xs
-- finally
let
  zs4 = 4:[]
  zs3 = 3:zs4
  zs2 = 2:zs3
  zs  = 1:zs2
  ys  = map even zs
  xs  = take 2 ys
in length xs
</code></pre>
<p>In this representation, everything is let bound it it's own definition, and nothing is applied except to some literal or to another let bound variable. The representation in memory looks something like this:</p>
<p><img src="cs141/./img/closures.png" alt="" /></p>
<p>These things in memory are called <em>closures</em>. A closure is an object in memory that contains:</p>
<ul>
<li>A pointer to some code that implements the function it represents (not shown)</li>
<li>A pointer to all the free variables that are in scope for that definition
<ul>
<li>A free variable is any variable in scope that is not a parameter</li>
</ul>
</li>
</ul>
<p>The closures form a graph, where the closures all point to each other.</p>
<p>Another example, using <code>map</code>:</p>
<pre><code class="language-haskell">map :: (a -&gt; b) -&gt; [a] -&gt; [b]
map _ [] = []
map f (x:xs) = f x : map f xs

-- removing all syntactic sugar, done by compiler

map = \f -&gt; \arg -&gt;
  case arg of
    []      -&gt; []
    (x: xs) -&gt; let
                 y  = f x
                 ys = map f xs
                in (y:ys)
</code></pre>
<p>Using this definition of <code>map</code> to evaluate the expression from before (<code>length (take 2 (map even [1,2,3,4]))</code>):</p>
<pre><code class="language-haskell">let
  zs4 = 4:[]
  zs3 = 3:zs4
  zs2 = 2:zs3
  zs  = 1:zs2
  xs  = map even zs
  ys  = take 2 xs
in length ys
-- new closures allocated by map, using 2nd case of map function
let
  zs4 = 4:[]
  zs3 = 3:zs4
  zs2 = 2:zs3
  zs  = 1:zs2
  y0 = even 1
  ys0 = map even zs2 -- new closures
  xs  = y0 : ys -- updated to be a cons cell
  ys  = take 2 xs
in length ys
</code></pre>
<p>The graph of closures representing this:</p>
<p><img src="cs141/./img/closures-2.png" alt="" /></p>
<h2 id="strictness-in-haskell"><a class="header" href="#strictness-in-haskell">Strictness in Haskell</a></h2>
<p>Things <em>can</em> be evaluated strictly in haskell, if you want. This is prefereable in some cases for performance reasons. The <code>\$!</code> operator forces strict function application. The version of the function below forces the recursive call to be evaluated first.</p>
<pre><code class="language-haskell">fac' :: Int -&gt; Int -&gt; Int
fac' 0 m = m
fac' n m = (fac' \$! (n-1)) (n*m)
</code></pre>
<h2 id="infinite-data-structures"><a class="header" href="#infinite-data-structures">Infinite Data Structures</a></h2>
<p>Laziness means data structures can be infinite in haskell. This is also facilitated by the lack of call stack, as there is no &quot;max recursion depth&quot; like in strict languages.</p>
<pre><code class="language-haskell">from :: Int -&gt; [Int]
from n = n : from (n+1)
</code></pre>
<p>This function builds an infinite list of a sequence of <code>Int</code>s, starting with the <code>Int</code> passed. An example usage, showing how lazy evaluation works with it:</p>
<pre><code class="language-haskell">take 3 (from 4)
=&gt; take 3 (4 : from 5)
=&gt; 4 : take 2 (from 5)
=&gt; 4 : take 2 (5 : from 6)
=&gt; 4 : 5 : take 1 (from 6)
=&gt; 4 : 5 : take 1 (6 : from 7)
=&gt; 4 : 5 : 6 : take 0 (from 7)
=&gt; 4 : 5 : 6 : []
=&gt; [4,5,6]
</code></pre>
<p>The infinite evaluation is <em>short-circuited</em>, as the compiler knows it only needs the first 3 elements.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="reasoning-about-programs"><a class="header" href="#reasoning-about-programs">Reasoning About Programs</a></h1>
<p>Haskell can use normal software testing methods to verify correctness, but because haskell is a pure language, we can do better and formally prove properties of our functions and types.</p>
<h2 id="natural-numbers"><a class="header" href="#natural-numbers">Natural Numbers</a></h2>
<p>Natural numbers can be defined as <code>data Nat = Z | S Nat</code> in haskell. Alternatively, using mathematical notation, this can be written $0 \in N ;; \forall n \in N,; n+1 \in N$. Addition can then be defined recursively:</p>
<pre><code class="language-haskell">add :: Nat -&gt; Nat -&gt; Nat
add Z     m = m
add (S n) m = S (add n m)
</code></pre>
<p>Addition has certain properties which must hold true:</p>
<ul>
<li>Left identity: <code>∀m :: Nat, add Z m == m</code>
<ul>
<li>$0 + m = m$</li>
</ul>
</li>
<li>Right identity: <code>∀m :: Nat, add m Z == m </code>
<ul>
<li>$m + 0 = m$</li>
</ul>
</li>
<li>Associativity: <code>∀x y z :: Nat, add x (add y z) == add (add x y) z</code>
<ul>
<li>$x + (y + z) = (x + y) + z$</li>
</ul>
</li>
</ul>
<p>These can be proven using equational reasoning, which proves that an equality holds in all cases. Generally, either a property can be proved by applying and un-applying either side of an equation, and/or by induction.</p>
<p>To prove the left identity is easy, as it is an exact match of one of our equations for <code>add</code>:</p>
<pre><code class="language-haskell">add Z m
-- applying add
= m
</code></pre>
<p>The right identity is a little harder, as we can't just directly apply one of our equations. We can instead induct on <code>m</code>. First, the base case:</p>
<pre><code class="language-haskell">add Z Z
-- applying add
= Z
</code></pre>
<p>Using the induction hypothesis <code>add m Z = m</code>, we need to show the inductive step holds for <code>S m</code> (<code>m+1</code>):</p>
<pre><code class="language-haskell">add (S m) Z
-- applying add
= S (add m Z)
-- applying induction hypothesis
= S m
</code></pre>
<p>This proves the right identity. To prove associativity we will again use induction, this time on <code>x</code>. The base case is <code>add Z (add y z)</code>:</p>
<pre><code class="language-haskell">add Z (add y z)
-- applying add
= add y z
-- un-applying add
= add (add Z y) z
</code></pre>
<p>The proof holds for <code>x = Z</code>. Here, the proof was approached from either end to meet in the middle, but written as a single list of operations for clarity. Sometimes it is easier to do this and work from either direction, especially when un-applying functions as it is more natural.</p>
<p>The induction hypothesis is <code>add x (add y z) == add (add x y) z</code>, and can be assumed. We need to prove the inductive step <code>add (S x) (add y z) == add (add (S x) y) z</code>:</p>
<pre><code class="language-haskell">add (S x) (add y z)
-- applying add
= S (add x (add y z))
-- applying induction hypothesis
= S (add (add x y ) z)
-- un-applying add
= add (S (add x y)) z
-- un-applying add
= add (add (S x) y) z
</code></pre>
<p>This proves associativity.</p>
<h2 id="induction-on-lists"><a class="header" href="#induction-on-lists">Induction on Lists</a></h2>
<p>We can induct on any recursive type, including lists: <code>data List a = Empty | Cons a (List a)</code>. Using this definition, we can prove map fusion. Map fusion states that we can turn multiple consecutive map operations into a single one with composed functions:</p>
<ul>
<li><code>map f (map g xs) = map (f.g) xs</code>
<ul>
<li><code>∀f :: b -&gt; c</code></li>
<li><code>∀g :: a -&gt; b</code></li>
<li><code>∀xs :: [a]</code></li>
</ul>
</li>
</ul>
<p>The definitions of <code>map</code> and <code>.</code> may be useful:</p>
<pre><code class="language-haskell">map :: (a -&gt; b) -&gt; [a] -&gt; [b]
map f []     = []
map f (x:xs) = f x : map f xs

(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
(.) f g x = f (g x)
</code></pre>
<p>Map fusion can be proved by induction on xs. The base case is <code>map f (map g []) = map (f.g) []</code>:</p>
<pre><code class="language-haskell">map f (map g [])
-- applying map
= map f []
-- applying map
= []
-- un-applying map
= map (f.g) []
</code></pre>
<p>Using the induction hypothesis <code>map f (map g xs) = map (f.g) xs</code>, we can prove the inductive case <code>map f (map g (x : xs)) = map (f.g) (x : xs)</code>:</p>
<pre><code class="language-haskell">map f (map g (x : xs))
-- applying map
= map f (g x : map g xs)
-- applying map
= f (g x) : map f (map g xs)
-- induction hypothesis
= f (g x) : map (f.g) xs
-- un-applying (.)
= (f.g) x : map (f.g) xs
-- un-applying map
= map (f.g) (x : xs)
</code></pre>
<h2 id="proving-a-compiler"><a class="header" href="#proving-a-compiler">Proving a Compiler</a></h2>
<p>Given a simple expression language:</p>
<pre><code class="language-haskell">data Expr = Val Int | Plus Expr Expr
</code></pre>
<p>And a simple instruction set:</p>
<pre><code class="language-haskell">data Instr = Push Int | Add
type Program = [Instr]
type Stack = [Int]
</code></pre>
<p>We can write an <code>exec</code> function as an interpreter for our instruction set:</p>
<pre><code class="language-haskell">exec :: Program -&gt; Stack -&gt; Stack
exec []                    s  = s
exec (Push n : p)          s  = exec p (n : s)
exec (Add    : p) (y : x : s) = exec p (x + y : s)
</code></pre>
<p>An <code>eval</code> function to evaluate our expressions:</p>
<pre><code class="language-haskell">eval :: Expr -&gt; Int
eval (Val n)    = n
eval (Plus l r) = eval l + eval r

</code></pre>
<p>And a <code>comp</code> function as a compiler for our <code>Expr</code> language to our <code>Instr</code> instruction set:</p>
<pre><code class="language-haskell">comp :: Expr -&gt; Program
comp (Val n) = [PUSH n]
comp (Plus l r) = comp l ++ comp r ++ [ADD]
</code></pre>
<p>Our compiler will be considered correct if for any expression, evaluating it yields the same result as compiling and then executing it:</p>
<pre><code class="language-haskell">∀ e :: Expr, s :: Stack . eval e : s == exec (comp e) s
</code></pre>
<p>This can be proved by induction on <code>e</code>. The base case for <code>Expr</code> is for <code>Val</code>s, and we want to show that <code>eval (Val n) s == exec (comp (Val n)) s</code>. This time, we start with the RHS:</p>
<pre><code class="language-haskell">exec (comp (Val n)) s
-- applying comp
= exec [Push n] s
-- applying exec
= exec [] (n : s)
-- applying exec
= (n : s)
-- unappplying eval
= eval (Val n) s
</code></pre>
<p>Our inductive case to be proved is <code>eval (Plus l r) s == exec (comp (Plus l r)) s</code>. Since the <code>Plus</code> constructor has two values of type <code>Expr</code>, there are two induction hypotheses:</p>
<ul>
<li>for <code>l</code>: <code>eval l : s == exec (comp l) s</code></li>
<li>for <code>r</code>: <code>eval r : s == exec (comp r) s</code></li>
</ul>
<pre><code class="language-haskell">exec (comp (Plus l r)) s
-- applying comp
= exec (comp l ++ comp r ++ [Add]) s
-- distributivity of (++)
= exec (comp l ++ (comp r ++ [Add])) s
-- distributivity lemma
= exec (comp r ++ [Add]) (exec (comp l) s)
-- distributivity lemma
= exec [Add] (exec (comp r) (exec (comp l) s))
-- induction hypothesis
= exec [Add] (exec (comp r) (eval l : s))
-- induction hypothesis
= exec [Add] (eval r : (eval l : s))
-- applying exec
= exec [] ((eval l + eval r) : s)
-- applying exec
= (eval l + eval r) : s
-- un-applying exec
= eval (Plus l r) s
</code></pre>
<p>The proof holds, but relies on a lemma proving the distributivity of the exec function, which states that executing a program where a list of instructions <code>xs</code> is followed by a list of instructions <code>ys</code> is the same as first executing <code>xs</code> and then executing <code>ys</code> with the stack that results from executing <code>xs</code>: <code>∀ xs ys::Program, s::Stack . exec (xs++ys) s == exec ys (exec xs s)</code>.</p>
<p>This can be proved by induction on <code>xs</code>. The base case is the empty list <code>[]</code>: <code>exec ([] ++ ys) s == exec ys (exec [] s)</code>:</p>
<pre><code class="language-haskell">exec ys (exec [] s)
-- applying exec
= exec ys s
-- un-applying (++)
= exec ([] ++ ys) s
</code></pre>
<p>The induction hypothesis is <code>exec (xs++ys) s == exec ys (exec xs s)</code>. The inductive step is <code>exec ((x : xs) ++ ys) s == exec ys (exec (x : xs) s)</code>. As <code>x</code> could be either <code>Push x</code> or <code>Add</code>, we perform case analysis on <code>x</code>, first with the case where <code>x = Push n</code>:</p>
<pre><code class="language-haskell">exec ys (exec (Push n : xs) s)
-- applying exec
= exec ys (exec xs (n : ns))
-- induction hypothesis
= exec (xs ++ ys) (n : s)
-- un-applying exec
= exec (Push n : (xs ++ ys)) s
-- un-applying (++)
= exec ((Push n : xs) ++ ys) s
</code></pre>
<p>The inductive step holds for the <code>Push n</code> case. The <code>Add</code> case:</p>
<pre><code class="language-haskell">exec ys (exec (Add : xs) s)
-- assuming stack has at least 2 elements
exec ys (exec (Add : xs) (b : a : s'))
-- applying exec
exec ys (exec xs (a + b : s'))
-- induction hypothesis
exec (xs ++ ys) (a + b : s')
-- un-applying exec
exec (Add : (xs ++ ys)) (b : a : s')
-- un-applying (++)
exec ((Add : xs) ++ ys) (b : a : s')
-- assumption
exec ((Add : xs) ++ ys) s
</code></pre>
<p>This proves the inductive case for the <code>Add</code> instruction, and therefore the proof for the distributivity of <code>exec</code> lemma, which supported our initial proof of the correctness of our compiler.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="functors--foldables"><a class="header" href="#functors--foldables">Functors &amp; Foldables</a></h1>
<h2 id="the--operator"><a class="header" href="#the--operator">The <code>\$</code> Operator</a></h2>
<p>The <code>\$</code> operator is an operator for function application. It has signature:</p>
<pre><code class="language-haskell">(\$) :: (a -&gt; b) -&gt; a -&gt; b
f \$ x = f x
</code></pre>
<p>At first it doesn't look like it does much, but it is actually defined as <code>infixr 0</code> meaning it is:</p>
<ul>
<li>An infix operator with right associativity</li>
<li>Has the lowest precedence possible.</li>
</ul>
<p>In contrast, normal function application is left associative and has the highest precedence possible. Practically, this means it can be used where you would otherwise have to use parentheses, to make code a lot cleaner. Some examples:</p>
<pre><code class="language-haskell">-- elem finds if an item x is contained in the list xs
elem :: Eq a =&gt; a -&gt; [a] -&gt; Bool
elem x xs = not (null (filter (==x) xs))
-- rewritten, without parentheses
elem x xs = not \$ null \$ filter (==x) xs
-- or using function composition (.)
elem x = not . null . filter (==x)
</code></pre>
<p>Another example, shown along with a trace of it's reduction:</p>
<pre><code class="language-haskell">map (\$ 4) [even, odd]
=&gt; (even $ 4) : map (\$ 4) [odd]
=&gt; (even \$ 4) : (odd \$ 4) : []
=&gt; True : (odd \$ 4) : []
=&gt; True : False : []
=&gt; [True, False]
</code></pre>
<h2 id="foldables"><a class="header" href="#foldables">Foldables</a></h2>
<p><a href="cs141/./functions.html#folds">It has already been shown</a> how many examples of recursive functions can be rewritten with a <code>fold</code>. <code>fold</code>ing is a an example of a useful design pattern in functional programming.</p>
<h3 id="a-trip-to-michaels-tree-nursery"><a class="header" href="#a-trip-to-michaels-tree-nursery">A Trip to Michael's Tree Nursery</a></h3>
<p>Binary trees are recursive data structures, that can be recursively operated on (much like lists). The example below shows a simple definition of a binary tree along with some functions to operate on it.</p>
<pre><code class="language-haskell">-- our binary tree type
data BinTree a = Leaf | Node (BinTree a) a (BinTree a)
 deriving Show

-- simple recursive functions
-- how big is the tree?
size :: BinTree a -&gt; Int
size Leaf = 0
size Node (l _ r) = 1 + size l + size r

-- is x contained within the tree?
member:: Eq a =&gt; a -&gt; BinTree a -&gt; Bool
member _ Leaf = False
member x (Node l y r) = x == y || member x l || member x r

-- what is the sum of all the Nums in the tree
tsum :: Num a =&gt; BinTree a -&gt; a
tsum Leaf =0
tsum (Node l n r) = n + tsum l + tsum r
</code></pre>
<p>These are all recursive functions operating on a tree, and can be generalised by defining our own version of a fold for trees, dubbed <code>toldr</code>. Note the similarities between <code>foldr</code> and <code>toldr</code>.</p>
<pre><code class="language-haskell">toldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; BinTree a -&gt; b
toldr f z Leaf = z
toldr f z (Node l x r) = f x (toldr f (toldr f z r) l)

tsum :: Num a =&gt; BinTree a -&gt; a
tsum = toldr (+) 0

member :: Eq a =&gt; a -&gt; BinTree a -&gt; Bool
member x = toldr (\y r -&gt; x==y || r) False

size :: BinTree a -&gt; Int
size = toldr(\_ r -&gt; 1 + r) 0
</code></pre>
<h3 id="the-foldable-typeclass"><a class="header" href="#the-foldable-typeclass">The <code>Foldable</code> Typeclass</a></h3>
<p>This abstraction does actually exist in the standard libary, as a typeclass. A type can be an instance of <code>Foldable</code> (like lists), which then allows <code>foldr</code> to be used on it.</p>
<pre><code class="language-haskell">class Foldable t where
  foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; t a -&gt; b

-- for lists
-- exists in prelude
instance Foldable [] where
  foldr f z [] = z
  foldr f z (x:xs) = f x (foldr f z xs)

-- for our bintree
instance Foldable BinTree where
  foldr _ z Leaf         = z
  foldr f z (Node l x r) = f x (foldr f (foldr f z r) l)
</code></pre>
<p>This instance of <code>Foldable</code> for <code>BinTree</code> can now be used to generalise our functions that operate on it:</p>
<pre><code class="language-haskell">sum :: (Foldable t, Num a) =&gt; t a -&gt; t
sum = foldr (+) 0

elem :: (Foldable t, Eq a) =&gt; a -&gt; t a -&gt; Bool
elem x = foldr (\y r -&gt; x==y || r) False

length :: Foldable t =&gt; t a -&gt; Int
length = foldr (\_ r -&gt; 1 + r) 0
</code></pre>
<p>These methods are actually part of the <code>Foldable</code> typeclass, so when defining an instance of <code>Foldable</code> on some type, you get them for free, and they are polymorphic over all foldable types.</p>
<p><code>Foldable</code> is also a derivable typeclass using the language extension <code>-XDeriveFoldable</code>, so all of this can be derived automatically.</p>
<h2 id="functors"><a class="header" href="#functors">Functors</a></h2>
<p>Bringing back our <code>safediv</code> function from <a href="cs141/./types.html#parametrised-data-types">previously</a>:</p>
<pre><code class="language-haskell">data Maybe a = Nothing | Just a

safediv :: Int -&gt; Int -&gt; Maybe Int
safediv _ 0 = Nothing
safediv x y = Just (x `div` y)

divAndAdd :: Int -&gt; Int -&gt; Maybe Int
divAndAdd x y = 5 + safediv x y -- doesn't work, type error

-- using a case statement
divAndAdd x y = case safediv x y of
  Nothing -&gt; Nothing
  Just r -&gt; Just (5+r)
-- bit messy
</code></pre>
<p>The pattern of applying a function a value within a <code>Maybe</code> can be generalise. Defining a function <code>pam</code> to do this for us:</p>
<pre><code class="language-haskell">pam :: (a -&gt; b) -&gt; Maybe a -&gt; Maybe b
pam _ Nothing = Nothing
pam f (Just x) = Just (f x)

-- much nicer!
divAndAdd :: Int -&gt; Int -&gt; Maybe Int
divAndAdd x y = pam (5+) (safediv x y)
</code></pre>
<p>It would be nice if there was some way to generalise the pattern of applying a function to element(s) in a container. The <code>Functor</code> typeclass does this for us. A type is a functor if we can apply a function to it. Lists are functors, as that is what the <code>map</code> function does. <code>Maybe</code> and <code>BinTree</code>s are also functors.</p>
<pre><code class="language-haskell">class Functor f where
  fmap :: (a -&gt; b) -&gt; f a -&gt; f b

instance Functor [] where
  fmap = map

instance Functor Maybe where
  fmap f Nothing = Nothing
  fmap f (Just x) = Just (f x)

instance Functor BinTree where
  fmap f (Leaf x) = Leaf (f x)
  fmap f (Node lr ) = Node (fmap f l) (fmap f r)
</code></pre>
<p>Functors can be thought of as &quot;boxes&quot;, and when given a function, will apply it to the value in the box, and return the result in the same box. Some examples of definitions using functors:</p>
<pre><code class="language-haskell">-- increases all Ints in the &quot;box&quot; by 5
incByFive :: Functor f =&gt; f Int -&gt; f Int
incByFive = fmap (+5)

-- applies the odd function to all Ints in the box
odds :: Functor f =&gt; f Int -&gt; f Bool
odds = fmap odd

-- redefining using fmap
divAndAdd :: Functor f =&gt; Int -&gt; Int -&gt; Maybe Int
divAndAdd x y = fmap (5+) (safediv x y)
</code></pre>
<p>Functor is also another typeclass that can be derived by GHC, using the <code>-XDeriveFunctor</code> extension.</p>
<h3 id="the--operator-1"><a class="header" href="#the--operator-1">The <code>&lt;\$&gt;</code> Operator</a></h3>
<p>An operator that is essentially just an infix version of the <code>fmap</code> function.</p>
<pre><code class="language-haskell">infixl 4 &lt;\$&gt;
(&lt;\$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
(&lt;\$&gt;) = fmap

fmap (replicate 6) (safediv 8 4)
== replicate 6 &lt;\$&gt; safediv 8 4
=&gt; Just [2,2,2,2,2,2]


-- redefining using &lt;\$&gt;
divAndAdd :: Functor f =&gt; Int -&gt; Int -&gt; Maybe Int
divAndAdd x y = (5+) &lt;\$&gt; (safediv x y)

</code></pre>
<h3 id="functor-laws"><a class="header" href="#functor-laws">Functor Laws</a></h3>
<p>There are certain laws that functors must obey for their properties to hold. A type <code>f</code> is a functor if there exists a function <code>fmap :: (a-&gt; b) -&gt; f a -&gt; f b</code> , and the following laws hold for it:</p>
<ul>
<li><code>fmap id = id</code>
<ul>
<li>If the values in the functor are mapped to themselves, the result will be an unmodified functor</li>
</ul>
</li>
<li><code>fmap (f.g) = (fmap f) . (fmap g)</code>
<ul>
<li>The fusion law</li>
<li>If two <code>fmap</code>s are applied one after the other, the result must be the same as a single <code>fmap</code> which applies the two functions in turn</li>
</ul>
</li>
<li>These laws imply that a data structure's &quot;shape&quot; does not change when <code>fmap</code>ped</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="applicative-functors"><a class="header" href="#applicative-functors">Applicative Functors</a></h1>
<h2 id="kinds"><a class="header" href="#kinds">Kinds</a></h2>
<ul>
<li>For the compiler to accept a program, it must be well typed</li>
<li><em>Kinds</em> are the &quot;types of types&quot;</li>
<li>Types are denoted with <code>expression :: type</code>
<ul>
<li>eg <code>True :: Bool</code></li>
</ul>
</li>
<li>Kinds are denoted the same: <code>type :: kind</code>
<ul>
<li><code>Bool :: *</code></li>
</ul>
</li>
<li>The compiler infers kinds of types the same way it infers types of expressions</li>
<li><code>*</code> is the kind of types</li>
<li><code>Bool :: *</code> because <code>Bool</code> has no type parameters
<ul>
<li><code>data Bool = True | False</code></li>
</ul>
</li>
<li><code>Maybe</code> is parametrised over some type <code>a</code>, so the kind signature <code>Maybe :: * -&gt; *</code> means that if given a type as an argument to the type constructor <code>Just</code>, it will give back some other type of kind <code>*</code></li>
<li><code>[] :: * -&gt; *</code>
<ul>
<li><code>[]</code> is the type constructor for lists</li>
</ul>
</li>
</ul>
<p>Kinds are important when defining typeclasses. Take <code>Functor</code>, for example:</p>
<pre><code class="language-haskell">class Functor f where
  fmap :: (a -&gt; b) -&gt; f a-&gt; f b
</code></pre>
<p>This definition shows that the type <code>f</code> is applied to one argument (<code>f a</code>), so <code>f :: * -&gt; *</code></p>
<pre><code class="language-haskell">-- Maybe :: * -&gt; *
instance Functor Maybe where
  fmap f Nothing = Nothing
  fmap f (Just x) = Just (f x)

-- invalid
-- Maybe a :: *
-- As the type is already applied to a
instance Functor (Maybe a) where
  fmap f Nothing = Nothing
  fmap f (Just x) = Just (f x)
</code></pre>
<h2 id="the-either-type"><a class="header" href="#the-either-type">The <code>Either</code> Type</a></h2>
<p>Either is usually used to represent the result of a computation when it could give one of two results. <code>Right</code> is used to represent success, and <code>a</code> is the wanted value. <code>Left</code> is used to represent error, with <code>e</code> as some error code/message.</p>
<pre><code class="language-haskell">data Either e a = Left e | Right a
Left :: e -&gt; Either e a
Right :: a -&gt; Either e a
</code></pre>
<p><code>Either</code> has kind <code>* -&gt; * -&gt; *</code>, as it must be applied to two types <code>e</code> and <code>a</code> before we get some other type.</p>
<p>Only types of kind <code>* -&gt; *</code> can be functors, so we need to apply <code>Either</code> to one argument first. The functor instance for <code>Either</code> applies the function to the <code>Right</code> value.</p>
<pre><code class="language-haskell">instance Functor (Either e) where
  fmap :: (a -&gt; b) -&gt; Either e a -&gt; Either e b
  fmap f (Left x)  = Left x
  fmap f (Right y) = Right (f y)
</code></pre>
<h2 id="the-unit-type-"><a class="header" href="#the-unit-type-">The Unit Type <code>()</code></a></h2>
<ul>
<li><code>()</code> is called the unit type</li>
<li><code>() :: ()</code>
<ul>
<li><code>()</code>, the unit value, has type <code>()</code></li>
<li><code>()</code> is the only value of type <code>()</code></li>
</ul>
</li>
<li>Can be thought of as defined <code>data () = ()</code></li>
<li>Or an empty tuple</li>
</ul>
<h2 id="semigroups-and-monoids"><a class="header" href="#semigroups-and-monoids">Semigroups and Monoids</a></h2>
<p>A type is a semigroup if it has some associative binary operation defined on it. This operator <code>(&lt;&gt;)</code> is the &quot;combine&quot; operator.</p>
<pre><code class="language-haskell">class Semigroup a where
  (&lt;&gt;) :: a -&gt; a -&gt; a

instance Semigroup [a] where
  -- (&lt;&gt;) :: [a] -&gt; [a] -&gt; [a]
  (&lt;&gt;) = (++)

instance Semigroup Int where
  -- (&lt;&gt;) :: Int -&gt; Int -&gt; Int
  (&lt;&gt;) = (+)
</code></pre>
<p>A type is a monoid if it is a semigroup that also has some identity value, called <code>mempty</code>:</p>
<pre><code class="language-haskell">class Semigroup a =&gt; Monoid a where
  mempty ::a

instance Monoid [a] where
  -- mempty :: [a]
  mempty = []

instance Monoid Int where
  -- mempty :: Int
  mempty = 0
</code></pre>
<h2 id="applicatives"><a class="header" href="#applicatives">Applicatives</a></h2>
<p>Applicative Functors are similar to normal functors, except with a slightly different type definition:</p>
<pre><code class="language-haskell">class Functor f =&gt; Applicative f where
  pure :: a -&gt; f a
  &lt;*&gt;  :: f (a -&gt; b) -&gt; f a -&gt; f b
</code></pre>
<p>The typeclass defines two functions:</p>
<ul>
<li><code>pure</code> just lifts the value <code>a</code> into the &quot;box&quot;</li>
<li><code>&lt;*&gt;</code> (the apply operator) takes some function (<code>a -&gt; b</code>) in a box <code>f</code>, and applies it to a value <code>a</code> in a box, returning the result in the same box.
<ul>
<li>&quot;box&quot; is a rather loose analogy. It is more accurate to say &quot;computational context&quot;.</li>
</ul>
</li>
</ul>
<p>Different contexts for function application:</p>
<pre><code class="language-haskell">-- vanilla function application
(\$) :: (a -&gt; b) -&gt; a -&gt; b
-- Functor's fmap
(&lt;\$&gt;) :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
-- Applicative's apply
(&lt;*&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
</code></pre>
<p><code>Maybe</code> and <code>Either e</code> are both applicative functors:</p>
<pre><code class="language-haskell">instance Applicative Maybe where
  pure x = Just x
  Nothing &lt;*&gt; _ = Nothing
  (Just f) &lt;*&gt; x = f &lt;\$&gt; x

instance Applicative (Either e) where
  pure = Right
  Left err &lt;*&gt; _ = Left err
  Right f  &lt;*&gt; x = f &lt;\$&gt; x
</code></pre>
<p>The &quot;context&quot; of both of these types is that they represent error. All data flow in haskell has to be explicit due to its purity, so these types allow for the propagation of error.</p>
<p>Another example of an applicative functor is a list:</p>
<pre><code class="language-haskell">instance Applicative [] where
  pure x = [x]
  fs &lt;*&gt; xs = [f x | f &lt;- fs, x &lt;- xs]
</code></pre>
<p>Every function in the left list is applied to every function in the right:</p>
<pre><code class="language-haskell">[f, g] &lt;*&gt; [x, y, z]
=&gt; [f x, f y, f z, g x, g y, g z]

g &lt;\$&gt; [x,y] &lt;*&gt; [a,b,c]
=&gt; [g x, g y] &lt;*&gt; [a,b,c]
=&gt; [g x a, g x b, g x c, g y a, g y b, g y c]
</code></pre>
<p><img src="cs141/./img/list-apply.png" alt="" /></p>
<p>The context represented by lists is nondeterminism, ie a function <code>f</code> given one of the arguments <code>[x, y, z]</code> could have result <code>[f x, f y, f z]</code>.</p>
<h2 id="applicative-laws"><a class="header" href="#applicative-laws">Applicative Laws</a></h2>
<p>Applicative functors, like normal functors, also have to obey certain laws:</p>
<ul>
<li><code>pure id &lt;*&gt; x = x</code>
<ul>
<li>The identity law</li>
<li>applying pure id does nothing</li>
</ul>
</li>
<li><code>pure f &lt;*&gt; pure x = pure (f x)</code>
<ul>
<li>Homomorphism</li>
<li><code>pure</code> preserves function application</li>
</ul>
</li>
<li><code>u &lt;*&gt; pure y = pure (\$ y) &lt;*&gt; u</code>
<ul>
<li>Interchange</li>
<li>Applying something to a pure value is the same as applying pure ($ y) to that thing</li>
</ul>
</li>
<li><code>pure (.) &lt;*&gt; u &lt;*&gt; v &lt;*&gt; w = u &lt;*&gt; (v &lt;*&gt; w)</code>
<ul>
<li>Composition</li>
<li>Function composition with <code>(.)</code> works within a <code>pure</code> context.</li>
</ul>
</li>
</ul>
<h2 id="left-and-right-apply"><a class="header" href="#left-and-right-apply">Left and Right Apply</a></h2>
<p><code>&lt;*</code> and <code>*&gt;</code> are two more operators, both defined automatically when <code>&lt;*&gt;</code> is defined.</p>
<pre><code class="language-haskell">const :: a -&gt; b -&gt; a
const x y = x

flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
flip f x y = f y x

(&lt;*) :: Applicative f =&gt; f a -&gt; f b -&gt; f a
a0 &lt;* a1 = const &lt;\$&gt; a0 &lt;*&gt; a1

(*&gt;) :: Applicative f =&gt; f a -&gt; f b -&gt; f b
a0 *&gt; a1 = flip const &lt;\$&gt; a0 &lt;*&gt; a1
</code></pre>
<p>In simple terms <code>*&gt;</code> is used for sequencing actions, discarding the result of the first argument. <code>&lt;*</code> is the same, except discarding the result of the second.</p>
<pre><code class="language-haskell">Just 4 &lt;* Just 8
=&gt; const &lt;\$&gt; Just 4 &lt;*&gt; Just 8
=&gt; Just (const 4) &lt;*&gt; Just 8
=&gt; Just (const 4 8)
=&gt; Just 4

Just 4 &lt;* Nothing
=&gt; const &lt;\$&gt; Just 4 &lt;*&gt; Nothing
=&gt; Just (const 4) &lt;*&gt; Nothing
=&gt; Nothing

Just 4 *&gt; Just 8
=&gt; flip const &lt;\$&gt; Just 4 &lt;*&gt; Just 8
=&gt; Just (flip const 4) &lt;*&gt; Just 8
=&gt; Just (flip const 4 8)
=&gt; Just (const 8 4)
=&gt; Just 8

Nothing *&gt; Just 8
=&gt; Nothing
</code></pre>
<p>These operators are perhaps easier to understand in terms of monadic actions:</p>
<pre><code class="language-haskell">as *&gt; bs = do as
              bs
as *&gt; bs = as &gt;&gt; bs

as &lt;* bs = do a &lt;- as
              bs
              pure a
</code></pre>
<h2 id="example-logging"><a class="header" href="#example-logging">Example: Logging</a></h2>
<p>A good example to illustrate the uses of applicative functors is logging the output of a compiler. If we have a function <code>comp</code> that takes some <code>Expr</code> type, representing compiler input, and returns some <code>Program</code> type, representing output :</p>
<pre><code class="language-haskell">comp :: Expr -&gt; Program
comp (Val n) = [PUSH n]
comp (Plus l r) = comp l ++ comp r ++ [ADD]
-- extending to return a String for a log
comp :: Expr -&gt; (Program, [String])
comp (val n) = ([PUSH n],[&quot;compiling a value&quot;])
comp (Plus l r) = (pl ++ pr ++ [ADD], &quot;compiling a plus&quot; : (ml ++ mr))
  where (pl, ml) = comp l
        (pr, mr) = comp r
</code></pre>
<p>This is messy and not very clear what is going on. There is a much nicer way to do this, using the <code>Writer</code> type:</p>
<pre><code class="language-haskell">-- w is the &quot;log&quot;
-- a is the containing type (the type in the &quot;box&quot;)
data Writer w a = MkWriter (a,w)
--type of MkWriter
MkWriter :: (a,w) -&gt; Writer w a
-- kind of Writer type
Writer :: * -&gt; * -&gt; *

instance Functor (Writer w) where
  -- fmap :: (a -&gt; b) -&gt; Writer w a -&gt; Writer w b
  fmap f (MkWriter (x,o)) = MkWriter (f x, o) -- applies the function to the x value

-- a function to write a log
-- generates a new writer with a msg and unit type in it's box
writeLog :: String -&gt; Writer [w] ()
writeLog msg = MkWriter((), [msg])
</code></pre>
<p>Using this to redefine <code>comp</code>:</p>
<pre><code class="language-haskell">comp :: Expr -&gt; Writer [String] Program
comp (Val n) = MkWriter ([PUSH n], m)
  where (MkWriter (_, m)) = writeLog &quot;compiling a value&quot;
comp (Plus l r) = MkWriter (pl ++ pr ++ [ADD], m ++ ml ++ mr)
  where (MkWriter (pl, ml)) = comp l
        (MkWriter (pr, mr)) = comp r
        (MkWriter (_, m))   = writeLog
</code></pre>
<p>This definition of comp combines the output using <code>Writer</code>, but is messy as it uses pattern matching to deconstruct the results of the recursive calls and then rebuild them into the result. It would be nice if there was some way to implicitly keep track of the log messages.</p>
<p>We can define an instance of the <code>Applicative</code> typeclass for <code>Writer</code> to do this. There is the additional constraint that <code>w</code> must be an instance of <code>Monoid</code>, because we need some way to combine the output of the log.</p>
<pre><code class="language-haskell">instance Monoid w =&gt; Applicative (Writer w) where
  --pure :: a -&gt; Writer w a
  pure x = MkWriter (x, mempty)
  -- &lt;*&gt; Monoid w =&gt; Writer w (a -&gt; b) -&gt; Writer w a -&gt; Writer w b
  MkWriter (f,o1) &lt;*&gt; MkWriter (x,o2) = MkWriter (f x, o1 &lt;&gt; o2)
  -- f is applied to x, and o1 and o2 are combined using their monoid instance
</code></pre>
<p>Using this definition, the <code>comp</code> function can be tidied up nicely using <code>&lt;*&gt;</code></p>
<pre><code class="language-haskell">comp :: Expr -&gt; Writer [String] Program
comp (Val n) = writeLog &quot;compiling a value&quot; *&gt; pure [PUSH n]
comp (Plus l r) = writeLog &quot;compiling a plus&quot; *&gt;
    ((\p p' -&gt; p ++ p' ++ [ADD]) &lt;\$&gt; comp l &lt;*&gt; comp r)
</code></pre>
<p>The first pattern uses <code>*&gt;</code>. Recall that <code>*&gt;</code> does not care about the left result, which in this case is the unit type, so only the result of the right <code>Writer</code> is used, which is the <code>[PUSH n]</code> put into a <code>Writer</code> by <code>pure</code>, with a <code>mempty</code>, or <code>[]</code> as the logged value.</p>
<p>The second pattern applies the anonymous function <code>(\p p' -&gt; p ++ p' ++ [ADD])</code> to the result of the recursive calls. The lambda defines how the results of the recursive calls are combined together, and the log messages are automatically combined by the definition of <code>&lt;*&gt;</code>. <code>*&gt;</code> is used again to add a log message to the program.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monads"><a class="header" href="#monads">Monads</a></h1>
<p><strong>ṱ̴̹͙̗̣̙ͮ͆͑̊̅h̸̢͔͍̘̭͍̞̹̀ͣ̅͢e̖̠ͫ̒ͦ̅̉̓̓́͟͞ ͑ͥ̌̀̉̐̂͏͚̤͜f͚͔͖̠̣͚ͤ͆ͦ͂͆̄ͥ͌o̶̡̡̝͎͎̥͖̰̭̠̊r̗̯͈̀̚b̢͙̺͚̅͝i̸̡̱̯͔̠̲̿dͧ̈ͭ̑҉͎̮d̆̓̂̏̉̏͌͆̚͝͏̺͓̜̪͓e̎ͯͨ͢҉͙̠͕͍͉n͇̼̞̙͕̮̣͈͓ͨ͐͛̽ͣ̏͆́̓ ̵ͧ̏ͤ͋̌̒͘҉̞̞̱̲͓k͔̂ͪͦ́̀͗͘n͇̰͖̓ͦ͂̇̂͌̐ȯ̸̥͔̩͒̋͂̿͌w̞̟͔̙͇̾͋̅̅̔ͅlͧ͏͎̣̲̖̥ẻ̴̢̢͎̻̹̑͂̆̽ͮ̓͋d̴̪͉̜͓̗̈ͭ̓ͥͥ͞g͊̾̋̊͊̓͑҉͏̭͇̝̰̲̤̫̥e͈̝̖̖̾ͬ̍͢͞</strong></p>
<p>Monads are another level of abstraction on top of applicatives, and allow for much more flexible and expressive computation. Functors =&gt; Applicatives =&gt; Monads form a hierarchy of abstractions.</p>
<h2 id="the-monad-typeclass"><a class="header" href="#the-monad-typeclass">The <code>Monad</code> typeclass</a></h2>
<pre><code class="language-haskell">class Applicative m =&gt; Monad m where
  (&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b

  return :: a -&gt; m a
  return = pure
</code></pre>
<p>The <code>&gt;&gt;=</code> operator is called <em>bind</em>, and applies a function that returns a wrapped value, to another wrapped value.</p>
<ul>
<li>The left operand is some monad containing a value <code>a</code></li>
<li>the right operand is a function of type <code>a -&gt; m b</code>, ie it takes some <code>a</code> and returns a monad containing something of type <code>b</code></li>
<li>The result is a monad of type <code>b</code></li>
</ul>
<p>The operator can essentially be thought of as feeding the wrapped value into the function, to get a new wrapped value. <code>x &gt;&gt;= f</code> unwraps the value in <code>x</code> from it, and applies the function to <code>f</code> to it. Understanding bind is key to understanding monads.</p>
<p><code>return</code> is just the same as <code>pure</code> for applicatives, lifting the value <code>a</code> into some monadic context.</p>
<p>Some example monad instances:</p>
<pre><code class="language-haskell">instance Monad Maybe where
  Nothing &gt;&gt;= _ = Nothing
  Just x  &gt;&gt;= f = f x

instance Monad (Either e) where
  Left l &gt;&gt;= _ = Left l
  Right r &gt;&gt;= f = f r

  pure = Right

instance Monad [] where
  xs &gt;&gt;= f = concat (map f xs)
</code></pre>
<p>Monads give effects: composing computations sequentially using <code>&gt;&gt;=</code> has an effect. With the <code>State</code> Monad this effect is &quot;mutation&quot;. With <code>Maybe</code> and <code>Either</code> the effect is that we may raise a failure at any step. Effects only happen when we want them, implemented by pure functions.</p>
<h2 id="monad-laws"><a class="header" href="#monad-laws">Monad Laws</a></h2>
<p>For a type to be a monad, it must satisfy the following laws:</p>
<ul>
<li><code>return a &gt;&gt;= h = h a</code>
<ul>
<li>Left identity</li>
</ul>
</li>
<li><code>m &gt;&gt;= return = m</code>
<ul>
<li>Right identity</li>
</ul>
</li>
<li><code>(m &gt;&gt;= f) &gt;&gt;= g = m &gt;&gt;= (\x -&gt; f x &gt;&gt;= g)</code>
<ul>
<li>Associativity</li>
</ul>
</li>
</ul>
<h2 id="example-evaluating-an-expression"><a class="header" href="#example-evaluating-an-expression">Example: Evaluating an Expression</a></h2>
<p>A type <code>Expr</code> is shown below that represents a mathematical expression, and an <code>eval</code> function to evaluate it. Note that it is actually unsafe and could crash at runtime due to a div by 0 error. The <code>safediv</code> function does this using <code>Maybe</code>.</p>
<pre><code class="language-haskell">data Expr = Val Int | Add Expr Expr | Div Expr Expr

eval :: Expr -&gt; Int
eval (Val n)   = n
eval (Add l r) = eval l + eval r
eval (Div l r) = eval l `div` eval r

safediv :: Int -&gt; Int -&gt; Maybe Int
safediv x 0 = Nothing
safediv x y = Just (x `div` y)
</code></pre>
<p>If we want to use <code>safediv</code> with <code>eval</code>, we need to change it's type signature. The updated <code>eval</code> is shown below using applicatives to write the function cleanly and propagate any errors:</p>
<pre><code class="language-haskell">eval :: Expr -&gt; Maybe Int
eval (Val n) = Just n
eval (Add l r) = (+) &lt;\$&gt; eval l &lt;*&gt; eval r
eval (Div l r) = safediv &lt;\$&gt; eval l &lt;*&gt; eval r
</code></pre>
<p>If any recursive calls return a <code>Nothing</code>, the entire expression will evaluate to <code>Nothing</code>. Otherwise, the <code>&lt;\$&gt;</code> and <code>&lt;*&gt;</code> will evaluate the expression within the <code>Maybe</code> context. However, this is still wrong as the last expression now has type of <code>Maybe (Maybe Int)</code>. This can be fixed using <code>&gt;&gt;=</code>. Note the use of lambdas.</p>
<pre><code class="language-haskell">eval (Div l r) = eval l &gt;&gt;= \x -&gt;
                 eval r &gt;&gt;= \y -&gt;
                 x `safediv` y
</code></pre>
<p>The <code>Expr</code> type can be extended to include a conditional expression, where If Condition True False`.</p>
<pre><code class="language-haskell">data Expr = Val Int
          | Add Expr Expr
          | Div Expr Expr
          | If Expr Expr Expr

eval :: Expr -&gt; Maybe Int
eval (Val n)    = Just n
eval (Add l r)  = eval l &gt;&gt;= \x -&gt;
                  eval r &gt;&gt;= \y -&gt;
                  Just (x+y)
eval (Div l r)  = eval l &gt;&gt;= \x -&gt;
                  eval r &gt;&gt;= \y -&gt;
                  x `safediv` y
eval (If c t f) = ifA &lt;\$&gt; eval c &lt;*&gt; eval t &lt;*&gt; eval f
  where ifA b x y = if b /= 0 then x else y
</code></pre>
<p>With this definition using applicatives, both branches of the conditional branch are evaluated. If there is an error in the false branch, the whole expression will fail. Here, using bind, the semantics are correct.</p>
<pre><code class="language-haskell">eval' (If c t f) = eval' c &gt;&gt;= \b -&gt;
    if b /= 0 then eval t else eval f
</code></pre>
<h2 id="-vs-"><a class="header" href="#-vs-"><code>&lt;*&gt;</code> vs <code>&gt;&gt;=</code></a></h2>
<p>Bind is a much more powerful abstraction than apply:</p>
<pre><code class="language-haskell">&lt;*&gt;  :: m (a -&gt; b) -&gt; m a -&gt; m b
(&gt;&gt;=) :: m a -&gt; (a -&gt; m b) -&gt; m b
</code></pre>
<ul>
<li>Apply operates on functions already inside a context
<ul>
<li>This function can't determine anything to do with the context</li>
<li>With a <code>Maybe</code>, it can't determine if the overall expression returns <code>Nothing</code> or not</li>
</ul>
</li>
<li>Bind takes a function that returns a context, and can therefore can determine more about the result of the overall expression
<ul>
<li>It knows if it's going to return <code>Nothing</code></li>
</ul>
</li>
</ul>
<h2 id="do-notation"><a class="header" href="#do-notation"><code>do</code> Notation</a></h2>
<p>Notice the pattern of <code>&gt;&gt;=</code> being used with lambdas a fair amount. This can be tidied up with some nice syntactic sugar, called <code>do</code> notation. Rewriting the earlier example:</p>
<pre><code class="language-haskell">eval :: Expr -&gt; Maybe Int
eval (Val n)   = return n
eval (Add l r) = do
    x &lt;- eval l
    y &lt;- eval r
    return (x+y)
eval (Div l r) = do
    x &lt;- eval l
    y &lt;- eval r
    x `safediv` y
</code></pre>
<p>This looks like imperative code, but is actually using monads behind the scenes. The arrows bind the results of the evaluation to some local definition, which can then be referred to further down the block.</p>
<ul>
<li>A block must always end with a function call that returns a monad -
<ul>
<li>usually <code>return</code>, but <code>safediv</code> is used too</li>
</ul>
</li>
<li>If any of the calls within the <code>do</code> block shown returns <code>Nothing</code>, the entire block will short-circuit to a <code>Nothing</code>.</li>
</ul>
<h2 id="example-the-writer-monad"><a class="header" href="#example-the-writer-monad">Example: The <code>Writer</code> Monad</a></h2>
<p>The example of <code>Writer</code> as an <a href="cs141/./cs141/applicatives#example-logging">applicative</a> instance can be extended to make it a <code>Monad</code> instance.</p>
<pre><code class="language-haskell">data Writer w a = MkWriter (a,w)

instance Functor (Writer w) where
  -- fmap :: (a -&gt; b) -&gt; Writer w a -&gt; Writer w b
  fmap f (MkWriter (x,o)) = MkWriter(f x, o)

instance Monoid w =&gt; Applicative (Writer w) where
  -- pure :: Monoid w =&gt; a -&gt; Writer w a
  pure x = MkWriter (x, mempty)
  -- &lt;*&gt; :: Monoid w =&gt; Writer w (a -&gt; b) -&gt; Writer w a -&gt; Writer w b
  MkWriter (f,o1) &lt;*&gt; MkWriter (x,o2) = MkWriter (f x, o1 &lt;&gt; o2)

instance Monoid w =&gt; Monad (Writer w) where
  -- return :: Monoid w =&gt; a -&gt; Writer w a
  return = MkWriter (x, mempty) --pure
  (Writer (x, o1)) &gt;&gt;= f = MkWriter (y, o2 &lt;&gt; o1)
                          where (MkWriter (y,o2)) = f x
</code></pre>
<p>Bind for <code>Writer</code> applies the function to the <code>x</code> value in the writer, then combines the two attached written values, and return the new value from the result of <code>f x</code> along with the combined values.</p>
<p>Now we have a monad instance for the <code>Writer</code> monad, we can rewrite our <code>comp</code> function with <code>do</code> notation:</p>
<pre><code class="language-haskell">comp' :: Expr -&gt; Writer [String] Program
comp' (Val n)    = do
                   writeLog &quot;compiling a value&quot;
                   pure [PUSH n]
comp' (Plus l r) = do writeLog &quot;compiling a plus&quot;
                   pl &lt;- comp l
                   pr &lt;- comp r
                   pure (pl ++ pr ++ [ADD])
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="type-level-programming"><a class="header" href="#type-level-programming">Type Level Programming</a></h1>
<p>Type level programming is about encoding more information in our types, so make them more descriptive. The more descriptive types are, the easier it is to avoid runtime errors, as the type checker can do more at compile time.</p>
<p>The GHC language extensions used here are:</p>
<ul>
<li><code>-XDataKinds</code></li>
<li><code>-XGATDs</code></li>
<li><code>-XKindSignatures</code></li>
<li><code>-XScopedTypeVariables</code></li>
<li><code>-XTypeFamilies</code></li>
</ul>
<h2 id="type-promotion"><a class="header" href="#type-promotion">Type Promotion</a></h2>
<p>As we already know, types have kinds:</p>
<ul>
<li><code>Bool :: *</code></li>
<li><code>Maybe :: * -&gt; *</code></li>
<li><code>[] :: * -&gt; *</code></li>
<li><code>State :: * -&gt; * -&gt; *</code></li>
</ul>
<p>Also recall that we have to partially apply type constructors with kinds greater than <code>* -&gt; *</code> to use them as monads:</p>
<pre><code class="language-haskell">-- Maybe :: * -&gt; *
instance Monad Maybe where
    ...

-- State :: * -&gt; * -&gt; *
instance Monad (State s) where
    ...

-- Either :: * -&gt; * -&gt; *
instance Monad Either where
    ... -- type error

instance Monad (Either e) where
    ... -- works
</code></pre>
<p><strong>Type Promotion</strong> is used to define our own kinds. The DataKinds extension allows for this. Without DataKinds, <code>data Bool = True | False</code> gives us two constructors, <code>True</code> and <code>False</code>. At the three levels in haskell:</p>
<ul>
<li>At the kind-level: <code>*</code></li>
<li>At the type-level <code>Bool</code></li>
<li>At the value-level: <code>True</code> or <code>False</code></li>
</ul>
<p>With DataKinds, we also get the following two new types, both of kind <code>Bool</code>:</p>
<ul>
<li><code>'True :: Bool</code></li>
<li><code>'False :: Bool</code></li>
</ul>
<p>The value constructors <code>True</code> and <code>False</code> have been promoted to the type level as <code>'True</code> and <code>'False</code>. A new kind is introduced too, <code>Bool</code> instead of just <code>*</code>. We now have booleans at the type level.</p>
<p><strong>DataKinds promotes all value constructors to type constructors, and all type constructors to kinds.</strong></p>
<p>Another example, recursively defined natural numbers. <code>Zero</code> is 0, and <code>Succ Nat</code> is <code>Nat + 1</code>.</p>
<pre><code class="language-haskell">data Nat = Zero | Succ Nat

-- values :: types
Zero :: Nat
Succ :: Nat -&gt; Nat

-- types :: kinds
'Zero :: Nat
'Succ :: Nat -&gt; Nat
</code></pre>
<h2 id="generalised-algebraic-data-types"><a class="header" href="#generalised-algebraic-data-types">Generalised Algebraic Data Types</a></h2>
<p>GADTs allow for more expressive type definitions. Normal ADT syntax:</p>
<pre><code class="language-haskell">data Bool = True | False
-- gives two values
True :: Bool
False :: Bool
</code></pre>
<p>Usually, we define the type and its values, which yields two value constructors. With a GADT, we explicitly specify the type of each data constructor:</p>
<pre><code class="language-haskell">data Bool where
  True :: Bool
  False :: Bool

data Nat where
  Zero :: Nat
  Succ :: Nat -&gt; Nat
</code></pre>
<p>The example below defines a recursively defined <code>Vector</code> type.</p>
<pre><code class="language-haskell">-- Normally
data Vector a = Nil | Cons a (Vector a)

-- GADT
data Vector a where
  Nil  :: Vector a
  Cons :: a -&gt; Vector a -&gt; Vector a
</code></pre>
<h2 id="example-a-safe-vector"><a class="header" href="#example-a-safe-vector">Example: A Safe Vector</a></h2>
<p>The vector definition above can use another feature, called KindSignatures, to put more detail into the type of the GADT definition:</p>
<pre><code class="language-haskell">data Vector (n :: Nat) a where
  Nil :: Vector n a
  Cons :: a -&gt; Vector n a -&gt; Vector n a
</code></pre>
<p>This definition includes an <code>n</code> to encode the size of the vector in the type. <code>n</code> is a type of kind <code>Nat</code>, as defined above. The values and types were promoted using DataKinds. The type variable <code>n</code> can also be replaced with concrete types:</p>
<pre><code class="language-haskell">data Vector (n :: Nat) a where
  Nil :: Vector `Zero a
  Cons :: a -&gt; Vector n a -&gt; Vector (`Succ n) a

-- example
cakemix :: Vector ('Succ ('Succ Zero)) String
cakemix = Cons &quot;Fish-Shaped rhubarb&quot; (Cons &quot;4 large eggs&quot; Nil)
</code></pre>
<p>This further constrains the types to make the types more expressive. Now we have the length of the list expressed at type level, we can define a safer version of the head function that rejects zero-length lists at compile time.</p>
<pre><code class="language-haskell">vhead :: Vector ('Succ n) a -&gt; a
-- this case will throw an error at compile time as it doesn't make sense
vhead Nil = undefined
vhead (Cons x xs) = x
</code></pre>
<p>Can also define a zip function for the vector type that forces inputs to be of the same length. The type variable <code>n</code> tells the compiler in the type signature that both vectors should have the same length.</p>
<pre><code class="language-haskell">vzip :: Vector n a -&gt; Vector n b -&gt; Vector n (a,b)
vzip Nil Nil = Nil
vzip (Cons x xs) (Cons y ys) = Cons (x,y) (vzip xs ys)
</code></pre>
<h2 id="singleton-types"><a class="header" href="#singleton-types">Singleton types</a></h2>
<p>Singletons are types with a 1:1 correspondence between types and values. Every type has only a single value constructor. The following GADT is a singleton type for natural numbers. The <code>(n :: Nat)</code> in the type definition annotates the type with it's corresponding value at type level. The type is parametrised over <code>n</code>, where <code>n</code> is the value of the type, at type level.</p>
<pre><code class="language-haskell">data SNat (n :: Nat) where
    SZero :: SNat 'Zero
    SSucc :: Snat n -&gt; SNat ('Succ n)

-- there is only one value of type SNat 'Zero
szero :: SNat 'Zero
szero = SZero

-- singleton value for one and it's type
sone :: SNat ('Succ 'Zero)
sone = SSucc SZero

stwo :: SNat ('Succ ('Succ Zero))
sone = SSucc sone
</code></pre>
<p><strong>There is only one value of each type</strong>. The data is stored at both the value and type level.</p>
<p>This can be used to define a replicate function for the vector:</p>
<pre><code class="language-haskell">vreplicate :: SNat n -&gt; a -&gt; Vector n a
vreplicate SZero x = Nil
vreplicate (SSucc n) x = Cons x (vreplicate n x)
</code></pre>
<p>The length of the vector we want is <code>SNat n</code> at type level, which is a singleton type. This allows us to be sure that the vector we are outputting is the same size as what we told it, making sure this type checks.</p>
<h2 id="proxy-types--reification"><a class="header" href="#proxy-types--reification">Proxy Types &amp; Reification</a></h2>
<p>We are storing data at the type level, which allows us to access the data at compile time and statically check it. If we want to access that data at runtime, for example to find the length of a vector, we need a <em>proxy type</em>. Proxy types allow for turning type level data to values, ie turning a type level natural number (<code>Nat</code>) into an <code>Int</code>. Haskell has no types at runtime (due to type erasure), so proxies are a hack around this.</p>
<pre><code class="language-haskell">-- a type NatProxy parametrised over some type a of kind Nat
data NatProxy (a :: Nat) = MkProxy
-- NatProxy :: Nat -&gt; *
-- MkProxy :: NatProxy a
</code></pre>
<p>This proxy type is parametrised over some value of type <code>a</code> with kind <code>Nat</code>, but there is never actually any values of type <code>a</code> involved, the info is at the type level. <code>a</code> is a phantom type.</p>
<pre><code class="language-haskell">zeroProxy :: NatProxy 'Zero
zeroProxy = MkProxy

oneProxy :: NatProxy ('Succ 'Zero)
oneProxy = MkProxy
</code></pre>
<p>These two proxies have the same value, but different types. The <code>Nat</code> type is in the phantom type <code>a</code> at type level.</p>
<p>We can then define a type class, called <code>FromNat</code>, that is parametrised over some type <code>n</code> of kind <code>Nat</code>:</p>
<pre><code class="language-haskell">class FromNat (n :: Nat) where
  fromNat :: NatProxy n -&gt; Int
</code></pre>
<p>The function <code>fromNat</code> takes a <code>NatProxy</code>, our proxy type, and converts it to an int. Instances can be defined for the two types of <code>Nat</code> to allow us to covert the type level <code>Nat</code>s to <code>Int</code>s.</p>
<pre><code class="language-haskell">-- instance for 'Zero
instance FromNat 'Zero where
  -- fromNat :: NatProxy 'Zero -&gt; int
  fromNat _ = 0

instance FromNat n =&gt; FromNat ('Succ n) where
    fromNat _ = 1 + fromNat (MkProxy :: NatProxy n)
</code></pre>
<p>The arguments to these functions are irrelevant, as the info is in the types. The variable <code>n</code> refers to the same type variable as in the instance head, using scoped type variables. This hack allows for passing types to functions using proxies, and the converting them to values using reification.</p>
<h2 id="type-families"><a class="header" href="#type-families">Type Families</a></h2>
<p>Type families allow for performing computation at the type level. A type family can be defined to allow addition of two type-level natural numbers:</p>
<pre><code class="language-haskell">type family Add (n :: Nat) (m :: Nat) :: Nat where
  Add 'Zero m = m
  Add ('Succ n) m = 'Succ (Add n m)

-- alternatively
type family (n :: Nat) + (m :: Nat) :: Nat where
  'Zero   + m = m
  'Succ n + m = 'Succ (n + m)
</code></pre>
<p>The type family for <code>(+)</code> is whats known as a closed type family: once it's defined it cannot be redfined or added to. This type family can be used to define an append function for our vector:</p>
<pre><code class="language-haskell">vappend :: Vector n a -&gt; Vector m a -&gt; Vector (n+m) a
vappend Nil         ys = ys
vappend (Cons x xs) ys = Cons x (vappend xs ys)
</code></pre>
<p>Importing <code>GHC.TypeLits</code> allows for the use of integer literals at type level instead of writing out long recursive type definitions for <code>Nat</code>. This means we can now do:</p>
<pre><code class="language-haskell">data Vector (n :: Nat) a where
  Nil :: Vector 0 a
  Cons :: a -&gt; Vector n a -&gt; Vector (n+1) a

vappend Nil          Nil          :: Vector 0 a
vappend (Cons 4 Nil) Nil          :: Vector 1 Int
vappend (Cons 4 Nil) (Cons 8 Nil) :: Vector 2 Int
</code></pre>
<h3 id="associated-open-type-families"><a class="header" href="#associated-open-type-families">Associated (Open) Type Families</a></h3>
<p>The definition below defines a typeclass for a general collection of items:</p>
<pre><code class="language-haskell">class Collection c where
  empty :: c a
  insert :: a -&gt; c a -&gt; c a
  member :: a -&gt; c a -&gt; Bool

instance Collection [] where
  empty = []
  insert x xs = x : xs
  member x xs = x `elem` xs
</code></pre>
<p>However, the list instance will throw an error, as <code>elem</code> has an <code>Eq</code> constraint on it, while the <code>member</code> type from the typeclass doesn't. Another example, defining the red-black tree as an instance of <code>Collection</code> (the tree is defined in one of the lab sheets):</p>
<pre><code class="language-haskell">instance Collection Tree where
  empty = empty
  insert x t = insert t x
  member x t = member x t
</code></pre>
<p>This will raise two type errors, as both <code>insert</code> and <code>member</code> for the tree need <code>Ord</code> constraints, which <code>Collection</code> doesn't have.</p>
<p>To fix this, we can attach an associated type family to a type class.</p>
<pre><code class="language-haskell">class Collection c where
  type family Elem c :: *

  empty :: c
  insert :: a -&gt; c -&gt; c
  member :: a -&gt; c -&gt; Bool
</code></pre>
<p>For an instance of <code>Collection</code> for some type <code>c</code>, we must also define a case for c for a type level function <code>Elem</code>, this establishing a relation between <code>c</code> and some type of kind <code>*</code>.</p>
<p>We can now define instance for list and tree, where <code>Eq</code> and <code>Ord</code> constraints are placed in instance definition.</p>
<pre><code class="language-haskell">instance Eq a =&gt; Collection [a] where
    type Elem [a] = a

    empty = []
    insert x xs = x : xs
    member x xs = x `elem` xs


instance Ord a =&gt; Collection (L.Tree a) where
    type Elem (L.Tree a) = a

    empty      = L.Leaf
    insert x t = L.insert t x
    member x t = L.member x t
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="es191"><a class="header" href="#es191">ES191</a></h1>
<p>A (yet incomplete) collection of notes for ES191 Electrical and Electronic Circuits.<br />
This one aims to be fairly comprehensive, so let me know if you think anything is missing.
If you're looking for notes on digital logic, see <a href="es191/../cs132/logic.html">CS132</a></p>
<h2 id="other-useful-resources"><a class="header" href="#other-useful-resources">Other Useful Resources</a></h2>
<ul>
<li><a href="https://www.khanacademy.org/science/electrical-engineering/ee-circuit-analysis-topic/circuit-elements/a/ee-sign-convention">https://www.khanacademy.org/science/electrical-engineering/ee-circuit-analysis-topic/circuit-elements/a/ee-sign-convention</a></li>
<li><a href="https://spinningnumbers.org/">https://spinningnumbers.org/</a></li>
<li><a href="https://www.electronics-tutorials.ws/opamp/opamp_1.html">https://www.electronics-tutorials.ws/opamp/opamp_1.html</a></li>
</ul>
<p><img src="es191/./img/table.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="circuit-symbols-and-conventions"><a class="header" href="#circuit-symbols-and-conventions">Circuit Symbols and Conventions</a></h1>
<h2 id="circuits-model-electrical-systems"><a class="header" href="#circuits-model-electrical-systems">Circuits model electrical systems</a></h2>
<ul>
<li><strong>Voltage</strong> is work done per unit charge</li>
<li><strong>Potential difference</strong>- difference in electrical potential between two points in an electric field</li>
<li>A force used to move charge between two points in space</li>
</ul>
<p>$$V = \frac{W}{q} = \frac{dW}{dq}$$</p>
<ul>
<li>Moving charges produce an electric <strong>current</strong></li>
<li>Moving charges can do electrical work the same way moving objects do mechanical work</li>
</ul>
<p>$$I = \frac{dq}{dt}$$</p>
<ul>
<li>Electrical energy is the capacity to do electrical work</li>
<li>Electrical power is the rate at which work is done</li>
</ul>
<p>$$P = IV = \frac{dq}{dt} \cdot \frac{dW}{dq} = \frac{dW}{dt}$$</p>
<h3 id="resistance"><a class="header" href="#resistance">Resistance</a></h3>
<ul>
<li>Resistance is the opposition to the flow of current</li>
<li><strong>Ohm's Law</strong>:</li>
</ul>
<p>$$ R = \frac{V}{I} $$</p>
<ul>
<li>Resistance is also proportional to the <strong>Resistivity</strong> of the material
<ul>
<li>$l$ and $A$ are the length and area of the conductor, respectively.</li>
</ul>
</li>
</ul>
<p>$$ R = \frac{\rho \cdot l}{A}$$</p>
<h2 id="sources-and-nodes"><a class="header" href="#sources-and-nodes">Sources and Nodes</a></h2>
<p>Everything in a circuit can be modelled as either a source, or a node.</p>
<h3 id="voltage-sources"><a class="header" href="#voltage-sources">Voltage Sources</a></h3>
<p><img src="es191/img/voltage-sources.png" alt="" /></p>
<ul>
<li>DC and AC voltage sources</li>
<li>DC source has positive and negative terminals</li>
<li>Ideal voltage source has <strong>0</strong> internal resistance (infinite conductance)</li>
<li>Supplies constant voltage regardless of load
<ul>
<li>This is an assumption, is not the case in reality</li>
</ul>
</li>
</ul>
<h3 id="current-sources"><a class="header" href="#current-sources">Current Sources</a></h3>
<p><img src="es191/img/current-source.jpg" alt="" /></p>
<ul>
<li>Ideal current source has infinite resistance (0 conductance)</li>
<li>Supplies constant current regardless of load
<ul>
<li>Also an assumption</li>
<li>In reality, will have some internal resistance and therefore a maximum power limit</li>
</ul>
</li>
</ul>
<h3 id="dependant-sources"><a class="header" href="#dependant-sources">Dependant sources</a></h3>
<p><img src="es191/img/dependant-sources.jpg" alt="" /></p>
<ul>
<li>Diamond-shaped</li>
<li>Sources depend on values in other parts of the circuit</li>
<li>Model real sources more accurately</li>
</ul>
<h3 id="nodes"><a class="header" href="#nodes">Nodes</a></h3>
<p><img src="es191/img/nodes.jpg" alt="" /></p>
<p>All <strong>passive</strong> elements: generate no electrical power.</p>
<ul>
<li>Resistors provide resistance/impedance in Ohms ($\Omega$)</li>
<li>Inductors provide inductance in Henries ($L$)</li>
<li>Capacitors provide capacitance in Farads ($F$)</li>
</ul>
<p><strong><em>The voltage rise across an impedance conducting current is in opposition to the flow of current in the impedance.</em></strong></p>
<h2 id="basic-conventions"><a class="header" href="#basic-conventions">Basic Conventions</a></h2>
<p>Electrical current always flows from high to low potential.</p>
<ul>
<li>If the direction of the current in a circuit is such that it leaves the <strong>positive</strong> terminal of a voltage source and enters the <strong>negative</strong> terminal, then the voltage is designated as <strong>negative</strong></li>
<li>If the direction of the current is such that it leaves the <strong>negative</strong> and enters the <strong>positive</strong>, then the voltage is <strong>positive</strong>
<ul>
<li><em>The sign of the loop current is the terminal that it flows into</em></li>
</ul>
</li>
</ul>
<p>The power absorbed/produced by a source is $P = IV$.</p>
<ul>
<li>A voltage source is <strong>absorbing</strong> power if it is supplying a <strong>negative</strong> current</li>
<li>A voltage source is <strong>producing</strong> power if it is supplying a <strong>positive</strong> current</li>
</ul>
<p>The power dissapated in a resistor is $P = I^2R$.</p>
<h2 id="resistors-in-series-and-parallel"><a class="header" href="#resistors-in-series-and-parallel">Resistors in series and parallel</a></h2>
<p><img src="es191/img/resistors.jpg" alt="" /></p>
<p>Resistors in series: $R_t = R_1 + R_2$</p>
<p>Resistors in parallel: $\frac{1}{R_t} = \frac{1}{R_1} + \frac{1}{R_2}$</p>
<p>Resistors dissipate electrical power, so there is a <em>drop</em> in voltage accross them, in the direction of current flow. Therefore, the voltage <em>rise</em> is in <em>opposition to the direction of current</em></p>
<h2 id="voltage-dividers"><a class="header" href="#voltage-dividers">Voltage dividers</a></h2>
<p><img src="es191/img/divider.jpg" alt="" /></p>
<p>Using two resistors to divide a voltage</p>
<p>In the general case:</p>
<p>$$V_{out} = V_{in} \times \frac{Z_2}{Z_1 + Z_2}$$</p>
<h2 id="current-dividers"><a class="header" href="#current-dividers">Current Dividers</a></h2>
<p>Similar deal to voltage divider</p>
<p><img src="es191/./img/current-divider.png" alt="" /></p>
<p>$$I_{R1} = I_T \times \frac{R_2}{R_1 + R_2}$$
$$I_{R2} = I_T \times \frac{R_1}{R_1 + R_2}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nodal-analysis"><a class="header" href="#nodal-analysis">Nodal Analysis</a></h1>
<h2 id="kirchhoffs-current-law"><a class="header" href="#kirchhoffs-current-law">Kirchhoff's Current Law</a></h2>
<p><strong>The sum of currents entering a node is equal to the sum of currents leaving a node.</strong></p>
<p><img src="es191/img/kcl.png" alt="" /></p>
<p>$$ -I_1 -I_2 + I_3 + I_4 + I_5 = 0 $$
$$ I_1 + I_2 = I_3 + I_4 + I_5 $$</p>
<ul>
<li>Currents flowing into a node are denoted as negative</li>
<li>Currents flowing out of a node are denoted positive</li>
<li>The sum of currents around a node <em>must</em> always be 0</li>
</ul>
<h2 id="nodal-analysis-1"><a class="header" href="#nodal-analysis-1">Nodal Analysis</a></h2>
<p>A technique used to analyse circuits to calculate unknown quantities. Allows the voltage at each circuit node to be calculated, using KCL.</p>
<p>An important point to remember is that the bottom of any circuit diagram is ground (0V), by convention.</p>
<h3 id="steps"><a class="header" href="#steps">Steps</a></h3>
<ul>
<li>Choose 1 node as the reference node</li>
<li>Label any remaining voltage nodes $V_1, V_2, etc...$</li>
<li>Substitute any known voltages</li>
<li>Apply KCL at each unknown node to form a set of simultaneous equations</li>
<li>Solve simultaneous equations for unknowns</li>
<li>Calculate any required values (usually currents)</li>
</ul>
<p>Generally speaking, there will be a nodal equation for each node, formed using KCL, and then these equations will solve simultaneously.</p>
<h2 id="example-1"><a class="header" href="#example-1">Example</a></h2>
<p>Calculate the voltages at nodes $V_1$ and $V_2$.</p>
<p><img src="es191/./img/nodal-example-1.png" alt="" /></p>
<p>There are 4 currents at $V_1$</p>
<ul>
<li>Flowing from 15V source to $V_1$ accross 2 $\Omega$ resistor</li>
<li>Flowing from $V_1$ to ground accross 16 $\Omega$ resistor</li>
<li>Flowing between $V_1$ and $V_2$ accross 7 $\Omega$ resistor</li>
<li>5A, from current source</li>
</ul>
<p>Each current is calculated using ohm's law, which gives the following nodal equation:</p>
<p>$$\frac{V_1 - 15}{2} + \frac{V_1}{16} + 5 + \frac{V_1-V_2}{7} = 0$$</p>
<p>When the direction of each current is not known it is all assumed to be positive, and the voltage at the node is labelled as postive, with any other voltages being labelled as negative. Similar can be done for node $V_2$:</p>
<p>$$\frac{V_2 - V_1}{7} - 8 + \frac{V_2+30}{9} = 0$$</p>
<p>We now have two equations with two unknowns, which can easily be solved.</p>
<p>$$11.29 V_1 - 2.29 V_2 = 40$$
$$-9 V_1 + 16 V_2 = 294$$
$$ V_1 = 8.2V , V_2 = 23.0V$$</p>
<h2 id="admittance-matrices"><a class="header" href="#admittance-matrices">Admittance Matrices</a></h2>
<p>The system of equations above can also be represented in matrix form</p>
<h1>$$
\begin{pmatrix}
11.29 &amp; -2.29\
-9 &amp; 16
\end{pmatrix}
,
\begin{pmatrix}
V_1 \ V_2
\end{pmatrix}</h1>
<p>\begin{pmatrix}
40 \ 294
\end{pmatrix}
$$</p>
<p>This matrix equation always takes the form $Y \times V = I$.</p>
<p>$Y$ is known as the <em>Admittance Matrix</em>.</p>
<h2 id="calculating-power-dissapated"><a class="header" href="#calculating-power-dissapated">Calculating Power Dissapated</a></h2>
<p>Sometimes, it is required that the power dissapated by voltage/current sources is calculated. For example, calculate the power supplied by the current sources in the following:</p>
<p><img src="es191/img/example-2.png" alt="" /></p>
<p>KCL at node $V_1$: $2 + \frac{V_1 - V_2}{1} + \frac{V_1 - V_3}{2} = 0$</p>
<p>KCL at node $V_2$: $-3 + \frac{V_2}{4} + \frac{V_2 - V_1}{1} = 0$</p>
<p>KCL at node $V_3$: $3 + \frac{V_3}{3} + \frac{V_3 - V_1}{2} = 0$</p>
<p>$$
\begin{pmatrix}
3 &amp; -2 &amp; -1\
4 &amp; -5 &amp; 0 \
3 &amp; 0 &amp; -5
\end{pmatrix}</p>
<p>,</p>
<p>\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
-4 \ -12 \ 18
\end{pmatrix}</p>
<p>\Longrightarrow</p>
<p>\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
-3.5 \ -0.4 \ -5.7
\end{pmatrix}
$$</p>
<p>From the node voltages, the power dissapated in the sources can be calculated. In the 2A source:</p>
<p>$$ P = IV = 2 \times (0 - V_1) = 2 \times (0 - 3.5) = 7 , W$$</p>
<p>And in the 3A source:</p>
<p>$$ P = 3 \times (V_2 - V_3) = 3 \times (-0.4 + 5.7) = 15.9 , W$$</p>
<p>Note that the voltage accross the current source is always calculated as the node the current is flowing to, minus the node the current is flowing from, ie (to - from). This makes the sign correct so it is known whether the source is delivering or absorbing power. <strong>If the direction of the current source oppose the direction of the voltage rise, it will be absorbing power.</strong>.</p>
<p>If correct, the total power delivered to the circuit will equal the total dissapated. This calculation can be done to check, if you're bothered.</p>
<h2 id="dependant-sources-1"><a class="header" href="#dependant-sources-1">Dependant Sources</a></h2>
<p>Some circuits contain current/voltage sources which are dependant upon other values in the circuit. In the example below, a current $I$ is assumed between the two nodes where the dependant voltage source is.</p>
<p><img src="es191/img/example-3.png" alt="" /></p>
<p>Calculate the power dissipated by the 50 $\Omega$ resistor, and the power delivered by the current source.</p>
<p>At Node $V_1$: $\frac{V_1 - 50}{5} + \frac{V_1}{50} + I = 0$</p>
<p>At Node $V_2$: $-I + \frac{V_2}{100} -4 = 0$</p>
<p>We have two equations in 3 unknowns, so another equation is needed. Using $I_a$:</p>
<p>$$I_a = \frac{V_1 - 50}{5},;;; 10I_a = V_2 - V_1$$
These can be equated about $I_a$ to give
$$V_2 - V_1 = 2V_2 - 100$$</p>
<p>This system of equations solves to give $V_1 = 60 V$, and $V_2 = 80 V$.</p>
<p>Therefore,</p>
<ul>
<li>The power delivered by the current source $P = IV = 4 \times 80 = 320 W$</li>
<li>The power dissapated by the 50 $\Omega$ resistor is $P = \frac{V^2}{R} = \frac{60^2}{50} = 72 W$</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="mesh-analysis"><a class="header" href="#mesh-analysis">Mesh Analysis</a></h1>
<p>Achieves a similar thing to nodal analysis, using Kirchhoff's voltage law, and meshes instead of nodes.</p>
<h2 id="kirchhoffs-voltage-law"><a class="header" href="#kirchhoffs-voltage-law">Kirchhoff's Voltage Law</a></h2>
<p><strong>The sum of voltages around a closed loop always equals zero</strong></p>
<p><img src="es191/./img/KVL.png" alt="" /></p>
<p>$$ -V_1 + V_2 - V_3 - V_4 = 0$$</p>
<h3 id="sign-convention"><a class="header" href="#sign-convention">Sign convention</a></h3>
<ul>
<li>If voltage rise and current in a voltage source are in the <em>same direction</em>, the voltage is denoted as <em>negative</em></li>
<li>If voltage rise and current are in <em>opposite direction</em>, voltage is <em>positive</em></li>
<li>In a resistor, current <em>opposes</em> voltage rise</li>
</ul>
<h2 id="steps-1"><a class="header" href="#steps-1">Steps</a></h2>
<ul>
<li>Identify meshes (loops) (<strong>always clockwise</strong>) and assign currents $I_1, I_2,$ etc to those loops</li>
<li>Apply KVL to each mesh to generate system of equations</li>
<li>Solve equations</li>
</ul>
<p>Where there are elements that are part of multiple meshes, subtract the currents of the other meshes from the mesh currently being considered to consider the total current through that circuit element.</p>
<h2 id="example-2"><a class="header" href="#example-2">Example</a></h2>
<p><img src="es191/./img/kvl-1.png" alt="" /></p>
<p>There are three meshes in this circuit, labelled $I_1$, $I_2$, $I_3$.</p>
<p>For $I_1$:
$$ -50 + 70I_1 + 20(I_1 - I_2) + 30(I_1 - I_3) + 40 I_1 = 0$$</p>
<p>For $I_2$:
$$20(I_2 - I_1) + 100I_2 + 80 I_2 + 10(I_2 - I_3) = 0$$</p>
<p>For $I_3$:
$$30(I_3 - I_1) + 10(I_3 - I_2) + 60I_3 + 90I_3 = 0 $$</p>
<p>This forms a system of equations:</p>
<p>$$
160 I_1 - 20 I_2 - 30 I_3 = 50 \
-20 I_1 + 210 I_2 - 10 I_3 = 0 \
-30 I_1 - 10 I_2 + 190 I_3 = 0 \
$$</p>
<p>Solving yields $I_1 = 325, mA$, $I_2 = 34, mA$, and $I_3 = 53,mA$.</p>
<h3 id="impedance-matrices"><a class="header" href="#impedance-matrices">Impedance Matrices</a></h3>
<p>Similar to how systems of equations from nodal analysis form admittance matrices, mesh analysis forms impedance matrices which describe the circuit being analysed. The matrix equation takes the form $Z \cdot I = V$. As an example, the matrix equation for the system above is:</p>
<p>$$</p>
<p>\begin{pmatrix}
160 &amp; -20 &amp; -30\
-20 &amp; 210 &amp; -10 \
-30 &amp; -10 &amp; 190
\end{pmatrix}</p>
<p>,</p>
<p>\begin{pmatrix}
I_1 \ I_2 \ I_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
50 \ 0 \ 0
\end{pmatrix}</p>
<p>$$</p>
<p>Therefore, the impedance matrix for the system is:</p>
<p>$$
Z =
\begin{pmatrix}
160 &amp; -20 &amp; -30\
-20 &amp; 210 &amp; -10 \
-30 &amp; -10 &amp; 190
\end{pmatrix}
$$</p>
<h2 id="another-example"><a class="header" href="#another-example">Another Example</a></h2>
<p>Determine the currents in the circuit shown below:</p>
<p><img src="es191/./img/kvl-2.png" alt="" /></p>
<p>Loop 1:
$$-10 + 10I_1 + 5(I_1 - I_2) = 0$$</p>
<p>Loop 2:
$$5(I_2 - I_1) + 20(I_2 - I_3) + V + 15I_2 = 0$$</p>
<p>Where there is a current source, a voltage $V$ is assumed accross it.</p>
<p>Loop 3:
$$2I_3 - 20 + 20(I_3 - I_2) = 0 $$</p>
<p>There are now 3 equations with 4 unknowns. However, it can be seen from the diagram that $I_2 = -4, A$ (the direction of the current source opposes our clockwise current), so the system can be solved as follows:</p>
<p>$$I_2 = -4 , A$$
$$I_1 = \frac{10 + 5 I_2}{15} = -0.67 , A$$
$$I_3 = \frac{20 + 20 I_2}{22} = -2.73 , A$$</p>
<h2 id="example-with-dependant-sources"><a class="header" href="#example-with-dependant-sources">Example with dependant sources</a></h2>
<p>Calculate the power dissapated in the 4 $\Omega$ resistor and the power delivered/absorbed by the current dependant voltage source.
<img src="es191/./img/kvl-3.png" alt="" /></p>
<p>KVL round $I_1$:</p>
<p>$$I_1 + 4(I_1 - I_3) + 5(I_1 - I_2) = 0$$</p>
<p>KVL round $I_2$:</p>
<p>$$5(I_2 - I_1) + 20(I_2 - I_3) - 50  = 0$$</p>
<p>KVL round $I_3$:</p>
<p>$$15I_a + 20(I_3 - I_2) + 4 (I_3 - I_1) = 0$$</p>
<p>$I_a = I_2 - I_3$, so this can be substituted into equation 3 to obtain a fourth equation:</p>
<p>$$
15(I_2 - I_3) + 20(I_3 - I_2) + 4(I_3 - I_1) = 0
$$</p>
<p>The system of equations then solves:</p>
<p>$$
\begin{pmatrix}
10 &amp; -5 &amp; -4\
-5 &amp; 25 &amp; -20 \
-4 &amp; -5 &amp; 9
\end{pmatrix}</p>
<p>,</p>
<p>\begin{pmatrix}
I_1 \ I_2 \ I_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
0 \ 50 \ 0
\end{pmatrix}</p>
<p>\Longrightarrow</p>
<p>\begin{pmatrix}
I_1 \ I_2 \ I_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
26 \ 29.6 \ 28
\end{pmatrix}
$$</p>
<p>The power dissapated in the 4 $\Omega$ resistor:
$$P = I^2R = 2^2 \times 4 = 16 W$$</p>
<p>The power delivered/absorbed by the dependant voltage source:
$$P = IV = 15I_a \times I_2 = 15(29.6-28) \times 28 = 672 W (absorbing)$$
The source is absorbing power as the current $I_2$ opposes the direction of voltage rise in the source.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thevenin-and-norton-equivalent-circuits"><a class="header" href="#thevenin-and-norton-equivalent-circuits">Thevenin and Norton Equivalent Circuits</a></h1>
<p>Thevenin's Theorem states that as far as its appearance from outside is concerned, any two terminal network of resistors and energy sources can be replaced by a series combination of an ideal voltage source V and a resistor R, where V is the open-circuit voltage of the network and R is the resistance that would be measured between the output terminals if the energy sources were removed and replaced by their internal resistance.</p>
<p>In practice, this can be used for reducing complex circuits to a more simple model: taking networks of resistors/impedances and reducing them to a simple circuit of one source and one resistance.</p>
<ul>
<li>Thevenin circuits contain a single voltage source and resistor in series</li>
<li>Norton circuits contain a single current source and a resistor in parallel</li>
</ul>
<h2 id="calculating-equivalent-circuits"><a class="header" href="#calculating-equivalent-circuits">Calculating Equivalent Circuits</a></h2>
<p>Any linear network viewed through 2 terminals is replaced with an equivalent single voltage &amp; resistor.</p>
<ul>
<li>The equivalent voltage is equal to the open circuit voltage between the two terminals ($V_{oc}$/$V_{th}$)</li>
<li>The equivalent resistance ($R_{th}$) is found by replacing all sources with their internal impedances and then calculating the impedance of the network, as seen by the two terminals.
<ul>
<li>This can be done alternatively by calculating the short circuit current ($I_{sc}$/$I_N$) between the two terminals, and then using ohms law: $R_{th} = \frac{V_{th}}{I_{sc}}$.</li>
</ul>
</li>
<li>The value of the voltage source in a Thevenin circuit is $V_{th}$</li>
<li>The value of the current source in a Norton circuit is $I_{N}$</li>
<li>The value of the resistor in either circuit is $R_{th}$</li>
</ul>
<p><img src="es191/./img/thve-1.png" alt="" /></p>
<p>Often, nodal/mesh analysis is needed to determine the open circuit voltage and/or short circuit current.</p>
<h2 id="maximum-power-transfer"><a class="header" href="#maximum-power-transfer">Maximum Power Transfer</a></h2>
<p>For the maximum power transfer between a source and a load resistance in a Thevenin circuit, the load resistance must be <em>equal to the thevenin resistance $R</em>{th}$_. This can be trivially proved, and is left as an exercise to the reader.</p>
<h2 id="example-1-1"><a class="header" href="#example-1-1">Example 1</a></h2>
<p>Determine the Thevenin equivalent of the following:</p>
<p><img src="es191/./img/thev-2.png" alt="" /></p>
<p>The open circuit voltage accross the two terminals can be calculated using the voltage divider rule, as the two resistors $R_1$ and $R_2$ split the voltage.</p>
<p><img src="es191/./img/thev-3.png" alt="" /></p>
<p>$$V_{oc} = V_{th} = 30 \times \frac{10k}{10k + 10k} = 15 , V$$</p>
<p>The short circuit current can be calculated by nodal analysis. When calculating the short circuit current, it is assumed that the two terminals are connected (shorted), so current can flow between them.</p>
<p><img src="es191/./img/thev-4.png" alt="" /></p>
<p>KCL at the node labelled V:</p>
<p>$$\frac{V-30}{10k} + \frac{V}{10k} + \frac{V}{10k} = 0$$
$$V = 10 , V$$</p>
<p>The voltage when the terminals are shorted is 10 V, so the short circuit current can be calculated using ohm's law:</p>
<p>$$I_{sc} = \frac{V}{R_2} = 1,mA$$</p>
<p>Which gives</p>
<p>$$R_{th} = \frac{V_{th}}{I_{sc}} = \frac{15, V}{1 , mA} = 15 , k\Omega$$</p>
<p>The resistance can alternatively be calculated by replacing the voltage source with it's internal resistance (0), and then determining the overall resistance of the network:</p>
<p><img src="es191/./img/thev-5.png" alt="" /></p>
<p>$$ R_{th} = R_2 + (R_1 || R_3) = R_2 + \frac{R_1 \cdot R_3}{R_1 + R_3} = 15 , k\Omega$$</p>
<p>The resulting Thevenin circuit is therefore:</p>
<p><img src="es191/./img/thev-6.png" alt="" /></p>
<h2 id="example-2-1"><a class="header" href="#example-2-1">Example 2</a></h2>
<p>Find the Thevenin equivalent circuit of the the network as seen by the two terminals A &amp; B, and therefore the power dissapated/absorbed by the 12V source.</p>
<p><img src="es191/./img/thev-7.png" alt="" /></p>
<h3 id="open-circuit"><a class="header" href="#open-circuit">Open Circuit</a></h3>
<p><img src="es191/./img/thev-8.png" alt="" /></p>
<p>Doing nodal analysis to determine voltages:</p>
<p>$V_1$:
$$-4.8  + \frac{V_1}{7.5} + \frac{V_1 - V_2}{2.5} = 0$$
$$4V_1 - 3 V_2 = 36$$
$V_2$:
$$\frac{V_2 - V_1}{2.5} + \frac{V_2}{10} + I = 0$$
$V_3$:
$$-I + \frac{V_3}{2.5} = 0$$</p>
<p>Combining 2 &amp; 3 by cancelling the assumed current $I$:</p>
<p>$$\frac{V_2 - V_1}{2.5} + \frac{V_2}{10} + \frac{V_3}{2.5} = 0$$
$$-4 V_1 + 5 V_2 +4 V_3 = 0 $$</p>
<p>Using $I_x$ to generate another equation:</p>
<p>$$I_x = V_1 - 7.5$$
$$V_3 - V_2 = I_x$$
$$V_1 + 7.5V_2 - 7.5V_3 = 0$$</p>
<p>This gives a system of 3 equations in 3 unknowns which can be solved to determine the node voltages:</p>
<p>$$
\begin{pmatrix}
4 &amp; -3 &amp; 0\
-4 &amp; 5 &amp; 4 \
1 &amp; 7.5 &amp; -7.5
\end{pmatrix}</p>
<p>,</p>
<p>\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
36 \ 0 \ 0
\end{pmatrix}
$$</p>
<p>$$
\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
12.64 \ 4.86 \ 6.55
\end{pmatrix}
$$</p>
<p>$V_3$ is equal to $V_{oc}$, so $V_{th} = 6.55 , V$</p>
<h3 id="short-circuit"><a class="header" href="#short-circuit">Short Circuit</a></h3>
<p><img src="es191/./img/thev-9.png" alt="" /></p>
<p>The same nodal analysis is needed, except this time the terminals are shorted. The steps are pretty much identical.</p>
<p>$V_1$ is the exact same, $4 V_1 - 3V_3 = 36$</p>
<p>$V_2$:
$$\frac{V_2 - V_1}{2.5} + \frac{V_2}{10} + I = 0$$
$V_3$:
$$-I + \frac{V_3}{2.5} + \frac{V_3}{1} = 0$$</p>
<p>2 &amp; 3 are combined in the same way, except yielding a slightly different equation, as this time current can flow to ground from $V_3$ through the 1 \Omega Resistor.</p>
<p>$$-4V_1 + 5V_2 + 14V_3 = 0$$</p>
<p>The third equation generated using $I_x$ is also the same, $V_1 + 7.5V_2 - 7.5V_3 = 0$</p>
<p>The solution to this system is very similar to above:</p>
<p>$$
\begin{pmatrix}
4 &amp; -3 &amp; 0\
-4 &amp; 5 &amp; 14 \
1 &amp; 7.5 &amp; -7.5
\end{pmatrix}</p>
<p>,</p>
<p>\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
36 \ 0 \ 0
\end{pmatrix}
$$</p>
<p>$$
\begin{pmatrix}
V_1 \ V_2 \ V_3
\end{pmatrix}</p>
<p>=</p>
<p>\begin{pmatrix}
9.82 \ 1.1 \ 2.42
\end{pmatrix}
$$</p>
<p>The short circuit current is then calculated as:</p>
<p>$$I_{sc} = \frac{V_3}{1} = 2.42 , A$$</p>
<h3 id="solution"><a class="header" href="#solution">Solution</a></h3>
<p>The Thevenin resistance is calculated as:</p>
<p>$$R_{th} = \frac{V_{oc}}{I_{sc}} = \frac{6.55}{2.42}$$</p>
<p><img src="es191/./img/thev-10.png" alt="" /></p>
<p>The power delivered to the 12V source is therefore:</p>
<p>$$P = IV = \frac{12 - 6.55}{2.7} \times 12 = 24 , W $$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-order-rc-circuits"><a class="header" href="#first-order-rc-circuits">First Order RC Circuits</a></h1>
<p>RC circuits are those containing resistors and capacitors. First order means they can be modelled by first order differential equations</p>
<h2 id="capacitors"><a class="header" href="#capacitors">Capacitors</a></h2>
<p>Capacitors are reactive elements in circuits that store charge. They work by creating an electric field between two parallel plates seperated by a dielectric insulator.</p>
<p><img src="es191/img/capacitor.png" alt="" /></p>
<ul>
<li>When charging, the electrons between the plates separate. At full charge, all electrons will be on opposite plates.</li>
<li>When discharging, the plates discharge and the charges recombine, forming a current</li>
</ul>
<h3 id="equations"><a class="header" href="#equations">Equations</a></h3>
<p>Capacitance of a specific capacitor, where</p>
<ul>
<li>$A$ = the area of the two plates</li>
<li>$d$ = the separation of the two plates</li>
<li>$\epsilon_r$ = the relative electric permittivity of the insulator</li>
<li>$\epsilon_0$ = the permittivity of free space</li>
</ul>
<p>$$C = \frac{A \epsilon_r \epsilon_0}{d}$$</p>
<p>The charge on a capacitor is equal to the product of the capacitance and the voltage accross it:
$$Q = C \cdot V$$</p>
<p>This can be used to derive the i-v equation for a capacitor:</p>
<p>$$C = \frac{Q}{V} = \frac{\int I , dt}{V}$$
$$\int I , dt = CV$$
$$I = C , \frac{dV}{dt} $$</p>
<p>This equation is important as it shows how current leads voltage in a capacitor by a phase of $\frac{\pi}{2}$ rads.</p>
<h3 id="energy"><a class="header" href="#energy">Energy</a></h3>
<p>The energy stored in a capacitor:</p>
<p>$$W = \frac{1}{2}CV^2 = \frac{1}{2}QV = \frac{Q^2}{2C}$$</p>
<h3 id="series-and-parallel-combinations"><a class="header" href="#series-and-parallel-combinations">Series and Parallel Combinations</a></h3>
<p>Capacitance combines in series and parallel in the opposite way to resistors.</p>
<p>For capacitors in series:
$$\frac{1}{C_t} = \frac{1}{C_1} + \frac{1}{C_2}$$</p>
<p>In parallel:
$$C_t = C_1 + C_2$$</p>
<h3 id="charging-and-discharging"><a class="header" href="#charging-and-discharging">Charging and Discharging</a></h3>
<ul>
<li>When a voltage is applied to a capacitor, an electric field is formed between the two plates, and the dielectric becomes polarised.</li>
<li>As the capacitor charges, the charges in the dielectric separate which forms a displacement current. At time $t=0$, the capacitor behaves as a <strong>short circuit</strong></li>
<li>Capacitors charge exponentially, so the time at which one is fully charged is describes as time $t=\infty$. At this time, the capacitor can take no more charge, so it behaves as an <strong>open circuit</strong></li>
<li>When discharging, the displaced charges flow round the circuit back to the other side of the capacitor.</li>
<li>The charge decays exponentially over time.</li>
</ul>
<h2 id="step-response"><a class="header" href="#step-response">Step Response</a></h2>
<p>Capacitors charge and discharge at exponential rates, and there are equations which describe this response to a step input.</p>
<p><img src="es191/./img/cap-circuit.png" alt="" /></p>
<p>The step response of a charging capacitor at time $t$, assuming the switch is closed at time $t=0$:</p>
<p>$$V_c(t) = V_{in} + (V_0 - V_{in}) e^{- \frac{t}{RC}}$$</p>
<p>Equations for current can be derived from this by differentiation:</p>
<p>$$I_c(t) = C \frac{d}{dt} V_c(t)$$</p>
<p>Assuming $V_0 = 0$, the equations for current and voltage when charging at time $t$ are:</p>
<p>$$I_c(t) = I_{in} ; e^{- \frac{t}{\tau}}$$
$$V_c(t) = V_{in} (1-e^{- \frac{t}{\tau}})$$</p>
<p>Where $I_{in}$ and $V_{in}$ are the input current and voltage, respectively. Similar equations exist for discharging. Voltage at time $t$ when discharging:</p>
<p>$$V(t) = V_0e^{- \frac{t}{\tau}}$$</p>
<h3 id="time-constant"><a class="header" href="#time-constant">Time constant</a></h3>
<p>$\tau = RC$ is the <em>time constant</em> of the circuit, which describes the rate at which it charges/discharges. 1 time constant is the time in seconds for which it takes the charge of a capacitor to rise by a factor of $1- e^{-1}$ (approx 63%). As charging and discharging are exponential, a capacitor will only be fully charged when $t=\infty$. However, in practical terms, a capacitor can be considered charged at $t = 5\tau$.</p>
<h2 id="example-3"><a class="header" href="#example-3">Example</a></h2>
<p>In the circuit below, determine equations for the response of the capacitor when the switch is moved to position 2.</p>
<p><img src="es191/./img/rc-ex-1.png" alt="" /></p>
<p>$V_0$ is equal to the voltage accross the capacitor at time $t=0$, which is the same as the voltage accross the 5 $k, \Omega$ resistor. When capacitors are fully charged, they are open circuit, so it is not conducing current, making the two voltages equal.</p>
<p><img src="es191/./img/rc-ex-1.1.png" alt="" /></p>
<p>$$V_0 = 24 \times \frac{5}{3+5} = 15V$$</p>
<p>$V_{in}$ is equal to the voltage of the charging circuit as seen by the capacitor. This can be calculated as the thevenin equivalent of the circuit when the switch is in the right position.</p>
<p><img src="es191/./img/rc-ex-1.2.png" alt="" /></p>
<p>$$V_{th} = -75 \times \frac{160k}{160k + 40k} = -60 , V$$
$$R_{th} = 8k + \frac{160k \times 40k}{160k + 40k} = 40 k\Omega$$</p>
<p>The time constant of the circuit:</p>
<p>$$\tau = R_{th} \times C = 40k \times 0.5m = 0.05$$</p>
<p>Therefore:</p>
<p>$$V_c(t) = V_{th} + (V_0 - V_{th}) e^{- \frac{t}{\tau}} = -60 + 75 e^{-0.05t} , V$$</p>
<p>The current can be calculated using $I = C \frac{d}{dt} V$:
$$I_c(t) = C \frac{d}{dt} V_c(t) = 0.5m \times \frac{d}{dt}( -60 + 75 e^{-0.05t}) = -1.87e^{-0.05t} ,mA$$</p>
<h2 id="another-example-1"><a class="header" href="#another-example-1">Another Example</a></h2>
<p>For the circuit shown below:</p>
<ul>
<li>Determine Thevenin circuit as seen by capacitor in position 1</li>
<li>Calculate the time constant of the circuit for time $t &gt; 0$</li>
<li>Derive an equation for $V_c(t)$ for $t &gt; 0$</li>
<li>Calculate the time taken for the capacitor voltage to fall to zero</li>
<li>Derive an equation for $I_c(t)$ for $t &gt; 0$</li>
</ul>
<p><img src="es191/./img/rc-ex-2.png" alt="" /></p>
<h3 id="t--0"><a class="header" href="#t--0">t &lt; 0</a></h3>
<p><img src="es191/./img/rc-ex2.1.png" alt="" /></p>
<p>The Thevenin voltage of the left hand bit of the circuit can be calculated by KCL:</p>
<p>$$\frac{V_{th} - 40}{20k} + \frac{V_{th}}{60k} - 8 = 0$$
$$V_{th} = 150 , V $$</p>
<p>Calculating Thevenin resistance by summing resistances:</p>
<p>$$R_{th} = 40k + \frac{20k \times 60k}{20k + 60k} = 55 , k \Omega$$</p>
<h3 id="t--0-1"><a class="header" href="#t--0-1">t &gt; 0</a></h3>
<p><img src="es191/./img/rc-ex2.2.png.png" alt="" /></p>
<p>The Thevenin voltage of the right hand side as seen by the capacitor, using the voltage divider rule:</p>
<p>$$V_{th} = -100 \times \frac{150k}{150k + 50k} = -75 , V$$</p>
<p>Thevenin Resistance:</p>
<p>$$R_{th} = 12.5k + \frac{150k \times 50k}{150k + 50k} = 50 , k \Omega$$</p>
<p>This gives the time constant $\tau = 50k \times 0.25\mu = 12.5 , ms$</p>
<p>Deriving transient equations:</p>
<p>$$V_c(t) = V_{th} + (V_0 - V_{th}) e^{- \frac{t}{\tau}} = -75 + (150 -- 75)e ^{- \frac{t}{12.5}}$$
$$V_c(t) = -75 + 225 e ^{- 80t}$$
$$I_c(t) = C \frac{d}{dt}V_c(t) = 0.25\mu \times (-75 + 225 e ^{- 80t}) = 4.5 e^{-80t} , mA$$</p>
<p>For $V_c$ to fall to zero:
$$ -75 + 225 e ^{- 80t} = 0$$
$$e^{-80t} = \frac{1}{3}$$
$$t = -\frac{1}{80} ln(0.33333) = 13.7 , ms$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-order-rl-circuits"><a class="header" href="#first-order-rl-circuits">First Order RL Circuits</a></h1>
<p>Basically the same as RC circuits, but with inductors instead.</p>
<h2 id="inductors"><a class="header" href="#inductors">Inductors</a></h2>
<p>Inductors are reactive components, similar to capacitors. The difference is that while capacitors store energy in electric fields, inductors store it in magnetic fields. They do this with coils of wire wrapped around ferromangetic cores. Inductance is measured in Henries H and has symbol $L$.</p>
<p>Inductance can be calculated as
$$L = \frac{\mu_0 \mu_r A N^2}{l}$$
where</p>
<ul>
<li>$N$ is the number of turns in the coil</li>
<li>$l$ is the circumference of the core</li>
<li>$A$ is the cross-sectional area of the core</li>
<li>$\mu_0$ is the permeability of free space</li>
<li>$\mu_r$ is the relative permeability of the core</li>
</ul>
<h3 id="inductance"><a class="header" href="#inductance">Inductance</a></h3>
<ul>
<li>
<p>Current passing through a conductor (the coil of wire) causes a change in magnetic flux which magnetises the coil.</p>
</li>
<li>
<p>This change in flux induces an EMF (Electro-Motive Force) in any conductor within it.</p>
</li>
<li>
<p>Faraday's Law states that the magnitude of the EMF induced in a circuit is proportional to the rate of change of flux linking the circuit</p>
</li>
<li>
<p>Lenz's Law states that the direction of the EMF is such that it tends to produce a current that opposes the change of flux responsible for inducing the EMF in the first place</p>
</li>
<li>
<p>Therefore, as we attempt to magnetise an inductor with a current, it induced a back EMF while it's field charges</p>
</li>
<li>
<p>One the inductor is fully charged, the back EMF dissapears and the inductor becomes a short circuit (it is just a coil of wire, after all).
<br><br></p>
</li>
<li>
<p>When a circuit forms a single coil, the EMF induced is given by the rate of change of the flux</p>
</li>
<li>
<p>When a circuit contains many coils of wire, the resulting EMF is the sum of those produced by each loop</p>
</li>
<li>
<p>If a coil contains N loops, the induced voltage $V$ is given by the following equation, where $\Phi$ is the flux of the circuit.
$$ V = -N \frac{d\Phi}{dt}$$</p>
</li>
<li>
<p>This property, where an EMF is induced by a changing flux, is known as inductance.</p>
</li>
</ul>
<h3 id="self---inductance"><a class="header" href="#self---inductance">Self - Inductance</a></h3>
<ul>
<li>A changing current causes a changing field</li>
<li>which then induced an EMF in any conductors in that field</li>
<li>When any current in a coil changes, it induced an EMF in the coil</li>
</ul>
<p>$$V= L \frac{dI}{dt}$$</p>
<p>This equation describes the I-V relationship for an inductor. It can be derived from the equations for faraday's law and inductance.</p>
<h3 id="energy-stored"><a class="header" href="#energy-stored">Energy Stored</a></h3>
<p>The energy stored in an inductor is given by
$$W = \frac{1}{2}LI^2$$</p>
<h3 id="series--parallel-combinations"><a class="header" href="#series--parallel-combinations">Series &amp; Parallel Combinations</a></h3>
<p>Inductors sum exactly the same way as resistors do. In series:</p>
<p>$$L_t = L_1 + L_2$$</p>
<p>And in parallel:</p>
<p>$$\frac{1}{L_t} = \frac{1}{L_1} + \frac{1}{L_2}$$</p>
<h3 id="dc-conditions"><a class="header" href="#dc-conditions">DC Conditions</a></h3>
<p>The final constant values of a circuit, where current and voltage are both in a &quot;steady-state&quot; is known as DC conditions. Under DC conditions:</p>
<ul>
<li>Capacitor acts as open circuit</li>
<li>Inductor acts as short circuit</li>
</ul>
<h2 id="response-of-rl-circuits"><a class="header" href="#response-of-rl-circuits">Response of RL Circuits</a></h2>
<p>Inductors exhibit the same exponential behaviour as capacitors. In a simple first order RL circuit:</p>
<p><img src="es191/./img/rl.png" alt="" /></p>
<ul>
<li>Inductor is initially uncharged with a current at 0</li>
<li>When the circuit is switched on at time t=0, $I$ is initially 0 as the inductor is open circuit.
<ul>
<li>$V_R$ is initially 0</li>
<li>$V_L$ is initially V</li>
</ul>
</li>
<li>As the inductor energises, $I$ increases, $V_R$ increases, so $V_L$ decreases
<ul>
<li>This is where the exponential behaviour comes from</li>
</ul>
</li>
</ul>
<h3 id="equations-for-step-response"><a class="header" href="#equations-for-step-response">Equations for Step Response</a></h3>
<p>Consider the circuit above, where thw switch is closed at time t=0. KVL can be used to derive an equation for the current in the circuit over time, which is shown below:</p>
<p>$$I(t) = \frac{V_{in}}{R} + (I_0 - \frac{V_{in}}{R}) e^{-\frac{t}{\tau}}$$</p>
<p>Where the time constant $\tau = \frac{L}{R}$. The inductor voltage at time $t$ is equal to:
$$V_L(t) = (V_{in} - I_0R) e^{-\frac{t}{\tau}}$$</p>
<p>When discharging, the current at time $t$ is equal to:
$$I(t) = I_0 e^{-\frac{t}{\tau}}$$</p>
<p>Note that $\frac{V_{in}}{R}$ is equal to current $I_{in}$ / $I_{\infty}$, by ohm's law.</p>
<h2 id="rc-vs-rl-circuits"><a class="header" href="#rc-vs-rl-circuits">RC vs RL Circuits</a></h2>
<p>RC circuits and RL circuits are similar in some respects, but different in others.</p>
<h3 id="rc-equations"><a class="header" href="#rc-equations">RC Equations</a></h3>
<p>$$I = C \frac{dV}{dt}$$
$$V_{in} = IR + V_C = RC \frac{dV}{dt} + V_C$$
$$V_C(t) = V_{in} + (V_0 - V_{in})e^{-frac{t}{\tau}}$$
$$\tau = RC$$</p>
<h3 id="rl-equations"><a class="header" href="#rl-equations">RL Equations</a></h3>
<p>$$V = L \frac{dI}{dt}$$
$$V_{in} = IR + V_L = IR + L \frac{d}{dt}I_L$$
$$I_L(t) = \frac{V_{in}}{R} + (I_0 - \frac{V_{in}}{R}) e^{-\frac{t}{\tau}}$$
$$\tau = \frac{L}{R}$$</p>
<h2 id="examples-1"><a class="header" href="#examples-1">Examples</a></h2>
<p>In the circuit below, the switch is opened at time $t=0$. Find:</p>
<ul>
<li>$I(t)$ for $t &gt; 0$</li>
<li>$I_0(t)$ for $t &gt; 0$</li>
<li>$V_0(t)$ for $t &gt; 0$</li>
</ul>
<p><img src="es191/./img/rl-ex-1.png" alt="" /></p>
<h3 id="it"><a class="header" href="#it">$I(t)$</a></h3>
<p>Looking for something of the form $I_L(t) = \frac{V_{in}}{R} + (I_0 - \frac{V_{in}}{R}) e^{-\frac{t}{\tau}}$</p>
<p>In steady state, before the switch is opened, all of the current flows through the inductor as it is short circuit, meaning $I_0 = 20 , A$.</p>
<p>When the switch is opened there is no energy supplied to the circuit, so the inductor discharges through the right hand half of the circuit. The inductor can see a resistance of $R_{eq} = 2 + 10 || 40$:</p>
<p>$$R = 2 + \frac{1}{\frac{1}{10} + \frac{1}{40}} = 10 , \Omega$$</p>
<p>There is no input voltage, so:
$$I_L(t) = 0 + (I_0 - 0) e^{-\frac{t}{\tau}}$$
$$\tau = \frac{2}{10} = 0.25$$
$$I(t) = 20 e^{-5t}$$</p>
<h3 id="i_0t"><a class="header" href="#i_0t">$I_0(t)$</a></h3>
<p>This can simply be calculated using the current divider rule:</p>
<p>$$I_0(t) = -20e^{-5t} \times \frac{10}{10 + 40} = -4 e^{-5t}$$</p>
<h3 id="v_0t"><a class="header" href="#v_0t">$V_0(t)$</a></h3>
<p>Using ohm's law:</p>
<p>$$V_0(t) = I_0(t)R = 40 \times -4 e^{-5t} = -160 e^{-5t} $$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ac-circuits"><a class="header" href="#ac-circuits">AC Circuits</a></h1>
<ul>
<li>AC current is the dominant form of electricity</li>
<li>Current changes direction at a fixed frequency (usually 50~60Hz)</li>
<li>AC voltage is generated by a rotating electromagnetic field
<ul>
<li>The angular velocity of this rotation determines the frequency of the current</li>
</ul>
</li>
</ul>
<p>An instantaneous voltage $V$ in a sine wave is described by:</p>
<p>$$V = V_p , \sin(\omega t + \phi)$$</p>
<p>Where:</p>
<ul>
<li>$V_p$ is the peak voltage</li>
<li>$\omega$ is the angular frequency (rad/s)</li>
<li>$\phi$ is the phase shift (radians)</li>
<li>The period of the wave is given by $T = \frac{ 2\pi}{f}$</li>
</ul>
<p><strong>$V_p$, $\omega$ and $\phi$ define a waveform</strong></p>
<p>As current and voltage are proportional, AC current is defined in a similar way:</p>
<p>$$I = I_p , \sin(\omega t + \phi)$$</p>
<h2 id="eulers-identity-and-phasors"><a class="header" href="#eulers-identity-and-phasors">Euler's Identity and Phasors</a></h2>
<p>A phasor is a vector that describes a point in a waveform. A vector has a magnitude and a direction, which describe the amplitude $V_p$ and the phase $\phi$ of the signal, respectively. The rate at which the phasor &quot;rotates&quot; is the frequency of the signal.</p>
<p><img src="es191/./img/phasor.png" alt="" /></p>
<p>An AC phasor can be represented as a complex number.</p>
<p>$$A \sin (\omega t + \phi) = A \cos \phi + jA \sin \phi = e^{j\phi}$$</p>
<p>This formula can be used to go from anywhere on a waveform to a phasor, for example:</p>
<p>$$V = 5 \sin(\omega t + 30) = 5 e ^{30j} = 5 ;\angle 30^{\circ}$$</p>
<h2 id="reactance-and-impedance"><a class="header" href="#reactance-and-impedance">Reactance and Impedance</a></h2>
<ul>
<li>The ratio of voltage to current is a measure of how a component opposes the flow of electricity</li>
<li>In a resistor, this is resistance</li>
<li>In inductors and capacitors, this property is <em>reactance</em>, $X$, measure in ohms $\Omega$</li>
<li>Can still be used in a similar way to resistance</li>
<li>Ohm's law still applies, $V = IX$</li>
<li>Capacitative reactance $X_C = \frac{1}{\omega C}$</li>
<li>Inductive reactance $X_L = \omega L$
<ul>
<li>$\omega$ is the angular frequency of the AC current</li>
</ul>
</li>
<li>Both reactance and resistance are <em>impedances</em></li>
<li>Impedance $Z$ is also measured in ohms</li>
<li>The impedance of a component is how hard it is for current to flow through it
<ul>
<li>Impedance represents not only the magnitude of the current, but the phase</li>
</ul>
</li>
</ul>
<h2 id="inductance-1"><a class="header" href="#inductance-1">Inductance</a></h2>
<p>The voltage accross an inductor is:
$$V_L = L \frac{d}{dt}I_L$$</p>
<p>In an AC circuit:
$$I_L = I_p \sin(\omega t + \phi) = I_p ;\angle \phi = I_p e^{j \phi}$$
$$V_L = L \frac{d}{dt}I_L = L \omega I_p \cos(\omega t) =  \omega L I_p \sin(\omega t + \phi + 90^{\circ}) = \omega L I_p ;\angle (\phi + 90^{\circ})$$</p>
<p>When an AC current flows through an inductor, an impedance applies</p>
<p>$$Z_L = \frac{V_L}{I_L} = \frac{\omega L I_p ;\angle (\phi + 90^{\circ})}{I_p ;\angle \phi } = \omega L ;\angle 90^{\circ}  = j \omega L$$</p>
<p><strong>The impedance of an inductor is $j$ times its reactance:</strong>
$$Z_L = j X_L = j \omega L$$</p>
<h2 id="capacitance"><a class="header" href="#capacitance">Capacitance</a></h2>
<p>Capacitors have a similar property:
$$I_C = C \frac{d}{dt} V_c$$
$$V_C = V_p \sin(\omega t + \phi) = V_p e^{j \phi}$$
$$I_C = C \frac{d}{dt} V_p \sin(\omega t + \phi) = \omega C V_p \sin(\omega t + \phi + 90^{\circ}) $$
$$Z_C = \frac{V_L}{I_L} = \frac{V_p e^{j \phi}}{\omega C V_p e^{j(\phi + 90)}} = \frac{1}{\omega C j}$$</p>
<p><strong>Capacitive Impedance:</strong></p>
<p>$$Z_C = - j X_c  =\frac{1}{j\omega C} $$</p>
<h2 id="complex-impedance"><a class="header" href="#complex-impedance">Complex Impedance</a></h2>
<p>Impedance not only changes the magnitude of an AC current, it also changes its phase.</p>
<ul>
<li>In a capacitor, voltage leads current by a phase of 90 degrees</li>
<li>In an inductor, current leads voltage by a phase of 90 degrees
<ul>
<li>CIVIL: Capacitor I leads V, V leads I in inductor</li>
</ul>
</li>
</ul>
<p>The diagram below shows the effect of reactance on phase shift.</p>
<p><img src="es191/./img/phase.png" alt="" /></p>
<p>Consider the circuit below, containing an inductor and resistor in series. The phasor diagram shows the effect of the impedances on the voltage. The inductor introduces a phase shift of 90 degrees into the voltage.</p>
<p><img src="es191/./img/AC-RL.png" alt="" /></p>
<p>The magnitude of the voltage accross both components is:
$$V = \sqrt{(V_R)^2 + (V_L)^2} = \sqrt{(IR)^2 + (IX_L)^2} = I \sqrt{R^2 + (X_L)^2} = IZ$$
where Z is the magnitude of the impedance, $Z = |\mathbf{Z}|$</p>
<p>From the phasor diagram, the phase shift of the impedance is:
$$\phi = \tan^{-1} \frac{V_L}{R_L} = \tan^{-1} \frac{IX_L}{IR} = \tan^{-1} \frac{X_L}{R}$$</p>
<p><strong>Complex impedances sum in series and parallel in the exact same way as normal resistance.</strong></p>
<h2 id="example-1-2"><a class="header" href="#example-1-2">Example 1</a></h2>
<p>Determine the complex impedance of the following combination at 50 Hz</p>
<p><img src="es191/./img/ac-ex1.png" alt="" /></p>
<p>$$Z_T = Z_C + Z_R + Z_L = R + jX_L - jX_C = R + j(\omega L - \frac{1}{\omega C})$$
At 50Hz, the angular frequency $\omega = 2 \pi f = 314$ rad/s
$$= 200 + j(314 \times 400m - \frac{1}{314 \times 50 \mu}) = 200 + 62j \Omega$$</p>
<h2 id="example-2-2"><a class="header" href="#example-2-2">Example 2</a></h2>
<p>Determine the complex impedance and therefore the current in the following combination</p>
<p><img src="es191/./img/ac-ex2.png" alt="" /></p>
<p>Since $V = 100 \sin(250t)$, $\omega = 250$</p>
<p>$$Z_T = R - jX_C = 100 - \frac{j}{\omega C} = 100 - j\frac{1}{250 \times 10^{-4}} = 100 - 40j$$</p>
<p>The current can be calculated from the impedance using ohm's law:
$$I = \frac{V}{Z} = \frac{100}{100 - 40j} = 0.86 + 0.34j = 0.93 ,\angle 21.8^{\circ}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diodes"><a class="header" href="#diodes">Diodes</a></h1>
<p>Diodes are semiconductor devices that allow current to flow <em>only in one direction.</em> Diodes look like this:</p>
<p><img src="es191/./img/diode.png" alt="" /></p>
<p>The diagram is labelled with an anode and a cathode. The voltage drop accross the diode is from anode -&gt; cathode, and the current is conducted in the direction pointed by the really big black arrow.</p>
<p>The type's of diode's we're concerned with are silicon diodes, which have a forward voltage of about 0.7V. This is only an approximation, but is the value to use in calculations.</p>
<h2 id="iv-characteristics"><a class="header" href="#iv-characteristics">IV characteristics</a></h2>
<p>Diodes are <em>non-linear</em> components:</p>
<p><img src="es191/./img/diode-iv.png" alt="" /></p>
<ul>
<li>When current is flowing from anode to cathode, the diode is <em>forward-biased</em>, and will conduct current</li>
<li>When the current is flowing backwards (the wrong way), the diode is <em>reverse-biased</em>.</li>
<li>At a large negative voltage, the diode will break down, and start to conduct current again
<ul>
<li>Don't let the voltage get this high, you wont like what happens.</li>
</ul>
</li>
</ul>
<h3 id="forward-voltage"><a class="header" href="#forward-voltage">Forward Voltage</a></h3>
<p>For the diode to conduct, it must have a minimum voltage accross it, known as the forward voltage. This is also <em>always</em> the total voltage drop accross the diode. For a silicon diode, this is 0.7V, which is why the I-V graph does not go up from zero. The diode can be said to &quot;open&quot; or &quot;switch on&quot; at about this voltage.</p>
<ul>
<li>If there is a voltage of 0.2V accross a diode, no current will flow</li>
<li>If there is a voltage of 0.6V accross a diode, a tiny amount of current may flow</li>
<li>At &gt;0.7V, the full current will flow with no resistance.</li>
</ul>
<h2 id="example-1-3"><a class="header" href="#example-1-3">Example 1</a></h2>
<p>Find the current and the voltages accross each component in the circuit below.</p>
<p><img src="es191/./img/diode-ex1.png" alt="" /></p>
<p>By Ohm's law, the current is:</p>
<p>$$I = \frac{V_{in} - V_D}{R_t} = \frac{10 - 0.7}{100 + 300} = 23.25 , mA$$</p>
<p>Thefore, the voltages are
$$V_{300R} = 300 \times 23.25 = 6.98 , V$$
$$V_{100R} = 100 \times 23.25 = 2.32 , V$$
$$V_D = 0.7V$$</p>
<h2 id="example-2-3"><a class="header" href="#example-2-3">Example 2</a></h2>
<p>Find the current through each resistor in the circuit below.</p>
<p><img src="es191/./img/diode-ex2.png" alt="" /></p>
<p>Doing KCL around node $V_x$:</p>
<p>$$\frac{V_x - 9}{500} + \frac{V_x}{100} + \frac{V_x - 0.7}{50} = 0$$
$$V_x = 1 , V$$</p>
<p>The three currents are then:</p>
<p>$$I_{500R} = \frac{9-1}{500} = 16 , mA$$
$$I_{100R} = \frac{1}{100} = 10 , mA$$
$$I_{50R} = \frac{1 - 0.7}{50} = 6 , mA$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transistors"><a class="header" href="#transistors">Transistors</a></h1>
<p>Transistors are semiconductor devices based on P-N junctions. They have three terminals, the arrangement of which depends on the kind of transistor:</p>
<ul>
<li>Base</li>
<li>Emitter</li>
<li>Collector</li>
</ul>
<p><img src="es191/./img/teminals.png" alt="" /></p>
<p>KCL applies, meaning the currents in the transistor sum to zero:
$$I_E = I_C + I_B$$</p>
<p>Transistors, like diodes are also semiconductors, meaning there is a voltage drop of 0.7 volts between the base and the emitter. When there is no collector current, transistors behave like a diode.</p>
<p>Transistors also have a <em>current gain</em>, meaning the current flowing into the collector is related to the current flowing into the base:
$$I_C = \beta I_B$$</p>
<h2 id="npn-transistors"><a class="header" href="#npn-transistors">NPN Transistors</a></h2>
<ul>
<li>The base-emitter junction behaves like a diode</li>
<li>A base current $I_B$ only flows when the voltage $V_BE$ is sufficiently positive, ie $ \ge 0.7V$.</li>
<li>The small base current controls the larger collector current, flowing from collector to emitter</li>
<li>$I_C = \beta I_B$ - the current gain, showing how base current controlls collector current</li>
</ul>
<p>Functionally, transistors are switches that emit a current from collector to emitter dependant upon the base current.</p>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<p>For the circuit below, find the base and collector currents using a gain of $\beta = 200$.</p>
<p><img src="es191/./img/trans-ex1.png" alt="" /></p>
<p>The base current can be calculated using ohm's law, taking into account the 0.7V drop between base and emitter:
$$I_B = \frac{10 - 0.7}{185k} = 50.2 ;\mu A$$</p>
<p>As there is sufficient voltage for the transistor to be on, the collector current is therefore:
$$I_C = \beta I_B = 200 \times 50.2 ;\mu A = 10 ; mA$$</p>
<h2 id="pnp-transistors"><a class="header" href="#pnp-transistors">PNP Transistors</a></h2>
<p>The diagram at the top of the page shows the circuit symbols for both kinds of transistor. The difference between the two is the way the emitter points, which is the direction of current flow in the transistor, and also the direction of voltage drop. An NPN transistor has a forward-biased junction, whereas PNP is reverse biased. Functionally, the difference between the two is that for a PNP transistor to be &quot;on&quot;, the emitter should be at $0.7V$ higher than the base.</p>
<h3 id="example-5"><a class="header" href="#example-5">Example</a></h3>
<p>Note that this circuit uses a PNP transistor, so the base is at a lower voltage than the emitter. Also note that one of the resistors is not labelled. This is because the value of it is irrelevant, as the collector current is dependant upon the bias of the transistor.</p>
<p><img src="es191/./img/trans-ex2.png" alt="" /></p>
<p>$$I_B = \frac{20-0.7}{185k} = 104 ;\mu A$$
$$I_C = 200 \times I_B = 20.8 ; mA$$</p>
<h2 id="emitter-current"><a class="header" href="#emitter-current">Emitter Current</a></h2>
<p>Notice that in the two examples, the collector current is much larger than the base, due to the large gain on the transistor. When there is a large gain $\beta$:
$$I_E = I_C + I_B$$
$$I_E = \beta I_B + I_B \approx \beta I_B$$
$$I_E \approx \beta I_B = I_C$$</p>
<p>From the example above: $I_E = I_B + I_C = 104 ;\mu A + 20.8 ; mA = 20.9 ; mA \approx I_C$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="op-amps"><a class="header" href="#op-amps">Op Amps</a></h1>
<p>Operational Amplifiers (Op-Amps) are high-gain electronic voltage amplifiers. They have two inputs, an output, and two power supply inputs. Op amps require external power, but this is implicit so is often emitted in circuit diagrams.</p>
<p>Op amps are differential amplifiers, meaning they output an amplified signal that is proportional to the difference of the two inputs. They have a very high gain, in the range of $10^4$ to $10^6$, but this is assumed to be infinite in ideal amplifiers. The output voltage is calculated by:</p>
<p>$$V_0 = A(V_2 - V_1)$$</p>
<p><img src="es191/./img/opamp.png" alt="" /></p>
<h2 id="ideal-model"><a class="header" href="#ideal-model">Ideal Model</a></h2>
<p>An ideal model of an op amp is shown below</p>
<p><img src="es191/./img/opamp-ideal.png" alt="" /></p>
<ul>
<li>Open loop gain is infinite
<ul>
<li>The gain of the op amp when there is no positive or negative feedback</li>
</ul>
</li>
<li>Input impedance ($Z_{in}$) is infinite
<ul>
<li>Ideally, no current flows into the amplifier</li>
</ul>
</li>
<li>Output impedance ($Z_{out}$) is zero
<ul>
<li>The output is assumed to act like a perfect voltage source to supply as much current as possible</li>
</ul>
</li>
<li>Bandwith is infinite
<ul>
<li>An ideal op amp can amplify any input frequency signal</li>
</ul>
</li>
<li>Offset Voltage is zero
<ul>
<li>The output will be zero when the input and output voltage are the same</li>
</ul>
</li>
</ul>
<h2 id="ideal-circuits"><a class="header" href="#ideal-circuits">Ideal Circuits</a></h2>
<p>Op amps can be used to design inverting and non-inverting circuits.</p>
<h3 id="inverting"><a class="header" href="#inverting">Inverting</a></h3>
<ul>
<li>Negative feedback is used to create an amplifier that is stable, ie doesn't produce a massive voltage output.</li>
<li>This creates closed loop gain, which controls the output of the amplifier</li>
<li>The non-inverting input is grounded</li>
<li>The negative feedback reverses the polarity of the output voltage</li>
<li>As the output of the op amp is only a few volts, and the gain of the op amp is very high, it can be assumed that the voltage at both inputs is equal to zero volts
<ul>
<li>This creates a &quot;virtual earth&quot; at the node shown on the diagram</li>
</ul>
</li>
</ul>
<p><img src="es191/./img/inv-opamp.png" alt="" /></p>
<p>Using KCL at this node, it can be shown that:</p>
<p>$$\frac{V_{out}}{V_{in}} = \frac{ - R_F}{R_{in}}$$</p>
<p>The gain of the amplifier is set by the ratio of the two resistors.</p>
<h3 id="non-inverting"><a class="header" href="#non-inverting">Non-Inverting</a></h3>
<p>Non-inverting amplifiers don't invert the voltage output, and use input at the non-inverting terminal of the op amp instead.</p>
<p><img src="es191/./img/noninv-opamp.png" alt="" /></p>
<p>The output of the amplifier is calculated by:</p>
<p>$$\frac{V_{out}}{V_{in}} =  1 + \frac{ R_F}{R_2}$$</p>
<h2 id="op-amps-as-filters"><a class="header" href="#op-amps-as-filters">Op Amps as Filters</a></h2>
<p>Filters take AC signals as input, and amplify/attenuate them based upon their frequency.</p>
<h3 id="low-pass-filter"><a class="header" href="#low-pass-filter">Low Pass Filter</a></h3>
<p>Take a simple inverting amplifier circuit, and add a capacitor in parallel.</p>
<p><img src="es191/./img/active-lowpass.png" alt="" /></p>
<p>The gain is now a function of the input frequency, which makes the circuit a filter. The reactance of the capacitor $X_C = \frac{1}{ \omega C}$. The impedance of the capacitor and resistor in parallel:</p>
<p>$$Z = \frac{R_2 jX_C}{R_2 + j X_C} = \frac{R_2}{1 + j \omega C R_2}$$</p>
<p>The gain as a function of $j\omega$ is therefore:</p>
<p>$$A(j \omega ) = \frac{V_{out}(j \omega)}{V_{in}(j \omega)} = \frac{-Z}{R_1} = - \frac{R_2}{1 + j \omega C R_2} \times \frac{1}{R_1}$$</p>
<ul>
<li>Gain is measured in decibels</li>
<li>As the input frequency increases, gain decreases</li>
<li>At very low frequencies, the gain is constant (0dB)
<ul>
<li>The capacitor has high reactance at low frequencies, and is open circuit at very low frequencies</li>
</ul>
</li>
<li>At very high frequencies, the gain tends towards $-\infty$ dB
<ul>
<li>The capacitor has a very low reactance at high frequencies (short circuit)</li>
</ul>
</li>
</ul>
<p><img src="es191/./img/active-lowpass-graph.png" alt="" /></p>
<h3 id="cutoff-frequency"><a class="header" href="#cutoff-frequency">Cutoff Frequency</a></h3>
<p>The cutoff frequency of a filter is the point at which the gain is equal to -3 dB, which corresponds to a fall in output by a factor of $\frac{1}{\sqrt{2}}$. For the filter shown above, this is:</p>
<p>$$f_c = \frac{1}{2 \pi R_2 C}$$</p>
<h3 id="high-pass-filter"><a class="header" href="#high-pass-filter">High Pass Filter</a></h3>
<p>A high pass filter is designed in a similar way</p>
<p><img src="es191/./img/active-highpass.png" alt="" /></p>
<p>This time, the impedance of the capacitor-resistor combination is:</p>
<p>$$Z = R_1 + \frac{1}{j \omega C} = \frac{1 + j \omega C R_1}{j \omega C}$$</p>
<p>Which makes the gain:</p>
<p>$$A(j \omega ) = \frac{V_{out}(j \omega)}{V_{in}(j \omega)} = \frac{-R_2}{Z} = - R_2 \times \frac{j \omega C}{1 + j \omega C R_1}$$</p>
<p>The cutoff frequency for this filter is:</p>
<p>$$f_c = \frac{1}{2 \pi R_1 C}$$</p>
<p>Which is similar to the other one, just with the other resistor.</p>
<p><img src="es191/./img/active-highpass-graph.png" alt="" /></p>
<h2 id="voltage-transfer-characteristics"><a class="header" href="#voltage-transfer-characteristics">Voltage Transfer Characteristics</a></h2>
<ul>
<li>The voltage transfer characteristic of an amplifier shows the output voltage as a function of the input voltage</li>
<li>The output range is equal to the range of the power supplies</li>
<li>Where the slope = 0, the amplifier is saturated</li>
<li>Where the slope &gt; 0, the gain is positive</li>
<li>Where the slope &lt; 0, the gain is negative</li>
<li>When the amplifier is saturated the signal becomes distorted</li>
</ul>
<p><img src="es191/./img/opamp-vtf.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="passive-filters"><a class="header" href="#passive-filters">Passive Filters</a></h1>
<p>Op amps are <em>active filters</em> because they require power. Passive filters use passive components (Resistors, Inductors, Capacitors) to achieve a similar effect. They are constructed using a potential divider with reactive components. The diagram below shows a potential divider with two impedances, $Z_1$ and $Z_2$:</p>
<p><img src="es191/./img/passive-filter.png" alt="" /></p>
<p>$$\frac{V_{out}}{V_{in}} = \frac{Z_2}{Z_1 + Z_2}$$</p>
<h2 id="transfer-functions"><a class="header" href="#transfer-functions">Transfer Functions</a></h2>
<p>The transfer function is the ratio of input to output (see <a href="es191/../es197/transfer.html">ES197 - Transfer Functions</a> for more details.). For a passive filter, this is the ratio of output voltage to input voltage, as shown above. For a filter, this will be a function of the input waveform, $H(j \omega)$. When $Z_1$ and $Z_2$ are both identical resistors $R$:</p>
<p>$$H(j \omega) = \frac{R}{R + R} = \frac{1}{2}$$</p>
<p>However, if $Z_2$ was a capacitor $C$, $Z_2 = \frac{1}{j \omega C}$:</p>
<p>$$H(j \omega) = \frac{Z_2}{Z_1 + Z_2} = \frac{1}{1 + j \omega R C}$$</p>
<p>The gain and phase of the output are then the magnitude and argument of the transfer function, respectively:
$$|H(j \omega)| = \frac{1}{\sqrt{(1 + \omega R C)^2}}$$
$$\angle H(j \omega) = \frac{\angle 0^{\circ}}{\tan^{-1}(\omega R C)} = -\tan^{-1}(\omega R C)$$</p>
<h2 id="cutoff-frequency-1"><a class="header" href="#cutoff-frequency-1">Cutoff Frequency</a></h2>
<p>Similar to active filters, passive filters also have a cutoff frequency $f_c$. This is the point at which the power output of the circuit falls by $\frac{1}{2}$, or the output gain falls by -3dB, a factor of $\frac{1}{\sqrt{2}}$. Using the above example again (a low pass RC filter):</p>
<p>$$|H(j \omega)| = \frac{1}{\sqrt{(1 + \omega R C)^2}} = \frac{1}{\sqrt{2}}$$
$$2 = 1 + (\omega R C)^2$$
$$\omega^2 = \frac{1}{R^2C^2}$$
$$\omega = \frac{1}{RC}$$
$$f_c = \frac{1}{2\pi RC}$$</p>
<p>This is also the point at which $H(j\omega) = 1 + j$</p>
<p>The filter bandwith is the range of frequencies that get through the filter. This bandwith is 0 to $f_c$ for low pass filters, or $f_c$ and upwards for high pass.</p>
<h2 id="rc-high-pass"><a class="header" href="#rc-high-pass">RC High Pass</a></h2>
<p><img src="es191/./img/RC-high.png" alt="" /></p>
<p>$$H(j \omega) = \frac{j \omega R C}{1 + j \omega R C}$$
$$|H(j \omega)| = \frac{\omega R C}{\sqrt{(1 + \omega R C)^2}}$$
$$\angle H(j \omega) = \frac{\angle 90^{\circ}}{\tan^{-1}(\omega R C)} = 90 -\tan^{-1}(\omega R C)$$
$$f_c = \frac{1}{2\pi RC}$$</p>
<h2 id="rc-low-pass"><a class="header" href="#rc-low-pass">RC Low Pass</a></h2>
<p><img src="es191/./img/RC-low.png" alt="" /></p>
<p>$$H(j \omega) =  \frac{1}{1 + j \omega R C}$$
$$|H(j \omega)| = \frac{1}{\sqrt{(1 + \omega R C)^2}}$$
$$\angle H(j \omega) = \frac{\angle 0^{\circ}}{\tan^{-1}(\omega R C)} = -\tan^{-1}(\omega R C)$$
$$f_c = \frac{1}{2\pi RC}$$</p>
<h2 id="rl-high-pass"><a class="header" href="#rl-high-pass">RL High Pass</a></h2>
<p><img src="es191/./img/RL-high.png" alt="" /></p>
<p>$$H(j \omega) =  \frac{j \omega L }{j \omega L + R}$$
$$|H(j \omega)| = \frac{\omega L }{\sqrt{R^2 + (\omega L)^2}}$$
$$\angle H(j \omega) = 90 -\tan^{-1}(\frac{\omega L}{R})$$
$$f_c = \frac{R}{2 \pi L}$$</p>
<h2 id="rl-low-pass"><a class="header" href="#rl-low-pass">RL Low Pass</a></h2>
<p><img src="es191/./img/RL-low.png" alt="" /></p>
<p>$$H(j \omega) =  \frac{R}{j \omega L + R}$$
$$|H(j \omega)| = \frac{R}{\sqrt{R^2 + (\omega L)^2}}$$
$$\angle H(j \omega) =  -\tan^{-1}(\frac{\omega L}{R})$$
$$f_c = \frac{R}{2 \pi L}$$</p>
<h2 id="2nd-order-circuits"><a class="header" href="#2nd-order-circuits">2nd Order Circuits</a></h2>
<p>For circuits more complex than those above, to find the transfer function, either:</p>
<ul>
<li>Find a thevenin equivalent circuit, as seen from the element</li>
<li>Combine multiple elements into single impedances</li>
</ul>
<p>Note that any of the above techniques only work for <em>simple first order circuits</em>.</p>
<h3 id="example-6"><a class="header" href="#example-6">Example</a></h3>
<p><img src="es191/./img/RCR-filter.png" alt="" /></p>
<p>Using $H(j \omega) = \frac{Z_2}{Z_1 + Z_2}$, where $Z_1 = R_1$, and $Z_2 = R_2 || jX_C$:</p>
<p>$$Z_2 = \frac{\frac{R_2}{j \omega C}}{R_2 + \frac{1}{j \omega C}} = \frac{R_2}{1 + j \omega R_2 C}$$
$$H(j \omega) = \frac{Z_2}{Z_2 + Z_1} = \frac{\frac{R_2}{1 + j \omega R_2 C}}{R_1 + \frac{R_2}{1 + j \omega R_2 C}} = \frac{R_2}{R_1 + j \omega R_1 R_2 C + R_2}$$
$$|H(j \omega)| = \frac{R_2}{\sqrt{(R_1 + R_2)^2 + (\omega R_1 R_2 C)^2}}$$
$$\angle H(j \omega) = - \tan^{-1} \frac{\omega R_1 R_2 C}{R_1 + R_2}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="equations-1"><a class="header" href="#equations-1">Equations</a></h1>
<p>Below are some of the main equations that I have found useful to have on hand. </p>
<equation-table>
<table><thead><tr><th><a href="es191/equations.html#capacitors">Capacitors</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#energy-stored">Energy Stored</a></td><td>$E = \frac{1}{2}CV^2 = \frac{1}{2}QV = \frac{Q^2}{2C}$</td></tr>
<tr><td><a href="es191/equations.html#capacitor-equation">Capacitor Equation</a></td><td>$C = \frac{Q}{V}$</td></tr>
<tr><td><a href="es191/equations.html#capacitance-equation">Capacitance equation</a></td><td>$C = \frac{A \epsilon_r \epsilon_0}{d}$</td></tr>
<tr><td><a href="es191/equations.html#series-capacitors">Series Capacitors</a></td><td>$\frac{1}{C_T} = \frac{1}{C_1} + \frac{1}{C_2}$</td></tr>
<tr><td><a href="es191/equations.html#parallel-capacitors">Parallel Capacitors</a></td><td>$C_T = C_1 + C_2$</td></tr>
<tr><td><a href="es191/equations.html#current-voltage">Current-Voltage</a></td><td>$C = \frac{Q}{V} = \frac{\int I , dt}{V}$</td></tr>
<tr><td><a href="es191/equations.html#step-response">Step Response</a></td><td>$V_c(t) = V_{in} + (V_0 - V_{in}) e^{- \frac{t}{RC}}$</td></tr>
<tr><td><a href="es191/equations.html#electric-field-strength">Electric Field Strength</a></td><td>$E=\frac{F}{Q} = \frac{1}{4\pi{}\epsilon{}_0}\frac{Q}{r^2} = \frac{V}{r}$</td></tr>
<tr><td><a href="es191/equations.html#capacitor-reactance">Capacitor Reactance</a></td><td>$X_c = \frac{1}{2\pi{}fC} = \frac{1}{jwC}$</td></tr>
<tr><td><a href="es191/equations.html#flux-density">Flux Density</a></td><td>$D = \frac{flux}{area} = \frac{charge}{area} (?)$</td></tr>
<tr><td><a href="es191/equations.html#magnetic-field-strength-of-straight-current-carrying-wire">Magnetic Field Strength of Straight Current Carrying Wire</a></td><td>$B=\frac{\mu{}_0I}{2\pi{}d}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#resistors">Resistors</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#resistors-in-series">Resistors in Series</a></td><td>$R_t = R_1 + R_2$</td></tr>
<tr><td><a href="es191/equations.html#resistors-in-parallel">Resistors in Parallel</a></td><td>$\frac{1}{R_t} = \frac{1}{R_1} + \frac{1}{R_2}$</td></tr>
<tr><td><a href="es191/equations.html#voltage-divider">Voltage Divider</a></td><td>$V_{out} = V_{in} \times \frac{Z_1}{Z_1 + Z_2}$</td></tr>
<tr><td><a href="es191/equations.html#current-divider">Current Divider</a></td><td>$I_{R1} = I_T \times \frac{R_2}{R_1 + R_2}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#inductors">Inductors</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#inductors-in-series">Inductors in Series</a></td><td>$L_t = L_1 + L_2$</td></tr>
<tr><td><a href="es191/equations.html#inductors-in-parallel">Inductors in Parallel</a></td><td>$\frac{1}{L_t} = \frac{1}{L_1} + \frac{1}{L_2}$</td></tr>
<tr><td><a href="es191/equations.html#induced-voltage">Induced Voltage</a></td><td>$V = -N \frac{d\Phi}{dt}$</td></tr>
<tr><td><a href="es191/equations.html#self-inductance">Self Inductance</a></td><td>$V= L \frac{dI}{dt}$</td></tr>
<tr><td><a href="es191/equations.html#energy-stored-1">Energy Stored</a></td><td>$W = \frac{1}{2}LI^2$</td></tr>
<tr><td><a href="es191/equations.html#step-response-of-rl-circuit-current">Step Response of RL Circuit (Current)</a></td><td>$I_L(t) = \frac{V_{in}}{R} + (I_0 - \frac{V_{in}}{R}) e^{-\frac{t}{\tau}}$</td></tr>
<tr><td><a href="es191/equations.html#step-response-of-rl-circuit-voltage">Step Response of RL Circuit (Voltage)</a></td><td>$V_L(t) = (V_{in} - I_0R) e^{-\frac{t}{\tau}}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#thevenin-and-norton-equivalent-circuits">Thevenin and Norton Equivalent Circuits</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#equivalent-resistance">Equivalent Resistance</a></td><td>$R_{th} = \frac{V_{th}}{I_{sc}}$</td></tr>
<tr><td><a href="es191/equations.html#thevenin---norton-conversion">Thevenin - Norton Conversion</a></td><td>$I_N = \frac{V_{th}}{R_{th}}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#ac-circuits">AC Circuits</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#instantaneous-voltage">Instantaneous Voltage</a></td><td>$V = V_p , \sin(\omega t + \phi)$</td></tr>
<tr><td><a href="es191/equations.html#instantaneous-current">Instantaneous Current</a></td><td>$I = I_p , \sin(\omega t + \phi)$</td></tr>
<tr><td><a href="es191/equations.html#ac-phasor---as-complex-number">AC Phasor - As complex number</a></td><td>$A \sin (\omega t + \phi) = A \cos \phi + jA \sin \phi = Ae^{j\phi}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#operational-amplifiers">Operational Amplifiers</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#output-of-inverting-amplifier">Output of Inverting Amplifier</a></td><td>$\frac{V_{out}}{V_{in}} = \frac{ - R_{in}}{R_F}$</td></tr>
<tr><td><a href="es191/equations.html#output-of-non-inverting-amplifier">Output of Non-Inverting Amplifier</a></td><td>$\frac{V_{out}}{V_{in}} = 1 + \frac{ - R_{in}}{R_F}$</td></tr>
</tbody></table>
<table><thead><tr><th><a href="es191/equations.html#filters">Filters</a></th><th></th></tr></thead><tbody>
<tr><td><a href="es191/equations.html#cutoff-frequncy">Cutoff Frequncy</a></td><td>$f_c = \frac{1}{2\pi{}R_2C}$</td></tr>
<tr><td><a href="es191/equations.html#gain-db">Gain (dB)</a></td><td>$,20\log\frac{Vout}{Vin}$</td></tr>
</tbody></table>
</equation-table>
<div class="equations">
<h2 id="capacitors-1"><a class="header" href="#capacitors-1">Capacitors</a></h2>
<h3 id="energy-stored-1"><a class="header" href="#energy-stored-1">Energy Stored</a></h3>
<p>The energy stored by a capacitor of capacitance, C with a voltage, v
$$E = \frac{1}{2}CV^2 = \frac{1}{2}QV = \frac{Q^2}{2C}$$</p>
<ul>
<li>$C$ = Capacitance, Farads, F</li>
<li>$V$ = Voltage, Volts, V</li>
<li>$Q$ = Charge, Coulombs, C</li>
</ul>
<h3 id="capacitor-equation"><a class="header" href="#capacitor-equation">Capacitor Equation</a></h3>
<p>The ratio of charge to voltage.
$$C = \frac{Q}{V}$$</p>
<ul>
<li>$C$ = Capacitance, Farads, F</li>
<li>$V$ = Voltage, Volts, V</li>
<li>$Q$ = Charge, Coulombs, C</li>
</ul>
<h3 id="capacitance-equation"><a class="header" href="#capacitance-equation">Capacitance equation</a></h3>
<p>$$C = \frac{A \epsilon_r \epsilon_0}{d}$$</p>
<ul>
<li>$A$ = the area of the two plates</li>
<li>$d$ = the separation of the two plates</li>
<li>$\epsilon_r$ = the relative electric permittivity of the insulator</li>
<li>$\epsilon_0$ = the permittivity of free space</li>
</ul>
<h3 id="series-capacitors"><a class="header" href="#series-capacitors">Series Capacitors</a></h3>
<p>$$\frac{1}{C_T} = \frac{1}{C_1} + \frac{1}{C_2}$$
$$C_T = \frac{1}{\frac{1}{C_1} + \frac{1}{C_2}}$$</p>
<h3 id="parallel-capacitors"><a class="header" href="#parallel-capacitors">Parallel Capacitors</a></h3>
<p>$$C_T = C_1 + C_2$$</p>
<h3 id="current-voltage"><a class="header" href="#current-voltage">Current-Voltage</a></h3>
<p>$$C = \frac{Q}{V} = \frac{\int I , dt}{V}$$</p>
<h3 id="step-response-1"><a class="header" href="#step-response-1">Step Response</a></h3>
<p>$$V_c(t) = V_{in} + (V_0 - V_{in}) e^{- \frac{t}{RC}}$$</p>
<ul>
<li>$V_c(t)$ = Voltage of the capacitor at time t, Volts</li>
<li>$V_{in}(t)$ = Voltage in, Volts</li>
<li>$V_0$ = Starting Voltage, Volts</li>
<li>$C$ = Capacitance, Farads, F
Derived from:
$$I_c(t) = C \frac{d}{dt} V_c(t)$$ </li>
</ul>
<h3 id="electric-field-strength"><a class="header" href="#electric-field-strength">Electric Field Strength</a></h3>
<p>$$E=\frac{F}{Q} = \frac{1}{4\pi{}\epsilon{}_0}\frac{Q}{r^2} = \frac{V}{r}$$</p>
<ul>
<li>$F$ = Force</li>
<li>$Q$ = Charge</li>
<li>$\epsilon{}_0$ = Permittivity of free space = $8.85\times10^{-12} Fm^{-1}$</li>
<li>$\frac{1}{4\pi{}\epsilon{}_0}$ = Constant</li>
<li>$V$ = Voltage Potential, Volts</li>
<li>$r$ = Separation</li>
</ul>
<h3 id="capacitor-reactance"><a class="header" href="#capacitor-reactance">Capacitor Reactance</a></h3>
<p>As the capacitor charges or discharges, a current flows through it which is restricted by the internal impedance of the capacitor. This internal impedance is commonly known as Capacitive Reactance
$$X_c = \frac{1}{2\pi{}fC} = \frac{1}{jwC}$$</p>
<ul>
<li>$X_c$ = Reactance of the Capacitor, Ohmns</li>
<li>$j$ = $i$ = $\sqrt{-1}$</li>
<li>$w$ = frequency, rads per second</li>
</ul>
<h3 id="flux-density"><a class="header" href="#flux-density">Flux Density</a></h3>
<p>The amount of flux passing through a defined area that is perpendicular to the direction of the flux.
$$D = \frac{flux}{area} = \frac{charge}{area} (?)$$</p>
<h3 id="magnetic-field-strength-of-straight-current-carrying-wire"><a class="header" href="#magnetic-field-strength-of-straight-current-carrying-wire">Magnetic Field Strength of Straight Current Carrying Wire</a></h3>
<p>Amperes Law: For any closed loop path, the sum of the products of the length elements and the magnetic field in the direction of the length elements is proportional to the electric current enclosed in the loop.
$$B=\frac{\mu{}_0I}{2\pi{}d}$$</p>
<ul>
<li>$B$ = Magnetic field strength  at distance <strong>d</strong></li>
<li>$I$ = Current</li>
<li>$\mu{}_0$ = Permeability of free space = $4\pi{}\times{}10^{-7} Tm/A$</li>
<li>$d$ = distance from the wire.</li>
</ul>
</div>
<div class="equations">
<h2 id="resistors"><a class="header" href="#resistors">Resistors</a></h2>
<h3 id="resistors-in-series"><a class="header" href="#resistors-in-series">Resistors in Series</a></h3>
<p>$$R_t = R_1 + R_2$$</p>
<h3 id="resistors-in-parallel"><a class="header" href="#resistors-in-parallel">Resistors in Parallel</a></h3>
<p>$$\frac{1}{R_t} = \frac{1}{R_1} + \frac{1}{R_2}$$</p>
<h3 id="voltage-divider"><a class="header" href="#voltage-divider">Voltage Divider</a></h3>
<p>$$V_{out} = V_{in} \times \frac{Z_1}{Z_1 + Z_2}$$</p>
<h3 id="current-divider"><a class="header" href="#current-divider">Current Divider</a></h3>
<p>$$I_{R1} = I_T \times \frac{R_2}{R_1 + R_2}$$</p>
</div>
<div class="equations">
<h2 id="inductors-1"><a class="header" href="#inductors-1">Inductors</a></h2>
<h3 id="inductors-in-series"><a class="header" href="#inductors-in-series">Inductors in Series</a></h3>
<p>Inductors act in the same way as resistors in terms of their behaviour in series and parallel.
$$L_t = L_1 + L_2$$</p>
<h3 id="inductors-in-parallel"><a class="header" href="#inductors-in-parallel">Inductors in Parallel</a></h3>
<p>$$\frac{1}{L_t} = \frac{1}{L_1} + \frac{1}{L_2}$$</p>
<h3 id="induced-voltage"><a class="header" href="#induced-voltage">Induced Voltage</a></h3>
<p>If a coil contains N loops, the induced voltage V is given by the following equation, where Φ is the flux of the circuit.
$$V = -N \frac{d\Phi}{dt}$$</p>
<h3 id="self-inductance"><a class="header" href="#self-inductance">Self Inductance</a></h3>
<p>A changing current causes a changing field, which then induced an EMF in any conductors in that field, When any current in a coil changes, it induced an EMF in the coil
$$V= L \frac{dI}{dt}$$</p>
<h3 id="energy-stored-2"><a class="header" href="#energy-stored-2">Energy Stored</a></h3>
<p>The energy stored by an inductor is given by:
$$W = \frac{1}{2}LI^2$$</p>
<h3 id="step-response-of-rl-circuit-current"><a class="header" href="#step-response-of-rl-circuit-current">Step Response of RL Circuit (Current)</a></h3>
<p>$$I_L(t) = \frac{V_{in}}{R} + (I_0 - \frac{V_{in}}{R}) e^{-\frac{t}{\tau}}$$</p>
<ul>
<li>$V_{in}$ - Voltage source</li>
<li>$R$ - Resistance of the resistor</li>
<li>$I_0$ - The initial current. (If is already charged, then will be short circuit current)</li>
<li>$\tau = \frac{L}{R}$</li>
</ul>
<h3 id="step-response-of-rl-circuit-voltage"><a class="header" href="#step-response-of-rl-circuit-voltage">Step Response of RL Circuit (Voltage)</a></h3>
<p>Inductor voltage at time t,
$$V_L(t) = (V_{in} - I_0R) e^{-\frac{t}{\tau}}$$</p>
<ul>
<li>$V_L(t)$ - Voltage across inductor at time t</li>
<li>$V_{in}$ - Voltage source</li>
<li>$R$ - Resistance of the resistor</li>
<li>$I_0$ - The initial current</li>
<li>$\tau = \frac{L}{R}$</li>
</ul>
</div>
<div class="equations">
<h2 id="thevenin-and-norton-equivalent-circuits-1"><a class="header" href="#thevenin-and-norton-equivalent-circuits-1">Thevenin and Norton Equivalent Circuits</a></h2>
<p>Thevenin circuits contain a single voltage source and resistor in series.
Norton circuits contain a single current source and a resistor in parallel</p>
<h3 id="equivalent-resistance"><a class="header" href="#equivalent-resistance">Equivalent Resistance</a></h3>
<p>$$R_{th} = \frac{V_{th}}{I_{sc}}$$
Any linear network viewed through 2 terminals is replaced with an equivalent single voltage &amp; resistor.</p>
<ul>
<li>The equivalent voltage is equal to the open circuit voltage between the two terminals ($V_{oc}$/$V_{th}$)</li>
<li>The equivalent resistance ($R_{th}$) is found by replacing all sources with their internal impedances and then calculating the impedance of the network, as seen by the two terminals.
<ul>
<li>This can be done alternatively by calculating the short circuit current ($I_{sc}$/$I_N$) between the two terminals, and then using ohms law: $R_{th} = \frac{V_{th}}{I_{sc}}$.</li>
</ul>
</li>
<li>The value of the voltage source in a Thevenin circuit is $V_{th}$</li>
<li>The value of the current source in a Norton circuit is $I_{N}$</li>
<li>The value of the resistor in either circuit is $R_{th}$</li>
</ul>
<h3 id="thevenin---norton-conversion"><a class="header" href="#thevenin---norton-conversion">Thevenin - Norton Conversion</a></h3>
<p>Thevenin and Norton are essentially the same, but in a different form. The $R_th$ is the same for both.
$$I_N = \frac{V_{th}}{R_{th}}$$</p>
<ul>
<li>$I_N$ - Norton Current</li>
<li>$V_{th}$ - Thevevin Voltage</li>
<li>$R_{th}$ - Thevenin Resistance
<img src="es191/./img/thve-1.png" alt="" /></li>
</ul>
</div>
<div class="equations">
<h2 id="ac-circuits-1"><a class="header" href="#ac-circuits-1">AC Circuits</a></h2>
<ul>
<li>AC current is the dominant form of electricity, </li>
<li>Current changes direction at a fixed frequency (usually 50~60Hz)</li>
<li>AC voltage is generated by a rotating electromagnetic field
<ul>
<li>The angular velocity of this rotation determines the frequency of the current</li>
</ul>
</li>
</ul>
<h3 id="instantaneous-voltage"><a class="header" href="#instantaneous-voltage">Instantaneous Voltage</a></h3>
<p>An instantaneous voltage V in a sine wave is described by
$$V = V_p , \sin(\omega t + \phi)$$</p>
<p>Where:</p>
<ul>
<li>$V_p$ is the peak voltage</li>
<li>$\omega$ is the angular frequency (rad/s)</li>
<li>$\phi$ is the phase shift (radians)</li>
<li>The period of the wave is given by $T = \frac{1}{f} =\frac{ 2\pi}{\omega}$</li>
</ul>
<h3 id="instantaneous-current"><a class="header" href="#instantaneous-current">Instantaneous Current</a></h3>
<p>As current and voltage are proportional, AC current is defined in a similar way:
$$I = I_p , \sin(\omega t + \phi)$$</p>
<h3 id="ac-phasor---as-complex-number"><a class="header" href="#ac-phasor---as-complex-number">AC Phasor - As complex number</a></h3>
<p>An AC phasor can be represented as a complex number.
$$A \sin (\omega t + \phi) = A \cos \phi + jA \sin \phi = Ae^{j\phi}$$</p>
</div>
<div class="equations">
<h2 id="operational-amplifiers"><a class="header" href="#operational-amplifiers">Operational Amplifiers</a></h2>
<h3 id="output-of-inverting-amplifier"><a class="header" href="#output-of-inverting-amplifier">Output of Inverting Amplifier</a></h3>
<p>The gain of the amplifier is set by the ratio of the two resistors. The negative feedback reverses the polarity of the output voltage (Hence Negative).
$$\frac{V_{out}}{V_{in}} = \frac{ - R_{in}}{R_F}$$</p>
<p><img src="es191/./img/inv-opamp.png" alt="" /></p>
<h3 id="output-of-non-inverting-amplifier"><a class="header" href="#output-of-non-inverting-amplifier">Output of Non-Inverting Amplifier</a></h3>
<p>Non-inverting amplifiers don't invert the voltage output, and use input at the non-inverting terminal of the op amp instead.
$$\frac{V_{out}}{V_{in}} = 1 + \frac{ - R_{in}}{R_F}$$</p>
<p><img src="es191/./img/noninv-opamp.png" alt="" /></p>
</div>
<div class="equations">
<h2 id="filters"><a class="header" href="#filters">Filters</a></h2>
<h3 id="cutoff-frequncy"><a class="header" href="#cutoff-frequncy">Cutoff Frequncy</a></h3>
<p>The cutoff frequency of a filter is the point at which the gain is equal to -3 dB, which corresponds to a fall in output by a factor of $\frac{1}{\sqrt{2}}$. For the filter shown above, this is:
$$f_c = \frac{1}{2\pi{}R_2C}$$</p>
<h3 id="gain-db"><a class="header" href="#gain-db">Gain (dB)</a></h3>
<p>Gain is measured in decibels
$$,20\log\frac{Vout}{Vin}$$</p>
<p>At very low frequencies, the gain is constant (0dB)
The capacitor has high reactance at low frequencies, and is open circuit at very low frequencies
At very high frequencies, the gain tends towards −$\infty$ dB
The capacitor has a very low reactance at high frequencies (short circuit)</p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="es193"><a class="header" href="#es193">ES193</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="functions-conics--asymptotes"><a class="header" href="#functions-conics--asymptotes">Functions, Conics &amp; Asymptotes</a></h1>
<h2 id="domain--range"><a class="header" href="#domain--range">Domain &amp; Range</a></h2>
<ul>
<li>The domain of a function is the set of all valid/possible input values
<ul>
<li>The x axis</li>
</ul>
</li>
<li>The range of a function is the set of all possible output values
<ul>
<li>The y axis</li>
</ul>
</li>
</ul>
<h2 id="odd--even-functions"><a class="header" href="#odd--even-functions">Odd &amp; Even Functions</a></h2>
<p>$$f(x) = f(-x) \Rightarrow \text{f is even}$$
$$f(-x) = -f(x) \Rightarrow \text{f is odd}$$
$$f(x) = -f(-x) \Rightarrow \text{f is odd}$$</p>
<h2 id="conics"><a class="header" href="#conics">Conics</a></h2>
<p>Equation of a circle with radius $r$ and centre $(x_0, y_0)$
$$(x-x_0)^2 + (y - y_0)^2 = r^2$$</p>
<p>Equation of an ellipse with centre $(x_0, y_0)$, major axis length $2a$ and minor axis length $2b$:
$$\frac{(x-x_0)^2}{a^2} + \frac{(y - y_0)^2}{b^2} = 1$$</p>
<p>Equation of a Hyperbola with vertex $(x_0, y_0)$:</p>
<p>$$\pm \frac{(x-x_0)^2}{a^2} \mp \frac{(y - y_0)^2}{b^2} = 1 $$
The asymptotes of this hyperbola are at:
$$(y-y_0) = \pm \frac{b}{a} (x-x_0) $$</p>
<h2 id="asymptotes"><a class="header" href="#asymptotes">Asymptotes</a></h2>
<p>There are 3 kinds of asymptotes:</p>
<ul>
<li>Vertical</li>
<li>Horizontal</li>
<li>Oblique (have slope)</li>
</ul>
<p>For a function $y = \frac{P(x)}{Q(x)}$:</p>
<ul>
<li>Vertical asymptotes lie where $Q(x) = 0$ and $P(X) \neq 0$</li>
<li>Horizontal asymptotes
<ul>
<li>If the degree of the denominator is bigger than the degree of the numerator, the horizontal asymptote is the x-axis</li>
<li>If the degree of the numerator is bigger than the degree of the denominator, there is no horizontal asymptote.</li>
<li>If the degrees of the numerator and denominator are the same, the horizontal asymptote equals the leading coefficient of the numerator divided by the leading coefficient of the denominator</li>
</ul>
</li>
<li>Oblique asymptotes
<ul>
<li>A rational function will approach an oblique asymptote if the degree of the numerator is one order higher than the order of the denominator</li>
<li>To find
<ul>
<li>Divide $P(x)$ by $Q(x)$</li>
<li>Take the limit as $x \to \infty$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Example: find the asymptotes of $y = \frac{-3x^2 +2}{x-1}$:</p>
<ul>
<li>Vertical asymptotes:
<ul>
<li>Where the denominator is 0</li>
</ul>
</li>
</ul>
<p>$$x-1 = 0  \Rightarrow x = 1$$</p>
<ul>
<li>Horizontal asymptotes:
<ul>
<li>There are none, as degree of the numerator is bigger than the degree of the denominator</li>
</ul>
</li>
<li>Oblique asymptotes:
<ul>
<li>Divide the top by the bottom using polynomial long division</li>
<li>Find the limit</li>
</ul>
</li>
</ul>
<p>$$y = \frac{-3x^2}{x-1} = -3x - 3 + \frac{-1}{x-1}$$</p>
<p>As $x \to \infty$, $y \to -3x -3 $, giving $y = -3x -3$ as an asymptote.</p>
<p><img src="es193/./img/asymptotes.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="complex-numbers"><a class="header" href="#complex-numbers">Complex Numbers</a></h1>
<h2 id="de-moivres-theorem"><a class="header" href="#de-moivres-theorem">De Moivre's Theorem</a></h2>
<p>$$(r(\cos \theta + i\sin \theta))^n = r^n (\cos n\theta + i\sin n\theta)$$</p>
<h2 id="complex-roots"><a class="header" href="#complex-roots">Complex Roots</a></h2>
<p>For a complex number</p>
<p>$$z = (r(\cos \theta + i\sin \theta))$$</p>
<p>The $n^{th}$ roots can be found using the formula</p>
<p>$$z^{\frac{1}{n}} = r^{\frac{1}{n}} \left(\cos \frac{\theta + 2 k \pi}{n} + i\sin \frac{\theta + 2 k \pi}{n}\right), ;; k = 0,1,2,...,n-1$$</p>
<h2 id="finding-trig-identities"><a class="header" href="#finding-trig-identities">Finding Trig Identities</a></h2>
<p>Trig identities can be found by equating complex numbers and using de moivre's theorem. The examples below are shown for n=2 but the process is the same for any n.</p>
<h3 id="identities-for-fntheta"><a class="header" href="#identities-for-fntheta">Identities for $f(n\theta)$</a></h3>
<p>Using de moivre's theorem to equate
$$\cos 2\theta + i\sin 2\theta = (\cos\theta + i\sin\theta)^2$$</p>
<p>Expanding
$$(\cos\theta + i\sin\theta)^2 = \cos^2\theta + 2i\sin\theta\cos\theta - \sin^2\theta$$</p>
<p>Equating real and imaginary parts
$$\cos 2\theta = \cos^2\theta - \sin^2\theta$$
$$\sin 2\theta = 2\sin\theta\cos\theta$$</p>
<h3 id="identities-for-fntheta-1"><a class="header" href="#identities-for-fntheta-1">Identities for $f^n(\theta)$</a></h3>
<ul>
<li>$z = \cos\theta + i\sin\theta$</li>
<li>$z^n + z^{-n} = 2\cos n\theta$</li>
<li>$z^n - z^{-n} = 2i\sin n\theta$</li>
</ul>
<p>To find the identity for $\cos^2\theta$, start with $z + z^{-1}$, and raise to the power of 2</p>
<p>$$z + z^{-1} = 2\cos\theta$$
$$(z + z^{-1})^2 =  (2\cos\theta)^2$$
$$ z^2 + 2 + z^{-2} = 4\cos^2\theta $$</p>
<p>Substituting in for the pairs of $z^n + z^{-n}$</p>
<p>$$(z^2 + z^{-2}) + 2 = 2\cos 2\theta + 2 =  4\cos^2\theta$$
$$\cos^2\theta = \frac{1}{2} \cos 2\theta + \frac{1}{2}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vectors"><a class="header" href="#vectors">Vectors</a></h1>
<h2 id="vector-equation-of-a-straight-line"><a class="header" href="#vector-equation-of-a-straight-line">Vector Equation of a Straight Line</a></h2>
<p>The vector $\bm{r}$ is the vector of any point along the line.</p>
<p>$$\bm{r} = \bm{a} + \lambda\bm{b}$$</p>
<p><img src="es193/./img/vec-line.png" alt="" /></p>
<p>$\bm{a}$ is any point on the line, and \bm{b} is the direction of the line. $\lambda$ is a parameter that represents the position of $\bm{r}$ relative to $\bm{a}$ along the line. The carteian form of this can be derived:
$$x = a_1 + \lambda b_1$$
$$y = a_2 + \lambda b_2$$
$$z = a_3 + \lambda b_3$$</p>
<p>Equating about lambda:
$$\frac{x - a_1}{b_1} = \frac{y - a_2}{b_2} = \frac{z - a_3}{b_3}$$</p>
<h2 id="scalardot-product"><a class="header" href="#scalardot-product">Scalar/Dot Product</a></h2>
<p>The dot product of two vectors:
$$\bm{a} \cdot \bm{b} = |\bm{a}||\bm{b}|\cos\theta = \sum a_n b_n$$</p>
<ul>
<li>If $\bm{a} \cdot \bm{b} = 0$, then $\theta = 90$ and $\cos\theta = 0$
<ul>
<li>The two vectors are perpendicular</li>
</ul>
</li>
<li>$\bm{a} \cdot \bm{a} = |\bm{a}|^2$</li>
</ul>
<p>The angle between two vectors can be calculated using the dot product
$$\cos\theta = \frac{\bm{a}\cdot\bm{b}}{ |\bm{b}| |\bm{a}| }$$</p>
<h2 id="projections"><a class="header" href="#projections">Projections</a></h2>
<p>The projection of vector $\bm{a}$ in the direction of $\bm{b}$ is given by the scalar product:</p>
<p>$$\frac{\bm{b}}{ |\bm{b}| } \cdot \bm{a} = \hat{\bm{b}} \cdot \bm{a}$$</p>
<p>This gives a vector in the direction of $\bm{b}$ with the magnitude of $\bm{a}$.</p>
<h2 id="equation-of-a-plane"><a class="header" href="#equation-of-a-plane">Equation of a Plane</a></h2>
<p>The vector equation of a plane is given by
$$\bm{r} \cdot \bm{n} = \bm{a} \cdot \bm{n}$$</p>
<p>Where $\bm{n}$ is the normal to the plane, and $\bm{a}$ is any point in the plane. This expands to the cartesian form:</p>
<p>$$n_1 x+ n_2 y + n_3 z = \bm{a} \cdot \bm{n}$$</p>
<h2 id="angle-between-planes"><a class="header" href="#angle-between-planes">Angle Between Planes</a></h2>
<p>The angle between two planes is given by the angle between their normals.
$$\cos\theta = \frac{\bm{n_1}\cdot\bm{n_2}}{ |\bm{n_1}| |\bm{n_2}| }$$</p>
<h2 id="intersection-of-2-planes"><a class="header" href="#intersection-of-2-planes">Intersection of 2 Planes</a></h2>
<p>Two planes will only intersect if their normal vectors intersect.</p>
<ul>
<li>First, check the two normals are non parallel
<ul>
<li>$\bm{n_1}\cdot\bm{n_2} \neq 0$</li>
</ul>
</li>
<li>Equate all 3 variables about either a parameter $\lambda$ or one of $x$, $y$, or $z$ to get an equation for the line along which the planes intersect in cartesian form</li>
</ul>
<h3 id="example-7"><a class="header" href="#example-7">Example</a></h3>
<p>Find the intersection of the planes $3x + y - 4z = 4$ (1) and $-x + y = 2$ (2).</p>
<p>(1) - (2):
$$4x - 6z = 2 \Rightarrow z = \frac{2x-1}{3}$$</p>
<p>(1) + 3(2):
$$4y + 2z = 10 \Rightarrow z = \frac{2y-5}{-1}$$</p>
<p>Equating the two with z:</p>
<p>$$\frac{2x-1}{3} = \frac{2y-5}{-1} = z$$</p>
<h3 id="using-cross-product"><a class="header" href="#using-cross-product">Using Cross Product</a></h3>
<p>For two normals to planes $\bm{n_1}$ and $\bm{n_2}$, the vector $\bm{b} = \bm{n_1} \times \bm{n_2}$ will lie in both planes. The line</p>
<p>$$\bm{r} = \bm{a} + \lambda (\bm{n_1} \times \bm{n_2}) $$</p>
<p>lies in both planes.</p>
<h2 id="distance-from-point-to-plane"><a class="header" href="#distance-from-point-to-plane">Distance from Point to Plane</a></h2>
<p>The shortest distance from the point $(x_0,,y_0,,z_0)$ to the plane $Ax + By + Cz + D = 0$ is given by:</p>
<p>$$\frac{ |Ax_0 + By_0 + Cz_0 + D | }{\sqrt{A^2 + B^2 + C^2}}$$</p>
<h2 id="vectorcross-product"><a class="header" href="#vectorcross-product">Vector/Cross Product</a></h2>
<p>The cross product of two vectors produces another vector, and is defined as follows</p>
<p>$$
\bm{a} \times \bm {b} = |\bm{a}||\bm{b}|\sin\theta,\hat{\bm{n}} =
\begin{vmatrix}
\bm i &amp; \bm j &amp; \bm k \
a_x &amp; a_y &amp; a_z \
b_x &amp; b_y &amp; b_z \
\end{vmatrix}
$$</p>
<p>$\theta$ is the angle between the two vectors, and $\hat{\bm{n}}$ is a unit vector perpendicular to both $\bm{a}$ and $\bm{b}$. The right-hand rule convention dictates that $\hat{\bm{n}}$ should always point up (ie, if $\bm{a}$ and $\bm{b}$ are your fingers, then $\hat{\bm{n}}$ is your thumb). The cross product is not commutative, as $\bm{a} \times \bm{b}$ = $-(\bm{b} \times \bm{a})$.</p>
<p><img src="es193/./img/cross.png" alt="" /></p>
<ul>
<li>The magnitude of the cross product $|\bm{a} \times \bm{b}|$ is equal to the area of the parallelogram formed by the two vectors.</li>
<li>Can be used to find a normal given 2 vectors/2 points in a plane</li>
</ul>
<h3 id="angular-velocity"><a class="header" href="#angular-velocity">Angular Velocity</a></h3>
<p>A spheroid rotates with angular velocity $\bm{\omega}$. A point $\bm{A}$ on the spheroid has velocity $\bm{v}= \bm{\omega} \times \bm{A}$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="matrices"><a class="header" href="#matrices">Matrices</a></h1>
<h2 id="determinant--inverse-of-a-2x2-matrix"><a class="header" href="#determinant--inverse-of-a-2x2-matrix">Determinant &amp; Inverse of a 2x2 Matrix</a></h2>
<p>The determinant of a 2x2 matrix:</p>
<p>$$
\begin{vmatrix}
a &amp; b \
c &amp; d
\end{vmatrix}
= ad - bc
$$</p>
<p>The inverse:</p>
<p>$$
\begin{pmatrix}
a &amp; b \
c &amp; d
\end{pmatrix}^{-1}
= \frac{1}{ad - bc}
\begin{pmatrix}
d &amp; -b \
-c &amp; a
\end{pmatrix}
$$</p>
<p><strong>The inverse of a matrix $M$ only exists where $\det ,M \neq 0$</strong></p>
<h2 id="minors--cofactors"><a class="header" href="#minors--cofactors">Minors &amp; Cofactors</a></h2>
<ul>
<li>There is a matrix minor corresponding to each element of a matrix</li>
<li>The minor is calculated by
<ul>
<li>ignoring the values on the current row and column</li>
<li>calculate the determinant of the remaining 2x2 matrix</li>
</ul>
</li>
</ul>
<p>Example:</p>
<p>$$
M =
\begin{pmatrix}
3 &amp; 0 &amp; 2\
2 &amp; 0 &amp; -2 \
0 &amp; 1 &amp; 1
\end{pmatrix}
$$</p>
<p>The minor of the top left corner is:</p>
<p>$$
\begin{vmatrix}
0 &amp; -2 \
1 &amp; 1
\end{vmatrix}
= 2
$$</p>
<p>The cofactor is the minor multiplied by it's correct sign. The signs form a checkerboard pattern:</p>
<p>$$
\begin{vmatrix}</p>
<ul>
<li>&amp; - &amp; +\</li>
</ul>
<ul>
<li>&amp; + &amp; - \</li>
</ul>
<ul>
<li>&amp; - &amp; +
\end{vmatrix}
$$</li>
</ul>
<p>The matrix of cofactors is denoted $C$.</p>
<h2 id="determinant-of-a-3x3-matrix"><a class="header" href="#determinant-of-a-3x3-matrix">Determinant of a 3x3 Matrix</a></h2>
<p>The determinant of a 3x3 matrix is calculated by multiplying each element in one row/column by it's cofactor, then summing them. For the matrix:</p>
<p>$$
M =
\begin{pmatrix}
a &amp; b &amp; c\
d &amp; e &amp; f \
h &amp; h &amp; i
\end{pmatrix}
$$</p>
<p>$$
\det M =
a \cdot
\begin{vmatrix}
e &amp; f \
h &amp; i
\end{vmatrix}</p>
<ul>
<li>b \cdot
\begin{vmatrix}
d &amp; f \
g &amp; i
\end{vmatrix}</li>
</ul>
<ul>
<li>c \cdot
\begin{vmatrix}
d &amp; e \
g &amp; h
\end{vmatrix}
$$</li>
</ul>
<p>This shows the expansion of the top row, but any column or row will produce the same result.</p>
<h2 id="inverse-of-a-3x3-matrix"><a class="header" href="#inverse-of-a-3x3-matrix">Inverse of a 3x3 Matrix</a></h2>
<ul>
<li>Calculate matrix of minors</li>
<li>Calculate matrix of cofactors $C$</li>
<li>Transpose $C^T$</li>
<li>Multiply by 1 over determinant</li>
</ul>
<p>$$M^{-1} = \frac{1}{\det M} C^T$$</p>
<h3 id="example-8"><a class="header" href="#example-8">Example</a></h3>
<p>$$
M =
\begin{pmatrix}
3 &amp; 0 &amp; 2\
2 &amp; 0 &amp; -2 \
0 &amp; 1 &amp; 1
\end{pmatrix}
$$</p>
<p>$$
M_{11} = +
\begin{vmatrix}
0 &amp; -2 \
1 &amp; 1
\end{vmatrix}
= 2 \qquad
M_{12} = -
\begin{vmatrix}
2 &amp; -2 \
0 &amp; 1
\end{vmatrix}
= -2 \qquad
M_{13} = +
\begin{vmatrix}
2 &amp; 0 \
0 &amp; 1
\end{vmatrix}
= 2 \qquad
$$</p>
<p>$$
M_{21} = -
\begin{vmatrix}
0 &amp; 2 \
1 &amp; 1
\end{vmatrix}
= 2 \qquad
M_{22} = +
\begin{vmatrix}
3 &amp; 2 \
0 &amp; 1
\end{vmatrix}
= 3 \qquad
M_{23} = -
\begin{vmatrix}
3 &amp; 0 \
0 &amp; 1
\end{vmatrix}
= -3 \qquad
$$</p>
<p>$$
M_{31} = +
\begin{vmatrix}
0 &amp; 2 \
0 &amp; -2
\end{vmatrix}
= 0 \qquad
M_{32} = -
\begin{vmatrix}
3 &amp; 2 \
2 &amp; -2
\end{vmatrix}
= 10 \qquad
M_{33} = +
\begin{vmatrix}
3 &amp; 0 \
2 &amp; 0
\end{vmatrix}
= 0 \qquad
$$</p>
<p>The transposed matrix of cofactors $C^T$ is therefore:</p>
<p>$$
C^T =
\begin{pmatrix}
2 &amp; 2 &amp; 0\
-2 &amp; 3 &amp; 10 \
2 &amp; -3 &amp; 0
\end{pmatrix}
$$</p>
<p>Explanding by the bottom row to calculate the determinant (it has 2 zeros so easy calculation):
$$\det M = 0 \times 0 + 1 \times 10 + 0 \times 0 = 10$$</p>
<p>Calculating inverse:</p>
<h1>$$
M^{-1} = \frac{1}{\det M} C^T = \frac{1}{10}
\begin{pmatrix}
2 &amp; 2 &amp; 0\
-2 &amp; 3 &amp; 10 \
2 &amp; -3 &amp; 0
\end{pmatrix}</h1>
<p>\begin{pmatrix}
0.2 &amp; 0.2 &amp; 0\
-0.2 &amp; 0.3 &amp; 1 \
0.2 &amp; -0.3 &amp; 0
\end{pmatrix}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="simultaneous-linear-equations"><a class="header" href="#simultaneous-linear-equations">Simultaneous Linear Equations</a></h1>
<p>Several methods for solving systems of simultaneous linear equations. All the examples shown are for 3 variables, but can easily be expanded 2 $n$ variables.</p>
<h2 id="cramers-rule"><a class="header" href="#cramers-rule">Cramer's Rule</a></h2>
<p>For a system of 3 equations:</p>
<ul>
<li>Calculate the determinant $\Delta$ of the matrix of coefficients</li>
<li>Calculate determinants $\Delta_1, \Delta_2,...,\Delta_n$ by replacing 1 column of the matrix with the solutions</li>
<li>Use determinants to calculate unknowns</li>
</ul>
<p>$$
\begin{aligned}
a_1 x + b_1 y + c_1 z = d_1\
a_2 x + b_2 y + c_2 z = d_2\
a_3 x + b_3 y + c_3 z = d_3
\end{aligned}
$$</p>
<h1>$$
\begin{pmatrix}
a_1 &amp; b_1 &amp; c_1 \
a_2 &amp; b_2 &amp; c_2 \
a_3 &amp; b_3 &amp; c_3
\end{pmatrix}
\cdot
\begin{pmatrix}
x \ y \ z
\end{pmatrix}</h1>
<p>\begin{pmatrix}
d_1 \ d_2 \ d_3
\end{pmatrix}
$$</p>
<p>$$
\Delta =
\begin{vmatrix}
a_1 &amp; b_1 &amp; c_1 \
a_2 &amp; b_2 &amp; c_2 \
a_3 &amp; b_3 &amp; c_3
\end{vmatrix}
\qquad
\Delta_1 =
\begin{vmatrix}
d_1 &amp; b_1 &amp; c_1 \
d_2 &amp; b_2 &amp; c_2 \
d_3 &amp; b_3 &amp; c_3
\end{vmatrix}
$$</p>
<p>$$
\Delta_2 =
\begin{vmatrix}
a_1 &amp; d_1 &amp; c_1 \
a_2 &amp; d_2 &amp; c_2 \
a_3 &amp; d_3 &amp; c_3
\end{vmatrix}
\qquad
\Delta_3 =
\begin{vmatrix}
a_1 &amp; b_1 &amp; d_1 \
a_2 &amp; b_2 &amp; d_2 \
a_3 &amp; b_3 &amp; d_3
\end{vmatrix}
$$</p>
<p>$$
x = \frac{\Delta_1}{\Delta} \quad y = \frac{\Delta_2}{\Delta} \quad z= \frac{\Delta_3}{\Delta}
$$</p>
<h2 id="matrix-inversion"><a class="header" href="#matrix-inversion">Matrix Inversion</a></h2>
<p>For a system of equations in matrix form
$$Mx = a$$
The solutions $x$ is given by</p>
<p>$$x = M^{-1}a$$</p>
<p><strong>The system has no solutions if $\det M = 0$</strong></p>
<h2 id="gaussian-elimination"><a class="header" href="#gaussian-elimination">Gaussian Elimination</a></h2>
<p>Eliminating variables from equations one at a time to give a solution. Generally speaking, for a system of 3 equations</p>
<p>$$
\begin{aligned}
a_1 x + b_1 y + c_1 z = d_1 \quad (1)\
a_2 x + b_2 y + c_2 z = d_2 \quad (2)\
a_3 x + b_3 y + c_3 z = d_3 \quad (3)
\end{aligned}
$$</p>
<p>First, eliminate x from $(2)$ and $(3)$
$$a_1(2) - a_2(1) = (2)(new)$$
$$a_1(3) - a_3(1) = (3)(new)$$</p>
<p>This gives</p>
<p>$$
\begin{aligned}
a_1 x + b_1 y + c_1 z = d_1 \quad (1)\
\tilde{b_2} y + \tilde{c_2} z = \tilde{d_2} \quad (2)\
\tilde{b_3} y + \tilde{c_3} z = \tilde{d_3} \quad (3)
\end{aligned}
$$</p>
<p>Then, eliminate y from $(3)$
$$b_2(3) - b_3(2) = (3)(new)$$</p>
<p>Giving</p>
<p>$$
\begin{aligned}
a_1 x + b_1 y + c_1 z = d_1 \quad (1)\
\tilde{b_2} y + \tilde{c_2} z = \tilde{d_2} \quad (2)\
\tilde{\tilde{c_3}} z = \tilde{\tilde{d_3}} \quad (3)
\end{aligned}
$$</p>
<p>This gives a solution for $z$, which can then be back-substituted to find the solutions for $x$ and $y$.</p>
<p>$$z = \frac{\tilde{\tilde{d_3}}}{\tilde{\tilde{c_3}}}$$
$$y = \frac{\tilde{d_2}}{\tilde{b_2}} - \frac{\tilde{c_2}}{\tilde{b_2}}z$$
$$x = \frac{d_1}{a_1} - \frac{b_1}{a_1}y - frac{c_1}{a_1}z$$</p>
<p>The advantages of this method are:</p>
<ul>
<li>No need for matrices (yay)</li>
<li>Works for homogenous and inhomogeneous systems</li>
<li>The matrix need not be square</li>
<li>Works for any size of system if a solution exists</li>
</ul>
<p>Sometimes, the solution can end up being in a parametric form, for example:</p>
<p>$$
\begin{aligned}
4 x + y + z = 9 &amp; \quad (1)\
x + y + z = 6 &amp; \quad (2)\
x + 2 y + 2 z = 11 &amp; \quad (3)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
4 x + y + z = 9 &amp; \quad (1)\
3y + 3z = 15 &amp; \quad 4(2) - 1(1)\
7 y + 7 z = 35 &amp; \quad 4(3) - 1(1)
\end{aligned}
$$</p>
<p>$$
\begin{aligned}
4 x + y + z = 9 &amp; \quad (1)\
3y + 3z = 15 &amp; \quad 4(2) - 1(1)\
0z = 0 &amp; \quad 3(3) - 7(2)
\end{aligned}
$$</p>
<p>This doesn't make sense, as the final equation is satisfied for any value of $z$. Substituting a parameter $\lambda$ for $z$ gives:</p>
<p>$$
z\ = \lambda \quad y = 5-\lambda \quad x = 1
$$</p>
<h2 id="gauss-seidel-iteration"><a class="header" href="#gauss-seidel-iteration">Gauss-Seidel Iteration</a></h2>
<p>Iterative methods involve starting with a guess, then making closer and closer approximations to the solution. If iterations tend towards a limit, then the system converges and the limit will be a solution. If the system diverges, there is no solution for this iteration. For the gauss-seidel scheme:</p>
<p>$$
\begin{aligned}
a_1 x + b_1 y + c_1 z = d_1 \quad (1)\
a_2 x + b_2 y + c_2 z = d_2 \quad (2)\
a_3 x + b_3 y + c_3 z = d_3 \quad (3)
\end{aligned}
$$</p>
<p>Rearrange to get iterative formulae:</p>
<p>$$
\begin{aligned}
x^{r+1} = (d_1 - b_1 y^r - c_1 z^r) / a_1 \quad (1)\
y^{r+1} = (d_2 - a_2x^{r+1} - c_2z^r) / b_2\quad (2)\
z^{r+1} = (d_3 - a_3x^{r+1} - b_3y^{r+1}) / c_3 \quad (3)
\end{aligned}
$$</p>
<p>Using these formulae, make a guess at a starting value and then continue to iterate. For example:</p>
<p>$$
\begin{aligned}
4 x + 1 y + 1 z = 9 \quad (1)\
1 x + 5 y + 1 z = 14 \quad (2)\
1 x + 1 y + 3 z = 12 \quad (3)
\end{aligned}
$$</p>
<p>Rearranging:</p>
<p>$$
\begin{aligned}
x^{r+1} = (9 - y^r - z^r) / 4 \quad (1)\
y^{r+1} = (14 - x^{r+1} - z^r) / 5\quad (2)\
z^{r+1} = (12- x^{r+1} - y^{r+1}) / 3 \quad (3)
\end{aligned}
$$</p>
<p>The solutions are $x=1$, $y=2$, $z=3$, as can be seen from the table below containing the iterations:</p>
<table><thead><tr><th>r</th><th>x</th><th>y</th><th>z</th></tr></thead><tbody>
<tr><td>0</td><td>0</td><td>0</td><td>0</td></tr>
<tr><td>1</td><td>2.25</td><td>2.35</td><td>2.467</td></tr>
<tr><td>2</td><td>1.046</td><td>2.098</td><td>2.952</td></tr>
<tr><td>3</td><td>0.988</td><td>2.012</td><td>3.000</td></tr>
<tr><td>4</td><td>0.997</td><td>2.001</td><td>3.001</td></tr>
</tbody></table>
<p>Note that this will only work if the system is diagonally dominant. For a system to be diagonally dominant, the divisor of the iterative equation must be greater than the sum of the other coefficients.</p>
<p>$$
\begin{aligned}
\bm{4} x + 1 y + 1 z = 9 \
1 x + \bm{5} y + 1 z = 14 \
1 x + 1 y + \bm{3} z = 12
\end{aligned}
$$</p>
<p>Systems can be rearranged to have this property:</p>
<p>$$
\begin{aligned}
\bm{2} x + 7 y + 1 z = 5 \
-1 x + \bm{3} y + 3 z = 2 \
-6 x + 2 y + \bm{2} z = -3
\end{aligned}
$$</p>
<p>Rearranges to:</p>
<p>$$
\begin{aligned}
\bm{-6} x + 2 y + 1 z = -3 \
2 x + \bm{7} y + 1 z = 5 \
-1 x + 3 y + \bm{8} z = 2
\end{aligned}
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="differentiation"><a class="header" href="#differentiation">Differentiation</a></h1>
<h2 id="implicit-differentiation"><a class="header" href="#implicit-differentiation">Implicit Differentiation</a></h2>
<p>When differentiating a function of one variable with respect to another (ie $\frac{dy}{dx} f(y)$), simply differentiate with respect to $y$, then multiply by $\frac{dy}{dx}$.</p>
<p>For example, find $\frac{dy}{dx}$ where $x^2y^3 - x^2 + 3y - 3 = 0$. First, using the product rule to differentiate the first term:
$$u = x^2 \quad u' = 2x$$
$$v = y^3 \quad v' = 3y^2 \frac{dy}{dx}$$</p>
<p>The equation with all terms differentiated:
$$2xy^3 + 3x^2y^2 \frac{dy}{dx} -2x + 3y \frac{dy}{dx} = 0$$</p>
<p>Rearranging to get in terms of $\frac{dy}{dx}$:
$$3x^2y^2 \frac{dy}{dx}+ 3y \frac{dy}{dx} = 2x - 2xy^3$$
$$\frac{dy}{dx} = \frac{2x - 2xy^3}{3x^2y^2 + 3}$$</p>
<h2 id="inverse-trig-functions"><a class="header" href="#inverse-trig-functions">Inverse Trig Functions</a></h2>
<p>All the derivatives of the inverse trig functions are given in the data book. They can be derived as follows ($\sin$ is used as an example).</p>
<p>$$y = \sin^{-1} x$$
$$\sin y = x$$</p>
<p>Differentiating both sides with respect to x</p>
<p>$$\frac{dy}{dx}\cos y = 1$$
$$\frac{dy}{dx} = \frac{1}{\cos y}$$</p>
<p>Using pythagorean identity $\cos y = \sqrt{1 - \sin^2 y}$</p>
<p>$$\frac{dy}{dx} = \frac{1}{\sqrt{1 - \sin^2 y}} = \frac{1}{\sqrt{1 - x^2}}$$</p>
<h2 id="differentials"><a class="header" href="#differentials">Differentials</a></h2>
<p>Differentials describe small changes to values/functions
$$y= f(x)$$
$$\frac{dy}{dx} = f'(x)$$
$$dy = f'(x) dx$$</p>
<p>Recall that $\frac{dy}{dx} \approx \frac{\delta y}{\delta x}$. This means this can be rewritten:
$$\delta y \approx f'(x) \delta x$$</p>
<p>Dividing both sides by $y = f(x)$:
$$\frac{\delta y}{y} =  \frac{x f'(x)}{f(x)} \frac{\delta x}{x}$$</p>
<p>$\frac{\delta y}{y}$ represents a <em>relative</em> change in y, and $\frac{\delta x}{x}$ represents a <em>relative</em> change in x. This can be used to give approximations of how one quantity changes based upon another.</p>
<p>For example, given the mass of a sphere $M = \frac{4}{3} \rho \pi r^3$, where $\rho$ is the material density, estimate the change in mass when the radius is increased by 2%.
$$M = \frac{4}{3} \rho \pi r^3$$
$$\frac{dM}{dr} = 4 \rho \pi r^2$$
$$\delta m = 4 \rho\pi r^2 \delta r$$</p>
<p>Dividing both sides by the original formula:
$$\frac{\delta M}{M} = \frac{4 \rho\pi r^2 \delta r}{\frac{4}{3} \rho \pi r^3} = 3 \frac{\delta r}{r}$$</p>
<p>$\frac{\delta r}{r}$ represents a relative change in radius, so when $r$ increases by 2%, $\frac{\delta r}{r} = 0.02$
$$\frac{\delta M}{M} = 3 \times 0.02 = 0.06$$</p>
<p>Meaning the mass increases by 6%.</p>
<h2 id="hyperbolic-functions"><a class="header" href="#hyperbolic-functions">Hyperbolic Functions</a></h2>
<p>Hyperbolic functions have similar identities to circular trig functions. They're the same, except anywhere there is a product of two $\sinh$s, the term should be negated. Hyperbolic functions can also be defined in terms of exponential functions, making them easy to differentiate.</p>
<p>$$y = \cosh x = \frac{1}{2} (e^x + e^{-x})$$
$$\frac{dy}{dx} = \frac{1}{2} (e^x - e^{-x}) = \sinh x$$</p>
<p>All the derivatives of hyperbolic functions are given in the formula book.</p>
<h2 id="parametric-differentiation"><a class="header" href="#parametric-differentiation">Parametric Differentiation</a></h2>
<p>For a function given in parametric form $y = f(t)$, $x = f(t)$:
$$\frac{dy}{dx} = \frac{dy}{dt} \times \frac{dt}{dx}$$
$$\frac{d}{dx} = \frac{dt}{dx} \times \frac{d}{dt}$$</p>
<p>$$\frac{d^2y}{dx^2} = \frac{d}{dx}(\frac{dy}{dx}) = \frac{dt}{dx} \times \frac{d}{dt}(\frac{dy}{dx})$$</p>
<h2 id="partial-differentiation"><a class="header" href="#partial-differentiation">Partial Differentiation</a></h2>
<p>For a function of two variables $z = f(x,y)$ there are two gradients at the point $z$, one in $x$ and one in $y$. To find the gradient in the x direction, differentiate $f(x,y)$ treating y as a constant. To find the gradient in the y direction, differentiate $f(x,y)$ treating x as a constant. These are the two partial derivatives of the function, $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$.</p>
<p>For example, for a function $z= 4x^3 + 5y^7 x$:
$$\frac{\partial z}{\partial x} = 12x^2 + 5y^7 $$
$$\frac{\partial z}{\partial y} = 35y^6 x$$</p>
<h2 id="implicit-partial-differentiation"><a class="header" href="#implicit-partial-differentiation">Implicit Partial Differentiation</a></h2>
<p>When a function of several variables is given and a partial derivative is required, differentiate the numerator of the partial derivative implicitly with respect to the denominator, and treat the third variable as constant. For example, find $\frac{\partial z}{\partial y}$ given $z^2 = x^2 + y^2$:</p>
<p>$$\frac{\partial}{\partial y} z^2 = \frac{\partial}{\partial y}(x^2 + y^2)$$</p>
<p>$$2z \frac{\partial z}{\partial y} = 2y$$
$$\frac{\partial z}{\partial y} = \frac y z$$</p>
<p>Another example, find $\frac{\partial z}{\partial x}$ given $z\cos z=x^2 y^3+z$
$$\frac{\partial}{\partial x}(z \cos z)  = \frac{\partial}{\partial x} (x^2 y^3 + z)$$
$$\frac{\partial z}{\partial x} \cos z - z \sin z \frac{\partial z}{\partial x} = 2x y^3 + \frac{\partial z}{\partial x}$$
$$\frac{\partial z}{\partial x}(\cos z - z \sin z -1) = 2xy^3$$
$$\frac{\partial z}{\partial x} = \frac{2xy^3}{\cos z - z \sin z -1}$$</p>
<h3 id="higher-order-partial-derivatives"><a class="header" href="#higher-order-partial-derivatives">Higher Order Partial Derivatives</a></h3>
<p>Three 2nd order derivatives for functions of 2 variables. For $z = f(x,y)$:
$$\frac{\partial^2 z}{\partial x^2} = \frac{\partial}{\partial x}(\frac{\partial z}{\partial x} ) $$
$$\frac{\partial^2 z}{\partial y^2} = \frac{\partial}{\partial y}(\frac{\partial z}{\partial y} ) $$
$$\frac{\partial^2 z}{\partial x \partial y} = \frac{\partial}{\partial x}(\frac{\partial z}{\partial y}) = \frac{\partial}{\partial y}(\frac{\partial z}{\partial x}) = \frac{\partial^2 z}{\partial y \partial x} $$</p>
<p>Note how for the last one, the order is interchangable as it yields the same result.</p>
<h3 id="chain-rule"><a class="header" href="#chain-rule">Chain Rule</a></h3>
<p>The chain rule for a function $w(x,y)$, where x and y are functions of a parameter $t$:
$$\frac{dw}{dt} = \frac{\partial w}{\partial x}\frac{dx}{dt} + \frac{\partial w}{\partial y} \frac{dy}{dt}$$</p>
<h2 id="total-differential"><a class="header" href="#total-differential">Total Differential</a></h2>
<p>The total differential represents the total height gain or lost when moving along the function described by $z = f(x,y)$
$$dz = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy$$</p>
<h3 id="contour-plots"><a class="header" href="#contour-plots">Contour Plots</a></h3>
<p>Along a line of a contour plot, the total differential is zero: the height doesn't change. This allows $\frac{dy}{dx}$ to be found
$$dh = \frac{\partial h}{\partial x}dx + \frac{\partial h}{\partial y}dy = 0$$
$$\frac{dy}{dx} = \frac{\partial h / \partial x}{\partial h / \partial y}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="integration"><a class="header" href="#integration">Integration</a></h1>
<h2 id="integration-by-parts"><a class="header" href="#integration-by-parts">Integration by Parts</a></h2>
<p>When an integral is a product of two functions (ie $\int e^x \sin x$), it can be integrated by parts:</p>
<p>$$\int u \frac{dv}{dx} ,dx = uv - \int v \frac{du}{dx} ,dx$$</p>
<p>(see also the DI method)</p>
<h2 id="improper-integrals"><a class="header" href="#improper-integrals">Improper Integrals</a></h2>
<p>An integral is improper if either</p>
<ul>
<li>One of its limits is infinity</li>
<li>The function is not defined for any point within the interval (bounds inclusive)</li>
</ul>
<p>To evaluate these integrals, replace the dodgy boundwith a variable $t$, evaluate the integral in terms of the variable, and then take the limit as the variable tends towards the bound.</p>
<p>$$\int_a^{\infty} f(x) , dx = \lim_{t \to \infty} \int_a^t f(x) , dx$$</p>
<p>Where functions are not continuous over the interval, may need to split the function into two integrals. For example, if $f(x)$ is not continuous at $x = c$ where $a &lt; c &lt;b$, then:
$$\int_a^b f(x) ,dx = \int_a^c f(x) ,dx  + \int_c^b f(x) ,dx $$</p>
<h2 id="reduction-formulae"><a class="header" href="#reduction-formulae">Reduction Formulae</a></h2>
<p>Reduction formulae involve rewriting an integral in terms of itself to get a recurrence relation. They usually involve some variable $n$ as well as other variables in the integral ($x$). For example, integrating $I_n = \int_0^{\infty} x^n e^{-x} dx$:</p>
<p>By parts:</p>
<p>$$u = x^n \qquad \frac{dv}{dx} =  e^{-x} $$
$$\frac{du}{dx} = n x^{n-1} \qquad v = -e^{-x}$$</p>
<p>$$\int u \frac{dv}{dx} ,dx = uv - \int v \frac{du}{dx} ,dx$$
$$\int_0^{\infty} x^n e^{-x} dx = -x^n e^{-x} - \int_0^{\infty} e^{-x} x^{n-1}$$
$$I_n = n \cdot I_{n-1}$$</p>
<p>Note how the integral is now in terms of itself, but with $n-1$. This creates a recursive definition that can be expanded to evaluate $I_5$</p>
<p>$$I_5 = 5I_4 = 5\times 4I_3 = 5 \times 4 \times 3 I_3 = 5 \times 4 \times 3 \times 2 \times 1 \times I_0 = 120 I_0$$
$$I_0 = \int_0^{\infty} x^0 e^{-x} dx = \int_0^{\infty} e^{-x} dx = \left( -e^{-\infty} + e^0 \right) = 1$$
$$I_5 = 5! = 120$$</p>
<h2 id="integration-by-substitution"><a class="header" href="#integration-by-substitution">Integration by Substitution</a></h2>
<p>Substitution is often useful in solving integrals.</p>
<ul>
<li>Choose a new function $u(x)$</li>
<li>Find $\frac{du}{dx}$</li>
<li>Substitute $u$ in</li>
<li>Swap $dx$ for $du$</li>
<li>Put limits in terms of $u$ (if appropriate)</li>
<li>Solve with respect to u</li>
</ul>
<p>Choosing a function $u$ to substitute depends on the integral, and there are certain patterns to spot which make it easier.</p>
<h3 id="example-9"><a class="header" href="#example-9">Example</a></h3>
<p>$$\int^1_0 \frac{y^2}{1 + y^6} , dy$$</p>
<p>Substituting $u = y^3$:
$$u = y^3 \qquad \frac{du}{3} = y^2 dy$$</p>
<p>$$\int^{y=1}_{y=0} \frac{du}{1+u^2}$$
Substituting the limits:
$$u = 1^3 = 1 \qquad u = 0^3 = 0$$
The integral becomes:
$$\frac 1 3 \int^1_0 \frac{du}{1+u^2} = \frac 1 3 \left[\arctan u \right]^1_0 = \frac 1 3 [\arctan 1 - \arctan 0] = \frac {\pi}{12}$$</p>
<h2 id="tan-substitutions"><a class="header" href="#tan-substitutions">$\tan$ Substitutions</a></h2>
<p>There are two standard $\tan$ substitutions that can be really useful when integrating trig functions.</p>
<h3 id="t--tan-frac-x-2-subs"><a class="header" href="#t--tan-frac-x-2-subs">$t = \tan \frac x 2$ Subs</a></h3>
<p>The first one:
$$t = \tan \frac x 2 \qquad dx =  \frac{2dt}{1+t^2}$$
$$\cos x = \frac{1-t^2}{1+t^2} \quad \sin x = \frac{2t}{1+t^2} \quad \tan x = \frac{2t}{1-t^2}$$</p>
<p>For example:</p>
<p>$$\int \cosec x , dx = \int \frac{1}{\sin x} , dx$$</p>
<p>Letting $t = \tan \frac x 2$:
$$dx =  \frac{2dt}{1+t^2} \quad \sin x = \frac{2t}{1+t^2}$$
$$\int \frac{1+t^2}{2t} \cdot \frac{2dt}{1+t^2} = \int \frac{dt}{t} = \ln|t| + c$$</p>
<h3 id="t--tan-x--subs"><a class="header" href="#t--tan-x--subs">$T = \tan x $ Subs</a></h3>
<p>$$T = \tan x \qquad dx = \frac{dT}{1+T^2}$$
$$\sin x = \frac{T}{\sqrt{1+T^2}} \qquad \cos x = \frac{1}{\sqrt{1+T^2}}$$</p>
<p>For example:
$$\int \frac{dx}{4\cos^2 x - \sin^2 x}$$</p>
<p>Letting $T = \tan x$:
$$\sin^2 x = \frac{T^2}{1+T^2} \quad \cos^2 x \frac{1}{1+T^2} \quad dx = \frac{dT}{1+T^2}$$</p>
<p>$$\int \frac{dx}{4\cos^2 x - \sin^2 x} = \int \frac{\frac{dT}{1+T^2}}{\frac{4}{1+T^2} - \frac{T^2}{1+T^2}} = \int \frac{\frac{dT}{1+T^2}}{\frac{4 - T^2}{1+T^2}} = \int \frac{dT}{4 - T^2}$$
$$= \frac 1 2 \tanh^{-1} \frac T 2 + C =  \frac 1 2 \tanh^{-1} \frac{\tan x}{2} + C = \frac 1 4 \ln \left|  \frac{2+\tan x}{2-\tan x} \right| + C$$</p>
<h2 id="standard-forms"><a class="header" href="#standard-forms">Standard Forms</a></h2>
<p>Integrals will sometimes be (or can be put into) standard forms which then evaluate directly to inverse trig functions. The full list is given in the data book but:</p>
<p>$$\int \frac{du}{\sqrt{a^2 - u^2}} = \arcsin \frac u a + c$$
$$\int \frac{du}{a^2 + u^2} = \frac 1 a \arctan \frac u a + c$$</p>
<h3 id="example-10"><a class="header" href="#example-10">Example</a></h3>
<p>$$ \int^5_2 \frac{dx}{\sqrt{x^2 + 2x - 8}} = \int^5_2 \frac{dx}{\sqrt{(x+1)^2 - 9}}$$</p>
<p>Substituting $u = x+1$</p>
<p>$$ \int^6_3 \frac{du}{\sqrt{u^2 - 9}} = \left[\cosh^{-1} \frac u 3 \right]^6_3 = \left[ \cosh^{-1} 2 - \cosh^{-1} 1\right] = \cosh^{-1} 2$$</p>
<h2 id="trigonometric-identities"><a class="header" href="#trigonometric-identities">Trigonometric Identities</a></h2>
<p>Trig identities are often useful in evaluating integrals, for example:</p>
<p>$$\int \sin 4x \cos 3x , dx$$</p>
<p>Using $2 \sin A \cos B =  (\sin (A+B) + sin (A-B))$:</p>
<p>$$\int \sin 4x \cos 3x , dx = \frac 1 2 \int \sin 7x + \sin x,  dx  = \frac 1 2 (- \frac 1 7 \cos 7x - \cos x) + c$$</p>
<p>$$\int \sin 4x \cos 3x , dx = - \frac 1 14 \cos 7x - \frac 1 2 cos x + c$$</p>
<h2 id="integration-as-a-limit"><a class="header" href="#integration-as-a-limit">Integration as a Limit</a></h2>
<p>The area under a curve $f(x)$ from $a \leq x \leq b$ is given by:
$$\int_a^b f(x) , dx$$
This can be approximated by dividing the area under the curve into a number of rectangles:</p>
<p><img src="es193/./img/int-rectangles.png" alt="" /></p>
<p>For $n$ rectangles over the width $b-a$, the width of each rectangle $\delta x = \frac{b-a}{n}$. The area of the rectangle is therefore given by $y(x_k)\cdot \delta x$. The sum of all the rectangles, and therefore total area is:
$$\sum^n_{k=1} y(x_k) \delta x$$</p>
<p>As $n \to \infty$, $\delta x \to 0$, so:
$$\int^b_a y(x) , dx = \lim_{\delta x \to 0} ,\sum^{x=b}_{x=a} y(x) \delta x$$</p>
<h2 id="volumes-of-revolution"><a class="header" href="#volumes-of-revolution">Volumes of Revolution</a></h2>
<p>For a function $y(x)$ rotated 360 degrees about the x axis, consider a disc of width $\delta x$ and radius y. The volume is given by $\pi y^2 \delta x$. The volume of all slices as $n \to \infty$ is
$$\sum^{x=b}_{x=a} \pi y^2 \delta x$$</p>
<p>Therefore the volume of revolution for a function $y(x)$ about the x axis is
$$V = \pi \int^b_a y^2 , dx$$</p>
<p>Volume of revolution about y axis:</p>
<p>$$V = \pi \int^b_a x^2 , dy$$</p>
<h2 id="centres-of-mass-for-planar-objects"><a class="header" href="#centres-of-mass-for-planar-objects">Centres of Mass for Planar Objects</a></h2>
<p>The centre of mass is the point through which gravity acts. In 1 dimension:</p>
<p><img src="es193/./img/CoM.png" alt="" /></p>
<p>The sum of the moments about 0 is $m_1 x_1 + m_2 x_2$. The moment of the total mass is $\bar x (m_1 + m_2)$. Equating these:
$$\bar x = \frac{m_1 x_1 + m_2 x_2}{m_1 + m_2} = \frac{\sum m_i x_i}{\sum m_i}= \frac{\text{sum of moments}}{\text{total mass}}$$</p>
<p>This can be expanded into 2 dimensions:
$$\bar x = \frac{\sum m_i x_i}{\sum m_i} \qquad \bar y = \frac{\sum m_i y_i}{\sum m_i}$$</p>
<p>For the centre of mass of an infinitely thin sheet with uniformly distributed mass, for x-axis consider thin slices of width $\delta x$.</p>
<ul>
<li>Area of slice = $y \cdot \delta x$</li>
<li>Mass of slice = $m \cdot y\cdot \delta x$</li>
<li>Moment of slice about y-axis = $x\cdot m\cdot y\cdot \delta x$</li>
<li>Sum of all moments as $\delta x \to 0$ = $\int_0^a (m \cdot x \cdot y) , dx$</li>
</ul>
<p>$$\bar x = \frac {\int_0^a (m  x  y) , dx}{m}$$</p>
<p>For the sum of the moments about y axis, take a horizontal slice with width $\delta y$ with length $(a-x)$</p>
<ul>
<li>Area of slice = $(a-x) \cdot \delta y$</li>
<li>Mass of slice = $m \cdot (a-x) \cdot \delta y$</li>
<li>Moments of slice about x-axis = $y \cdot m \cdot (a-x) \cdot \delta y$</li>
<li>Sum of all moments as $y \to 0$ = $\int_0^{a} (y m (a-x)) , dy$</li>
</ul>
<p>$$\bar y = \frac{\int_0^{a} my(a-x), dy}{m}$$</p>
<p>Note that usually, mass $m$ is mass per unit area.</p>
<h3 id="example-11"><a class="header" href="#example-11">Example</a></h3>
<p>Find centre of mass of plane lamina shown</p>
<p><img src="es193/./img/lamina.png" alt="" /></p>
<p>By symmetry, clearly $\bar x = 0$. For $\bar y$, let $m$ be the mass per unit area, and consider a horizontal strip of width $y$.</p>
<ul>
<li>Area of strip is $2x \delta y$</li>
<li>Mass of strip is $2mx \delta y$</li>
<li>Moment of one strip about x axis is $2mxy \delta y$</li>
</ul>
<p>Total moment as $\delta y \to 0$:</p>
<p>$$\int_0^{4a^2} 2 m x y , dy = 2m \int_0^{4a^2} y \sqrt{4a^2 - y} , dy = \frac{256ma^5}{15}$$</p>
<p>For the total mass $M$, total area of the shape:
$$\int^{2a}<em>{2a} y , dy = \int^{2a}</em>{2a} (4a^2 -x^2) , dy = \frac{32a^3}{3}$$</p>
<p>So total mass M = $m \times M = \frac{32ma^3}{3}$</p>
<p>$$\bar y = \frac{Mx}{M} = \frac{256ma^5}{15} \div \frac{32ma^3}{3} = \frac{8a^2}{5}$$</p>
<h2 id="moments-of-inertia-for-laminae"><a class="header" href="#moments-of-inertia-for-laminae">Moments of Inertia for Laminae</a></h2>
<p>The moment of inertia $I$ is a measure of how difficult it is to rotate an object. Suppose a lamina is divided into a large number of small elments, each with mass $\delta m$ at distance $r$ from the origin $O$. The moment of inertia of one element is defined to be $r^2 \delta m$. Taking the sum of all moments as $\delta m \to 0$</p>
<p>$$I = \int r^2 , dm$$</p>
<p>The bounds of the integral should be chosen appropriately such as to include the entire lamina.</p>
<ul>
<li>For a lamina lying in the x-y plane, the moment of inertia about z-axis is the sum of the moments about x and y axes.
<ul>
<li>$I_{oz} = I_{ox} + I_{oy}$</li>
</ul>
</li>
<li>For an axis $L'$ parallel to $L$ at a distance $d$ and both lying in the same plane as the lamina with mass $M$, where $L$ passes through the centere of the lamina:
<ul>
<li>$I_{L'} = I_L + Md^2$</li>
</ul>
</li>
</ul>
<h3 id="example-12"><a class="header" href="#example-12">Example</a></h3>
<p>Find the moment of inertia of a thin rectangular plate of mass $M$, length $2a$ and width $2b$ about an axis through its centre of gravity which is normal to its plane.</p>
<p>Assuming the plate lies in the x-y axis, the question is asking for the moment about the z-axis. To find this, the moments about both x and y axes are required as $I_{oz} = I_{ox} + I_{oy}$. To find $I_{oy}$:</p>
<p><img src="es193/./img/inertia-example.png" alt="" /></p>
<ul>
<li>Let the mass per unit area $m = \frac{M}{4ab}$</li>
<li>A strip of width $\delta x$ at distance $x$ from $Oy$ has mass $m\cdot 2b \cdot \delta x$</li>
<li>The moment of inertia of the strip is $x^2 \cdot m\cdot 2b \cdot \delta x$</li>
</ul>
<p>Taking the limit of the sum of all the strips:
$$\int_{-a}^a 2bmx^2 , dx = \frac{4ba^3}{3}$$</p>
<p>As $m = \frac{M}{4ab}$,
$$I_{oy} = \frac{4ba^3}{3}\cdot \frac{M}{4ab} = \frac{Ma^2}{3}$$</p>
<p>$I_{ox}$ is identically dervied and equals $\frac{Mb^2}{3}$. Summing the two moments gives:</p>
<p>$$I_{oz} = \frac{Ma^2}{3} + \frac{Mb^2}{3} = \frac{M}{3} (a^2 + b^2)$$</p>
<h2 id="lengths-of-curves"><a class="header" href="#lengths-of-curves">Lengths of Curves</a></h2>
<p>The length of the arc of a curve $y(x)$ between $x=a$ and $x=b$ is given by
$$\int_a^b \sqrt{1 + \left(\frac{dy}{dx}\right)^2} , dx$$</p>
<p>Alternatively, for parametrised curves:
$$\int^{t=t_2}_{t=t_1} \sqrt{\left(\frac{dx}{dt}\right)^2 + \left(\frac{dy}{dt}\right)^2} , dt$$</p>
<h2 id="surface-areas-of-revolution"><a class="header" href="#surface-areas-of-revolution">Surface Areas of Revolution</a></h2>
<p>Similar to volumes of revolution, the surface area of a function when rotated about the x axis is given by:</p>
<p>$$A = 2\pi \int_a^b y ,\sqrt{1 + \left(\frac{dy}{dx}\right)^2}, dx$$</p>
<h3 id="example-13"><a class="header" href="#example-13">Example</a></h3>
<p>The surface are of the parabola $y^2 = 8x$ between $x=0$ and $x=2$, when rotated about x axis:
$$y = 2 \sqrt{2x} \qquad \frac{dy}{dx} =  \sqrt{\frac 2 x}$$</p>
<p>$$A = 2 \pi \int^2_0 2 \sqrt{2x} \sqrt{1 + \frac 2 x} , dx = 4 \sqrt 2 \pi \int^2_0 \sqrt{x+2} , dx$$
$$A = \frac{64\sqrt 2 \pi - 32 \pi }{3}$$</p>
<h2 id="mean-values-of-a-function"><a class="header" href="#mean-values-of-a-function">Mean Values of a Function</a></h2>
<p>For a function $f(x)$ over the interval $[a,b]$</p>
<p>Mean value:
$$\frac{1}{b-a} \int^b_a f(x) dx$$</p>
<p>Root mean square value:
$$\sqrt{\frac{1}{b-a} \int^b_a \left(f(x)\right)^2 dx}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="differential-equations"><a class="header" href="#differential-equations">Differential Equations</a></h1>
<h2 id="first-order"><a class="header" href="#first-order">First Order</a></h2>
<p>A first order differential equation has $\frac{dy}{dx}$ as it's highest derivative. For the two methods below, it is important the equation is in the correct form specified.</p>
<h3 id="seperating-variables"><a class="header" href="#seperating-variables">Seperating Variables</a></h3>
<p>For an equation of the form
$$\frac{dy}{dx} = f(x) g(y)$$</p>
<p>The solution is
$$\int \frac{dy}{g(y)} = \int f(x) dx$$</p>
<h3 id="integrating-factors"><a class="header" href="#integrating-factors">Integrating Factors</a></h3>
<p>For an equation of the form
$$\frac{dy}{dx} + P(x)y = Q(x)$$</p>
<p>An integrating factor $\mu$ can be found such that:
$$\mu(x) = e^{\int P(x) dx}$$</p>
<p>Multiplying through by $\mu$ gives</p>
<p>$$\mu(x)\frac{dy}{dx} + \mu(x)P(x)y = \mu(x)Q(x)$$</p>
<p>Then, applying he product rule backwards gives a solution:</p>
<p>$$\frac{d}{dx}(\mu(x) y) = \mu(x)Q(x)$$
$$\mu(x) y = \int \mu(x) Q(x) dx$$</p>
<h2 id="second-order"><a class="header" href="#second-order">Second Order</a></h2>
<p>A second order ODE has the form:
$$a \frac{d^2 y}{dx^2} + b\frac{dy}{dx} + cy = f(x)$$</p>
<p>The equation is <em>homogeneous</em> if $f(x) = 0$.</p>
<p>The <em>auxillary equation</em> is
$$ak^2 + bk +c = 0$$</p>
<p>This gives two roots $k_1$ and $k_2$, which determine the <em>complementary function</em>:</p>
<table><thead><tr><th>Roots</th><th>Complementary Function</th></tr></thead><tbody>
<tr><td>$k_1$ and $k_2$ both real</td><td>$y = Ae^{k_1 x} + Be^{k_2 x}$</td></tr>
<tr><td>$k_1 = k_2$, both real</td><td>$y = (A+Bx) e^{kx}$</td></tr>
<tr><td>$k_1 = \alpha + \beta i$ and $k_2 = \alpha - \beta i$</td><td>$y = e^{\alpha x} (A\cos\beta x + B \sin\beta x)$</td></tr>
</tbody></table>
<p>The complementary function is the solution. Sometimes, initial conditions will be given which allow the constants $A$ and $B$ to be found.</p>
<h3 id="non-homogeneous-systems"><a class="header" href="#non-homogeneous-systems">Non-Homogeneous Systems</a></h3>
<p>If the system is non-homogenous, ie $f(x) \neq 0$, then a <em>particular integral</em> is needed too, and the solution will have form $y = c.f. + p.i.$. The particular integral is found using a trial solution, then substituting it into the equation to find the coefficients. Note that if the particular integral takes the same form as the complementary function, an extra $x$ will need to be added to the particular integral for it to work, it $ae^{kx}$ would become $axe^{kx}$</p>
<table><thead><tr><th>$f(x)$</th><th>Trial Solution</th></tr></thead><tbody>
<tr><td>const $k$</td><td>const $\alpha$</td></tr>
<tr><td>polynomial $ax^r +...+ bx + c$</td><td>$\alpha x^r + ... + \beta x + \gamma$</td></tr>
<tr><td>$a\cos kx$ or $a\sin kx$</td><td>$\alpha\cos kx + \beta\sin kx$</td></tr>
<tr><td>$ae^{kx}$</td><td>$\alpha e^{kx}$</td></tr>
</tbody></table>
<h3 id="example-14"><a class="header" href="#example-14">Example</a></h3>
<p>$$\frac{d^2y}{dx^2} + 4y = \sin x \qquad y(0) = 1, ; \frac{dy}{dx}(0) = 1$$</p>
<p>Auxillary equation:
$$k^2 + 4 = 0$$
$$k = \pm 2i$$</p>
<p>Complementary function is therefore:
$$y = A\cos 2x + B\sin 2x$$</p>
<p>System is non-homogeneous, so have to find a particular integral. For this equation $f(x) = \sin x$, so the p.i. is $y = a\cos x + b \sin x$.
$$y = a \cos x + b\sin x$$
$$\frac{dy}{dx} = -a \sin x + b\cos x$$
$$\frac{d^2 y}{dx^2} = -a \cos x -b \sin x$$</p>
<p>Substituting this into the original equation:
$$\frac{d^2y}{dx^2} + 4y = -a \cos x -b \sin x + 4a \cos x + 4b\sin x = \sin x$$</p>
<p>Comparing coefficients:
$$\cos x: \quad 3a = 0 \Rightarrow a = 0$$
$$\sin x: \quad 3b = 1 \Rightarrow b = 1/3$$</p>
<p>The general solution is therefore:
$$y = c.f. + p.i. = A\cos 2x + B\sin 2x + \frac{1}{3} \sin x$$</p>
<p>Using initial conditions to find constants, for $y(0) = 1$
$$1 = A\cos 0 + B\sin 0 + \frac{1}{3} \sin 0 \Rightarrow A = 1 $$</p>
<p>For $\frac{dy}{dx}(0) = 1$
$$\frac{dy}{dx} = -2A \sin 2x + 2B \cos 2x + \frac{1}{3} cos x$$
$$1 = -2A \sin 0 + 2B \cos 0 + \frac{1}{3} cos 0$$
$$1 = 2B + \frac{1}{3} \Rightarrow B = \frac{1}{3}$$</p>
<p>Particular solution for given initial conditions is therefore:
$$y = \cos 2x + \frac{1}{3} \sin 2x + \frac{1}{3} sin x$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="laplace-transforms"><a class="header" href="#laplace-transforms">Laplace Transforms</a></h1>
<p>The laplace transform transforms a function from the time domain to the laplace domain. For a continuous function $f(t)$. with $t \geq 0 $, the laplace transform is defined as 
$$\mathcal{L} { f(t)}(s) = \int_0^{\infty}e^{-st}f(t) ,dt$$</p>
<p>The notation used is
$$\mathcal{L} { f(t)} = F(s)$$
Where $F(s)$ is the function in the laplace domain. Tables of laplace transforms for common functions are given in the formula book, so there is no need to work out most transforms manually.</p>
<p>Transforms are linearly independent in the same way integrals are:
$$\mathcal{L}{4x + 2y} = 4\mathcal{L}{x} + 2\mathcal{L}{y}$$</p>
<p>For example, find the laplace transform of $f(t) = \frac{1}{2}e^{-2t} + 3\sin 4t$:
$$\mathcal{L} { f(t)} = \frac{1}{2}\mathcal{L} { e^{-2t}} + 3 \mathcal{L} { \sin 4t}$$
$$= \frac{1}{2} \frac{1}{s+2} + 3 \times \frac{4}{s^2 + 16} = \frac{1}{2s + 4} + \frac{12}{s^2 + 16}$$</p>
<h2 id="inverse-transforms"><a class="header" href="#inverse-transforms">Inverse Transforms</a></h2>
<p>Transforms also have an inverse:
$$f(t) = \mathcal{L}^{-1} { F(s)}$$</p>
<p>For example, find $f(t)$ from $F(s) = \frac{3}{s^2 + 25 + 5}$
$$f(t) = \mathcal{L}^{-1} \left{ \frac{3}{s^2 + 25 + 5}\right} = \mathcal{L}^{-1} \left{ \frac{3}{(s+1)^2 + 4}\right}$$
$$ = \frac{3}{2}\mathcal{L}^{-1} \left{ \frac{2}{(s+1)^2 + 4}\right} = \frac{3}{2} e^{-t}\sin 2t$$</p>
<p>Sometimes, partial fractions and/or completing the square is required to get the equation into a form recognisable from the table.</p>
<h2 id="differential-equations-1"><a class="header" href="#differential-equations-1">Differential Equations</a></h2>
<p>Laplace transforms exist of derivatives:</p>
<p>$$\mathcal{L} \left{ \frac{dx}{dt} \right} = sX(s) - x(0)$$</p>
<p>$$\mathcal{L} \left{ \frac{d^2x}{dt^2} \right} = s^2X(s) - sx(0) - \frac{dx}{dt}(0)$$</p>
<p>This can be used to solved differential equations, by laplace transforming the differential equation to make an algebraic one, then inverse laplace transforming the result back.</p>
<h3 id="example-15"><a class="header" href="#example-15">Example</a></h3>
<p>Solve:</p>
<p>$$\ddot y + 3 \dot y + 2y = e^t \qquad y(0) = 1 \quad \dot y(0) = 1$$
$$\mathcal{L} { \ddot y + 3 \dot y + 2y }= \mathcal{L} {e^t} $$
$$s^2 Y(s) - sy(0) - \dot y(0) + 3(sY(s) - y(0)) + 2Y(s) = \frac{1}{s-1}$$
$$s^2 Y(s) - s - 1 + 3sY(s) - 3 + 2Y(s) = \frac{1}{s-1}$$
$$(s^2 + 3s + 2)(Y(s)) - s - 4 = \frac{1}{s-1}$$
$$Y(s) = \frac{1}{(s-1)(s^2 + 3s + 2)} + \frac{s+4}{(s^2 + 3s + 2)} = \frac{s^2 + 3s - 3}{(s-1)(s+1)(s+2)}$$</p>
<p>Need to use partial fractions to inverse transform
$$\frac{s^2 + 3s - 3}{(s-1)(s+1)(s+2)} = \frac A {s-1} +  \frac B {s+1} + \frac C {s+2}$$
$$s^2 + 3s - 3 = A(s+1)(s+2) + B(s-1)(s+2) + C(s+1)(s-1)$$
$$s = 1 \Rightarrow A= \frac 1 6$$
$$s = -1 \Rightarrow B = \frac 5 2$$
$$s = -2 \Rightarrow C = \frac {-5} 3 $$</p>
<p>$$Y(s) = \frac 1 6 \cdot \frac 1 {(s-1)} + \frac 5 2 \cdot \frac 1 {(s+1)} - \frac 5 3 \cdot \frac 1 {(s+2)}$$</p>
<p>Taking inverse laplace transforms using table:
$$y(t) = \frac 1 6 e^t + \frac 5 2 e^{-t} - \frac 5 3 e^{-2t}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="probability--statistics"><a class="header" href="#probability--statistics">Probability &amp; Statistics</a></h1>
<h2 id="probability"><a class="header" href="#probability">Probability</a></h2>
<h3 id="set-theory"><a class="header" href="#set-theory">Set Theory</a></h3>
<ul>
<li>A set is a collection of elements
<ul>
<li>Elements are members of a set</li>
</ul>
</li>
<li>$s \in S$ means &quot;the element $s$ is a member of the set $S$</li>
<li>The empty set $\varnothing$ contains no elements
<ul>
<li>It is empty</li>
</ul>
</li>
<li>$S = {1,3,5,7,9}$
<ul>
<li>$S$ is a set consisting of those integers</li>
</ul>
</li>
<li>$S = {n: n ;\text{is a prime number and}; n \leq 12}$
<ul>
<li>$S = {1,2,3,5,7,11}$</li>
</ul>
</li>
<li>$S = {x: x^2 = 4 ;\text{and}; x ;\text{is odd}}$
<ul>
<li>$S = \varnothing$</li>
</ul>
</li>
<li>$A \subset S$
<ul>
<li>$A$ is a subset of $S$</li>
<li>$a \in A$ implies $a \in S$</li>
</ul>
</li>
<li>$\varnothing \in S$ for all sets $S$</li>
<li>$A = B$ if and only if $A \subset B$ and $B \subset A$</li>
<li>$A \cup B$ is the union of $A$ and $B$
<ul>
<li>Set of elements belonging to $A$ <em>or</em> $B$</li>
</ul>
</li>
<li>$A \cap B$ is the intersection of $A$ and $B$
<ul>
<li>Set of elements belonging to $A$ <em>and</em> $B$</li>
</ul>
</li>
<li>Disjoint sets have no common elements
<ul>
<li>$A \cap B = \varnothing$</li>
</ul>
</li>
<li>$A \setminus B$ is the different of $A$ and $B$
<ul>
<li>Set of elements belonging to <em>$A$ but not $B$</em></li>
</ul>
</li>
<li>$A^c$ is the complement of $A$
<ul>
<li>Set of elements <em>not</em> belonging to $A$</li>
</ul>
</li>
</ul>
<h3 id="random-processes--probability"><a class="header" href="#random-processes--probability">Random Processes &amp; Probability</a></h3>
<p>The probability of event $A$ occurring is denoted $P(A)$. This is the relative frequency of event $A \in S$ occurring in a random process within sample space S.</p>
<ul>
<li>$S$
<ul>
<li>Certain or sure event, guaranteed 100% to happen</li>
</ul>
</li>
<li>$\varnothing$
<ul>
<li>Impossible event, won't happen</li>
</ul>
</li>
<li>${a} \in S$
<ul>
<li>Elementary event, the only event that can happen, the only possible outcome</li>
</ul>
</li>
<li>$A \cup B$
<ul>
<li>Event that occurs if <em>$A$ or $B$</em> occurs</li>
</ul>
</li>
<li>$A \cap B$
<ul>
<li>Event that occurs if <em>$A$ and $B$</em> occur</li>
</ul>
</li>
<li>$A^c = S \setminus A$
<ul>
<li>Event that occurs if $A$ <em>does not</em> occur</li>
</ul>
</li>
<li>$A \cup B = \varnothing$
<ul>
<li>Events $A$ and $B$ are <em>mutually exclusive</em></li>
</ul>
</li>
</ul>
<h4 id="example-16"><a class="header" href="#example-16">Example</a></h4>
<p>Toss a coin 3 times and observe the sequence of heads and tails.</p>
<ul>
<li>Sample space $S ={\text{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}}$</li>
<li>Event that $\geq 2$ heads occur in succession $A = {\text{HHH,HHT,THH}}$</li>
<li>Event that 3 heads or 3 tails occur $B = {\text{HHH,TTT}}$
<br><br></li>
<li>$A \cup B = { \text{HHH,HHT,THH,TTT}}$</li>
<li>$A \cap B = {\text{HHH}}$</li>
<li>$A^c = {\text{HTH,HTT,THT,TTH,TTT}}$</li>
<li>$A^c \cup B ={\text{TTT}}$</li>
</ul>
<h4 id="another-example-2"><a class="header" href="#another-example-2">Another Example</a></h4>
<p>Sample space $S={17,18,19,20,21,22}$. Each number is an individual event.</p>
<table><thead><tr><th>Events</th><th>Frequency</th><th>Relative Frequency</th></tr></thead><tbody>
<tr><td>17</td><td>3</td><td>3/35</td></tr>
<tr><td>18</td><td>4</td><td>4/35</td></tr>
<tr><td>19</td><td>9</td><td>9/35</td></tr>
<tr><td>20</td><td>11</td><td>11/35</td></tr>
<tr><td>21</td><td>6</td><td>6/35</td></tr>
<tr><td>22</td><td>2</td><td>2/35</td></tr>
</tbody></table>
<h3 id="axioms--laws-of-probability"><a class="header" href="#axioms--laws-of-probability">Axioms &amp; Laws of Probability</a></h3>
<ul>
<li>$0 \leq P(A) \leq 1$ for all $ A \subset S$
<ul>
<li>Probabilities are always between 0 and 1 inclusive</li>
</ul>
</li>
<li>$P(S) = 1$
<ul>
<li>Probability of the certain event is 1</li>
</ul>
</li>
<li>If $A \cap B = \varnothing$ then $P(A \cup B) = P(A) + P(B)$
<ul>
<li>If two events are disjoint, then the probability of either occurring is equal to the sum of their two probabilities</li>
</ul>
</li>
<li>$P(\varnothing) = 0$
<ul>
<li>The probability of the impossible event is zero</li>
</ul>
</li>
<li>$P(A^c) = 1 - P(A)$
<ul>
<li>The probability of all the elements not in A occurring is the opposite of the probability of all the elements in A occurring</li>
</ul>
</li>
<li>If $A \subset B$, then $P(A) \leq P(B)$
<ul>
<li>The probability of A will always be less than or equal to the probability of B when A is a subset of B</li>
</ul>
</li>
<li>$P(A \setminus B) = P(A) - P(A \cup B)$
<ul>
<li>The probability of A minus B is equal to the probability of A minus the probability of A and B</li>
</ul>
</li>
<li>$P(A \cup B) = P(A) + P(B) - P(A \cap B)$
<ul>
<li>Probability of A or B is equal to probability of A plus the probability of B minus the probability of A and B</li>
<li>This is important</li>
</ul>
</li>
</ul>
<h4 id="example-17"><a class="header" href="#example-17">Example</a></h4>
<p>In a batch of 50 ball bearings:</p>
<ul>
<li>15 have surface damage ($A$)
<ul>
<li>$P(A) = 0.3$</li>
</ul>
</li>
<li>12 have dents ($B$)
<ul>
<li>$P(B) = 0.24$</li>
</ul>
</li>
<li>6 both have defects ($A \cap B$)
<ul>
<li>$P(A \cap B) = 0.12$</li>
</ul>
</li>
</ul>
<p>The probability a single ball bearing has surface damage or dents:
$$P(A \cup B) = P(A) + P(B) - P(A \cap B) = 0.3 + 0.24 - 0.12 = 0.42$$</p>
<p>The probability a single ball bearing has surface damage but no dents:
$$P(A \cap B^c) = P(A \setminus B) = P(A) - P(A \cap B) = 0.3 - 0.12 = 0.18$$</p>
<h3 id="conditional-probability--bayes-theorem"><a class="header" href="#conditional-probability--bayes-theorem">Conditional Probability &amp; Bayes' Theorem</a></h3>
<p>A conditional probability $P(A | B)$ is the probability of event $A$ occurring, <em>given</em> that the event $B$ has occurred.</p>
<p>$$P(A|B) = \frac{P(A \cap B)}{P(B)}$$</p>
<p>Bayes' theorem:</p>
<p>$$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$$</p>
<p>Axioms of conditional probability:</p>
<ul>
<li>$P(B) = P(B|A)P(A) + P(B|A^c)P(A^c)$</li>
<li>$P(A \cup B | C) = P(A|C) + P(B|C) - P(A \cap B | C)$</li>
</ul>
<h4 id="example-18"><a class="header" href="#example-18">Example</a></h4>
<p>In a semiconductor manufacturing process:</p>
<ul>
<li>$A$ is the event that chips are contaminated
<ul>
<li>$P(A) = 0.2$</li>
</ul>
</li>
<li>$F$ is the event that the product containing the chip fails
<ul>
<li>$P(F|A) = 0.1$ and $P(F|A^c) = 0.005$</li>
</ul>
</li>
</ul>
<p>Determining the rate of failure:
$P(F) = P(F|A)P(A) + P(F|A^c)P(A^c) = 0.1 \times 0.2 + 0.005 \times 0.8 = 0.024$</p>
<h3 id="independent-events"><a class="header" href="#independent-events">Independent Events</a></h3>
<p>Two events are independent when the probability of one occurring does not dependend on the occurrence of the other. An event $A$ is independent if and only if
$$P(A \cap B) = P(A)P(B)$$</p>
<h4 id="example-19"><a class="header" href="#example-19">Example</a></h4>
<p>Using the coin flip example again with a sample space $S$ and 3 events $A, B, C$</p>
<ul>
<li>$S = {\text{HHH, HHT, HTH, HTT, THH, THT, TTH, TTT}}$
<ul>
<li>$P(S) = 1$</li>
</ul>
</li>
<li>$A = {\text{HHH, HHT, HTH, HTT}}$
<ul>
<li>$P(A) = 0.5$</li>
</ul>
</li>
<li>$B = {\text{HHH, HHT, THH, THT}}$
<ul>
<li>$P(B) = 0.5$</li>
</ul>
</li>
<li>$C = {\text{HHT, THH}}$
<ul>
<li>$P(C) = 0.25$</li>
</ul>
</li>
</ul>
<p>A and C are independent events:</p>
<ul>
<li>$A \cap C = {\text{HHT}}$</li>
<li>$P(A \cap C) = 0.25 = 0.5 \times 0.25 = P(A)P(C)$</li>
</ul>
<p>B and C are not independent events:</p>
<ul>
<li>$B \cap C = {\text{HHT,THH}}$</li>
<li>$P(B \cap C) = 0.25 \neq 0.25 \times 0.5 = P(B)P(C)$</li>
</ul>
<h2 id="discrete-random-variables"><a class="header" href="#discrete-random-variables">Discrete Random Variables</a></h2>
<p>For a random process with a <em>discrete</em> sample space $S$, a discrete random variable $X$ is a function that assigns a real number to each outcome $s \in S$.</p>
<ul>
<li>$X$ is a measure related to the random distribution.</li>
<li>Denoted $P(X = a)$</li>
</ul>
<p>Consider a weighted coin where $P(H) = 0.75$ and $P(T) = 0.25$. Tossing the coin twice gives a sample space $S = {\text{TT, TH, HT, HH}}$, which makes the number of heads a random variable $X(s) = {0, 1, 2}$. Since successive coin tosses are independent events:</p>
<ul>
<li>$P(TT) = 0.0625$</li>
<li>$P(TH) = 0.1875$</li>
<li>$P(HT) = 0.1875$</li>
<li>$P(HH) = 0.5625$</li>
</ul>
<p>Events are also mutually exclusive, so:</p>
<ul>
<li>$f(0) = P(TT) = 0.0625$</li>
<li>$f(1) = P(TH) + P(HT) = 0.375$</li>
<li>$f(2) = P(HH) = 0.5625$</li>
</ul>
<p>This gives a probability distribution function $f(x) = P(X=x)$ of:</p>
<table><thead><tr><th>$x$</th><th>$f(x)$</th></tr></thead><tbody>
<tr><td>$0$</td><td>$0.0625$</td></tr>
<tr><td>$1$</td><td>$0.375$</td></tr>
<tr><td>$2$</td><td>$0.5625$</td></tr>
</tbody></table>
<h3 id="cumulative-distribution-functions"><a class="header" href="#cumulative-distribution-functions">Cumulative Distribution Functions</a></h3>
<p>The cumulative probability function gives a &quot;running probability&quot;
$$F_X(x_i) = P(X \leq x_i) = \sum^i_{j=1}f(x_j)$$</p>
<ul>
<li>if $x_i \leq x_j$ then $F_X(x_i) \leq F_X(x_j)$</li>
<li>$F_X(x_1) = f(x_1)$</li>
<li>$F_X(x_n) = 1$</li>
</ul>
<p>Using coin example again:</p>
<table><thead><tr><th>$x$</th><th>$F_X(x)$</th></tr></thead><tbody>
<tr><td>$0$</td><td>$0.0625$</td></tr>
<tr><td>$1$</td><td>$0.4375$</td></tr>
<tr><td>$2$</td><td>$1$</td></tr>
</tbody></table>
<h3 id="expectation--variance"><a class="header" href="#expectation--variance">Expectation &amp; Variance</a></h3>
<ul>
<li>Expectation is the average value, ie the value most likely to come up
<ul>
<li>The mean of $X$</li>
</ul>
</li>
</ul>
<p>$$E(X) = \sum^n_{i=1} x_if(x_i) = \mu_X$$</p>
<ul>
<li>Variance is a measure of the spread of the data</li>
</ul>
<p>$$Var(X) = \sum^n_{i=1}(x_i - \mu_x)^2 f(x_i) = E(X^2) - (E(X))^2 = \sigma^2_X$$</p>
<ul>
<li>Standard deviation $\sigma_X = \sqrt{Var(X)}$</li>
</ul>
<p>Using the weighted coin example once more:</p>
<p>$$E(X) = 0 \times 0.0625 + 1 \times 0.375 + 2 \times 0.5625  = 1.5$$
$$E(X^2) =  0^2 \times 0.0625 + 1^2 \times 0.375 + 2^2 \times 0.5625 = 2.625 $$
$$Var(X) = E(X^2) - (E(X))^2 = 1.5 - 2.625^2 = 0.375$$</p>
<h3 id="standardised-random-variable"><a class="header" href="#standardised-random-variable">Standardised Random Variable</a></h3>
<p>The standardised random variable is a normalised version of the discrete random variable, obtained by the following transformation:
$$X^* = \frac{X - \mu_X}{\sigma_X}$$</p>
<ul>
<li>$E(X^*) = 0$</li>
<li>$Var(X^*) =1$</li>
</ul>
<h2 id="binomial-distribution"><a class="header" href="#binomial-distribution">Binomial Distribution</a></h2>
<ul>
<li>The binomial distribution models random processes consisting of repeated <em>independent</em> events</li>
<li>Each event has only 2 outcomes, success or failure
<ul>
<li>$P(success) = p$</li>
<li>$P(failure) = q = 1 - p$</li>
</ul>
</li>
</ul>
<p>The probability of $k$ successes in $n$ events:</p>
<p>$$b(k;n;p) = {n \choose k}p^k q^{n-k}, ;; k = 0,1,2,...,n$$</p>
<ul>
<li>Probability of no success $= q^n$</li>
<li>Probability of $\geq 1 $ successes is $1 - q^n$</li>
</ul>
<h3 id="expectation--variance-1"><a class="header" href="#expectation--variance-1">Expectation &amp; Variance</a></h3>
<p>$$\mu = np$$
$$\sigma^2 = npq$$</p>
<h3 id="example-20"><a class="header" href="#example-20">Example</a></h3>
<p>A fair coin is tossed 6 times.
$$p = q = 0.5$$</p>
<p>Probability of exactly 2 heads out of 6
$$b(2;6;0.5) = {6 \choose 2} \times 0.5 ^2 \times 0.5 ^4 = \frac{15}{64}$$</p>
<p>Probability of $\geq 1$ heads
$$1 - q^6 = 1 - 0.5 ^6 = \frac{63}{64}$$</p>
<p>Probability of $\geq 4$ heads</p>
<p>$$b(4;6;0.5) + b(5;6;0.5) + b(6;6;0.5) = {6 \choose 2} (\frac{1}{2})^4 (\frac{1}{2})^2 + {6 \choose 2} (\frac{1}{2})^5 (\frac{1}{2})^1 + (\frac{1}{2})^6 = \frac{11}{32}$$</p>
<p>Expected value $E(X)$
$$\mu = np = 6 \times 0.5 = 3$$</p>
<p>Variance
$$\sigma^2 = npq = 6 \times 0.5 \times 0.5 = 1.5$$</p>
<h2 id="poisson-distribution"><a class="header" href="#poisson-distribution">Poisson Distribution</a></h2>
<p>Models a random process consisting of repeated occurrence of a single event within a fixed interval. The probability of $k$ occurrences is given by
$$p(k;\lambda) = \frac{\lambda^k}{k!}e^{-\lambda}, ;; k = 0,1,2,...$$</p>
<p>The poisson distribution can be used to approximate the binomial distribution with $\lambda = np$. This is only valid for large $n$ and small $p$</p>
<h3 id="expectation--variance-2"><a class="header" href="#expectation--variance-2">Expectation &amp; Variance</a></h3>
<p>$$\mu = \sigma^2 = \lambda$$</p>
<h3 id="example-21"><a class="header" href="#example-21">Example</a></h3>
<p>The occurrence of typos on a page is modelled by a poisson distribution with $\lambda = 0.5$.</p>
<p>The probability of 2 errors:
$$p(2;0.5) = \frac{0.5^2}{2!}e^{-0.5} = 0.076$$</p>
<h2 id="continuous-random-variables"><a class="header" href="#continuous-random-variables">Continuous Random Variables</a></h2>
<p>Continuous random variables map events from a sample space to an interval. Probabilities are written $P(a \leq X \leq b)$, where $X$ is the random variable. $X$ is defined with a continuous function, the probability density function.</p>
<ul>
<li>The function must be positive
<ul>
<li>$f(x) \geq 0$</li>
</ul>
</li>
<li>The total area under the curve of the function must be 1
<ul>
<li>$\int^{\infty}_{-\infty} f(x) dx = 1$</li>
</ul>
</li>
<li>$P(a \leq X \leq b) = \int^b_a f(x) dx$</li>
</ul>
<h3 id="example-22"><a class="header" href="#example-22">Example</a></h3>
<p>$$
f(x) =
\begin{cases}
a (x - x^2) &amp; 0 \leq x \leq 1 \
0 &amp; otherwise
\end{cases}
$$</p>
<p>Require that $\int^{\infty}<em>{-\infty} f(x) dx = 1$, so have to find $a$:
$$\int^{\infty}</em>{-\infty} f(x) dx = \int^1_0 a (x - x^2) dx = a \left[\frac{x^2}{2} - \frac{x^3}{3}\right]^1_0 = \frac{a}{6} \Rightarrow a = 6$$</p>
<p>Calculating some probabilities:
$$P(0 \leq X \leq 0.5) = \int_0^{0.5} f(x) dx = \int_0^{0.5} 6(x - x^2) dx = 6 \left[\frac{x^2}{2} - \frac{x^3}{3}\right]<em>0^{0.5} = 0.5$$
$$P(0.25 \leq X \leq 0.75) = \int</em>{0.25}^{0.75} f(x) dx = \int_{0.25}^{0.75} 6(x - x^2) dx = 6 \left[\frac{x^2}{2} - \frac{x^3}{3}\right]_{0.25}^{0.75} = \frac{11}{16}$$</p>
<h3 id="cumulative-distribution-function"><a class="header" href="#cumulative-distribution-function">Cumulative Distribution Function</a></h3>
<p>The cumulative distribution function $F_X$ up to the point $a$ is given as
$$F_X(a) = \int_{-\infty}^a f(x) dx$$</p>
<ul>
<li>if $a \leq b$, then $F_X(a) \leq F_X(b)$</li>
<li>$\lim_{ x \to -\infty} F_X(x) = 0$</li>
<li>$\lim_{ x \to \infty} F_X(x) = 1$</li>
<li>$\frac{d}{dx} F_X(x) = f(x)$
<ul>
<li>Derivative of cumulative distribution function is the probability distribution function</li>
</ul>
</li>
</ul>
<p>Using previous example, let $F_X(x) = \int_{-\infty}^x f(t) dt$. For $x &lt; 0$
$$F_X(x) = 0$$</p>
<p>For $0 \leq x \leq 1$
$$F_X(x) = \int_0^x 6(t-t^2) dt = 6 \left[\frac{t^2}{2} - \frac{t^3}{3}\right]_0^x = 3x^2 - 2x^3$$</p>
<p>For $x &gt; 1$
$$F_X(x) = \int_0^1 6(t-t^2) dt = 6 \left[\frac{t^2}{2} - \frac{t^3}{3}\right]_0^1 = 1$$</p>
<h3 id="expectation--variance-3"><a class="header" href="#expectation--variance-3">Expectation &amp; Variance</a></h3>
<p>Where $X$ is a continuous random variable:</p>
<p>$$E(X) = \int^{\infty}<em>{-\infty} x f(x) dx = \mu$$
$$Var(X) = \int^{\infty}</em>{-\infty} (x - \mu)^2 f(x) dx = \sigma^2_X = E(X^2) - \mu^2$$</p>
<h2 id="uniform-distribution"><a class="header" href="#uniform-distribution">Uniform Distribution</a></h2>
<p>A continuous distribution with p.d.f:</p>
<p>$$
f(x) =
\begin{cases}
\frac{1}{b-a} &amp; a \leq x \leq b \
0 &amp; otherwise
\end{cases}
$$</p>
<p><img src="es193/./img/uniform.png" alt="" /></p>
<p>Expectation and variance:</p>
<p>$$\mu = \frac{a+b}{2}$$
$$\sigma^2 = \frac{(b-a)^2}{12}$$</p>
<p>Cumulative distribution function:</p>
<p>$$
F_X(x) =
\begin{cases}
0 &amp; -\infty &lt; x &lt; a\
\frac{x-a}{b-a} &amp; a \leq x \leq b \
0 &amp; b &lt; x &lt; \infty&gt;&gt;
\end{cases}
$$</p>
<p><img src="es193/./img/uniform-cum.png" alt="" /></p>
<h2 id="exponential-distribution"><a class="header" href="#exponential-distribution">Exponential Distribution</a></h2>
<p>A continuous distribution with p.d.f:</p>
<p>$$
f(x) =
\begin{cases}
0 &amp; -\infty &lt; x &lt; 0 \
ve^{-vx} &amp; 0 \leq x &lt; \infty
\end{cases}
$$</p>
<p><img src="es193/./img/exponential.png" alt="" /></p>
<p>Expectation and variance:</p>
<p>$$\mu = \frac{1}{v}$$
$$\sigma^2 = \frac{1}{v^2}$$</p>
<p>Cumulative distribution function:</p>
<p>$$
F_X(x) =
\begin{cases}
0 &amp; -\infty &lt; x &lt; 0\
1 - e^{-vx} &amp; 0 \leq x &lt; \infty \
\end{cases}
$$</p>
<p><img src="es193/./img/exponential-cum.png" alt="" /></p>
<ul>
<li>Recall that a discrete random process $X$ where a single event occurs $i$ times in a fixed interval is modelled by a Possion distribution $p(k;\lambda)$
<ul>
<li>$E(X) = \lambda$</li>
</ul>
</li>
<li>Consider a situation where the event occurs at a constant mean rate $v$ per unit time</li>
<li>Let $\lambda = vt$, then $P(0) = e^{-vt}$ and probability of $\geq 1$ events occurring is $1-e^{-vt}$</li>
<li>Suppose the <em>continuous</em> random variable $Y$ is the time between occurrences of successive events</li>
<li>If there is a period of time $t$ with no events, then $Y &gt; t$ and $P(Y &gt; t) = e^{-vt}$</li>
<li>If $\geq 1$ events occur then $Y \leq t$ and $P(Y \leq t) = 1 - e^{-vt}$</li>
</ul>
<p><strong>If the number of events per interval of time is Possion distributed, then the length of time between events is exponentially distributed</strong></p>
<h3 id="example-23"><a class="header" href="#example-23">Example</a></h3>
<p>Calls arrive randomly at the telephone exchange at a mean rate of 2 calls per minute. The number of calls per minute $X$ is a d.r.v. which can be modelled by a Poisson distribution with $\lambda = 2$. The probability of 1 call in any given minute is:</p>
<p>$$P(X = 1) = \frac{\lambda e^(-\lambda)}{1!} = 2e^{-2} = 0.27$$</p>
<p>The time between consecutive calls $Y$ is a c.r.v. modelled by an exponential distribution with $v = \frac{\lambda}{t} = \frac{2}{1} = 2$. The probability of at least 1 ($\geq 1$) minute between calls is:
$$p(1 \leq Y \leq \infty) = \int_1^{\infty} v e^{-vt}dt = \int_1^{\infty} 2 e^{-2t}dt = \left[-e^{-2t}\right]_1^{\infty} = 0.135$$</p>
<h2 id="normal-distribution"><a class="header" href="#normal-distribution">Normal Distribution</a></h2>
<p>A distribution with probability density function:</p>
<p>$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}$$</p>
<p>Expectation $E(X) = \mu$ and variance $Var(X) = \sigma^2$. Normal distribution is denoted $N(\mu,\sigma^2)$ and is defined by its mean and variance.</p>
<p><img src="es193/./img/normal.png" alt="" /></p>
<h3 id="standardised-normal-distribution"><a class="header" href="#standardised-normal-distribution">Standardised Normal Distribution</a></h3>
<p>$X$ is a random variable with distribution $N(\mu,\sigma^2)$. The standardised random variable $U$ is distributed $N(0,1)$ and can be obtained with the transform:
$$U = \frac{X - \mu}{\sigma}$$
and has p.d.f.
$$f(u) = \frac{1}{\sqrt{2\pi}} e^{-\frac{u^2}{2}}$$</p>
<p>$P(X \leq b) = P(U \leq \beta)$ where $\beta = \frac{b - \mu}{\sigma}$. Values for the standard normal distribution are tabulated in the data book.</p>
<h3 id="example-24"><a class="header" href="#example-24">Example</a></h3>
<p>The length of bolts $x$ from a production process are distributed normally with $\mu = 2.5$ and $\sigma^2 = 0.01$.</p>
<p>$$u = \frac{x - \mu}{\sigma} = \frac{x - 2.5}{0.1}$$
The probability the length of a bolt is between 2.6 and 2.7 cm (values obtained from table lookups):
$$P(2.6 \leq X \leq 2.7) = P(\frac{2.6- 2.5}{0.1} \leq U \leq \frac{2.7 - 2.5}{0.1}) = P(1 \leq U \leq 2)$$
$$=  P(0 \leq U \leq 2) - P(0 \leq U \leq 1) = 0.4772 - 0.3413 = 0.1359$$</p>
<h3 id="confidence-intervals"><a class="header" href="#confidence-intervals">Confidence Intervals</a></h3>
<p>A confidence interval is the interval in which we would expect to find an estimate of a parameter, at a specified probability level. For example, the interval covering 95% of the population of $N(\mu,\sigma^2)$ is $\mu \pm 1.96 \sigma$.</p>
<p>For a random variable $X$ with distribution $N(67.5,2.5^2)$, the standard variate $u = \frac{x - 67.5}{2.5}$. For confidence interval at 95% probability:</p>
<p>$$Q(u) = \frac{0.95}{2} = 0.475$$</p>
<p>Using table lookups, $u = \pm 1.96$, and:
$$x = \mu \pm 1.96 \sigma = 67.5 \pm 1.96 \times 2.5 = 67.5 \pm 4.9$$</p>
<p>For confidence interval at 99.9% probability:</p>
<p>$$Q(u) = \frac{0.999}{2} = 0.4995$$</p>
<p>Table lookups again, $u = \pm 3.3$, and:
$$x = \mu \pm 3.3 \sigma = 67.5 \pm 3.3 \times 2.5 = 67.5 \pm 8.25$$</p>
<h3 id="normal-approximation-to-binomial-distribution"><a class="header" href="#normal-approximation-to-binomial-distribution">Normal Approximation to Binomial Distribution</a></h3>
<p>The normal distribution gives a close approximation to the binomial distribution, provided:</p>
<ul>
<li>$n$ is large</li>
<li>neither $p$ nor $q$ are close to zero</li>
<li>$\mu = np$ and $\sigma^2 = npq$</li>
</ul>
<p>For example, take a random process consitsting of 64 spins of a fair coin $n = 64$ and $p = q = 0.5$. The probability of 40 heads is:
$$P(40) = {60 \choose 40} \times 0.5^{64} = 0.01359 $$
$$\mu = np = 32, ;; \sigma = \sqrt{npq} = 4$$</p>
<p>For a normal approximation, must use the interval around 40 (normal is continuous, binomial is discrete) $[39.5,40.5]$:</p>
<p>$$P(39.5 \leq X \leq 40.5) = P(\frac{39.5 - 32}{4} \leq X \leq \frac{39.5 - 32}{4}) = 0.4832 - 0.4696 = 0.0136$$</p>
<h3 id="normal-approximation-to-poisson-distribution"><a class="header" href="#normal-approximation-to-poisson-distribution">Normal Approximation to Poisson Distribution</a></h3>
<p>The normal distribution gives a close approximation to the binomial distribution, provided:</p>
<ul>
<li>$\lambda$ is large</li>
<li>$\mu = \sigma^2 = np$</li>
</ul>
<p>For example, say a radioactive decay emits a mean of 69 particles per seconds. A standard normal approximation to this is:</p>
<p>$$u = \frac{x - \mu}{\sigma} = \frac{x-69}{\sqrt{69}}$$</p>
<p>The probability of emitting $\leq 60$ particles in a second is therefore:
$$P(0 \leq X \leq 60) = P(\frac{0-69}{\sqrt{69}} \leq X \leq \frac{60.5 - 69}{\sqrt{69}}) = 0.5 - 0.3473 = 0.1527$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="es197"><a class="header" href="#es197">ES197</a></h1>
<p>This section, similar to ES191, also aims to be fairly comprehensive as a reference. I probably won't cover much of the matlab/simulink stuff.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="translational-mechanical-systems"><a class="header" href="#translational-mechanical-systems">Translational Mechanical Systems</a></h1>
<ul>
<li>Translational systems involve movement in 1 dimension</li>
<li>For example, a the suspension in a car going over bumps going up and down</li>
<li>System diagrams can be used to represent systems</li>
</ul>
<p><img src="es197/./img/mech1-1.png" alt="" /></p>
<ul>
<li>Diagrams include:
<ul>
<li>Masses</li>
<li>Springs</li>
<li>Dampers</li>
</ul>
</li>
</ul>
<h2 id="elements"><a class="header" href="#elements">Elements</a></h2>
<p>There are element laws to model each of the three elements involved in mechanical systems. They are modelled using two key variables:</p>
<ul>
<li>Force $F(t)$ in newtons ($N$)</li>
<li>Displacement $x(t)$ in meters ($m$)
<ul>
<li>Also sometimes velocity $v(t) = \dot{x}$ in meters per second ($ms^{-1}$)</li>
</ul>
</li>
</ul>
<p>When modelling systems, some assumptions are made:</p>
<ul>
<li>Masses are all perfectly rigid</li>
<li>Springs and dampers have zero mass</li>
<li>All behaviour is assumed to be linear</li>
<li></li>
</ul>
<h3 id="mass"><a class="header" href="#mass">Mass</a></h3>
<ul>
<li>Stores kinetic/potential energy</li>
<li>Energy storage is reversible
<ul>
<li>Can put energy in OR take it out</li>
</ul>
</li>
</ul>
<p><img src="es197/./img/mass.png" alt="" /></p>
<p>Elemental equation (Newton's second law):</p>
<p>$$m \frac{d^2}{dt^2}x = m\ddot{x} = ma = f(t)$$</p>
<p>Kinetic energy stored:</p>
<p>$$W = \frac{1}{2}mv^2$$</p>
<h3 id="spring"><a class="header" href="#spring">Spring</a></h3>
<ul>
<li>Stores potential energy</li>
<li>Also reversible energy store
<ul>
<li>Can be stretched/compressed</li>
</ul>
</li>
</ul>
<p><img src="es197/./img/spring.png" alt="" /></p>
<p>Elemental equation (Hooke's law):</p>
<p>$$f(t) = k(x_1(t) - x_2(t))$$</p>
<p>The spring constant k has units $Nm^{-1}$. Energy Stored:</p>
<p>$$W = \frac{1}{2} k (x_1 - x_2)$$</p>
<p>In reality, springs are not perfectly linear as per hooke's law, so approximations are made. Any mechanical element that undergoes a change in shape can be described as a <em>stiffness element</em>, and therefore modelled as a spring.</p>
<h3 id="damper"><a class="header" href="#damper">Damper</a></h3>
<p>Dampers are used to reduce oscillation and introduce friction into a system.</p>
<ul>
<li>Dissapates energy as heat</li>
<li>Non reversible energy transfer</li>
<li>Takes energy out of the system</li>
</ul>
<p><img src="es197/./img/damper.png" alt="" /></p>
<p>Elemental equation:</p>
<p>$$f(t) = B(\dot{x_1} - \dot{x_2})$$</p>
<p>B is the damper constant and has units $Nsm^{-1}$</p>
<h2 id="interconnection-laws"><a class="header" href="#interconnection-laws">Interconnection Laws</a></h2>
<p>Compatibility Law</p>
<ul>
<li>Elemental velocities are identical at points of connection</li>
</ul>
<p>Equilibrium Law</p>
<ul>
<li>Sum of external forces acting on a body equals mass x acceleration</li>
<li>All forces acting on a body in equilibrium equals zero</li>
</ul>
<h3 id="fictitiousdalembert-forces"><a class="header" href="#fictitiousdalembert-forces">Fictitious/D'alembert Forces</a></h3>
<p>D'alembert principle is an alternative form of Newtons' second law, stating that the force on a body is equal to mass times acceleration: $F - ma = 0$. $-ma$ is the inertial, or fictitious force. When modelling systems, the <em>inertial force always opposes the direction of motion.</em></p>
<h2 id="example-25"><a class="header" href="#example-25">Example:</a></h2>
<p>Form a differential equation describing the system shown below.</p>
<p><img src="es197/./img/mech1-2.png" alt="" /></p>
<p>4 forces acting on the mass:</p>
<ul>
<li>Spring: $F = kx$</li>
<li>Damper: $F = B\dot{x}$</li>
<li>Inertial/Fictitious force: $F = m\ddot{x}$</li>
<li>The force being applied, $f(t)$</li>
</ul>
<p><img src="es197/./img/mech1-2.1.png" alt="" /></p>
<p>The forces all sum to zero:</p>
<p>$$f(t) - kx - B\dot{x} - m\ddot{x} = 0$$
$$f(t) = m\ddot{x} + B\dot{x} + kx$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rotational-mechanical-systems"><a class="header" href="#rotational-mechanical-systems">Rotational Mechanical Systems</a></h1>
<h2 id="dynamic-systems"><a class="header" href="#dynamic-systems">Dynamic Systems</a></h2>
<ul>
<li>A system is a set of interconnected elements which transfer energy between them</li>
<li>In a dynamic system, energy between elements varies with time</li>
<li>Systems interact with their environments through:
<ul>
<li>Input
<ul>
<li>System depends on</li>
<li>Do no affect environment</li>
</ul>
</li>
<li>Output
<ul>
<li>System does not depend on</li>
<li>Affects Environment</li>
</ul>
</li>
</ul>
</li>
<li>Mathematical models of dynamic systems are used to describe and predict behaviour</li>
<li>Models are <em>all, always</em> approximations</li>
</ul>
<h3 id="lumped-vs-distributed-systems"><a class="header" href="#lumped-vs-distributed-systems">Lumped vs Distributed Systems</a></h3>
<ul>
<li>In a lumped system, properties are concentrated at 1 or 2 points in an element
<ul>
<li>For example
<ul>
<li>Inelastic mass, force acts at centre of gravity</li>
<li>Massless spring, forces act at either end</li>
</ul>
</li>
<li>Modelled as an ODE</li>
<li>Time is only independent variable</li>
</ul>
</li>
<li>In a distributed system, properties vary throughout an element
<ul>
<li>For example, non-uniform mass</li>
<li>Time and position are both independent variables</li>
<li>Can be broken down into multiple lumped systems</li>
</ul>
</li>
</ul>
<h3 id="linear-vs-non-linear-systems"><a class="header" href="#linear-vs-non-linear-systems">Linear vs Non-Linear Systems</a></h3>
<ul>
<li>For non-linear systems, model is a non-linear differential equation</li>
<li>For linear systems, equation is linear</li>
<li>In a linear system, the resultant response of the system caused by two or more input signals is the sum of the responses which would have been caused by each input individually
<ul>
<li>This is not true in non-linear systems</li>
</ul>
</li>
</ul>
<h3 id="discrete-vs-continuous-models"><a class="header" href="#discrete-vs-continuous-models">Discrete vs Continuous Models</a></h3>
<ul>
<li>In discrete time systems, model is a <em>difference</em> equation
<ul>
<li>output happens at discrete time steps</li>
</ul>
</li>
<li>In continuous systems, model is a <em>differential</em> equation
<ul>
<li>output is a continuous function of the input</li>
</ul>
</li>
</ul>
<h2 id="rotational-systems"><a class="header" href="#rotational-systems">Rotational Systems</a></h2>
<p>Rotational systems are modelled using two basic variables:</p>
<ul>
<li>Torque $\tau$ measured in $Nm$
<ul>
<li>A twisting force</li>
<li>Analogous to force in Newtons</li>
</ul>
</li>
<li>Angular displacement $\theta$ measured in radians
<ul>
<li>Angular velocity $\omega = \dot{\theta}$</li>
<li>Analogous to displacement in meters</li>
</ul>
</li>
</ul>
<h2 id="element-laws"><a class="header" href="#element-laws">Element Laws</a></h2>
<h3 id="moment-of-inertia"><a class="header" href="#moment-of-inertia">Moment of Inertia</a></h3>
<ul>
<li>Rotational mass about an axis</li>
<li>Stores kinetic energy in a reversible form</li>
<li>Shown as rotating disc with inertia $J$, units $Kgm^2$</li>
</ul>
<p><img src="es197/./img/mech2-1.png" alt="" /></p>
<p>Elemental equation:
$$\tau (t) = J \frac{d^2}{dt^2}\theta(t) = J \ddot{\theta}(t)$$</p>
<p>Energy Stored:
$$W = \frac{1}{2} J \omega^2$$</p>
<p><strong><em>The force $J \ddot{\theta}$ acts in the opposite direction to the direction the mass is spinning</em></strong></p>
<h3 id="rotational-spring"><a class="header" href="#rotational-spring">Rotational Spring</a></h3>
<ul>
<li>Stores potential energy by twisting</li>
<li>Reversible energy store</li>
<li>Produced torque proportional to the angular displacement at either end of spring</li>
</ul>
<p><img src="es197/./img/rot-spring.png" alt="" /></p>
<p>Elemental Equation:</p>
<p>$$\tau(t) = k(\theta_1(t) - \theta_2(t))$$</p>
<p>Stored Energy:</p>
<p>$$W = \frac{1}{2} k (\theta_1(t) - \theta_2(t))^2$$</p>
<h3 id="rotational-damper"><a class="header" href="#rotational-damper">Rotational Damper</a></h3>
<ul>
<li>Dissapates energy as heat</li>
<li>Non-reversible</li>
<li>Energy dissapated $\propto$ angular velocity</li>
</ul>
<p>Elemental Equation:</p>
<p>$$\tau(t) = B(\omega_1(t) - \omega_2(t))$$</p>
<h2 id="interconnection-laws-1"><a class="header" href="#interconnection-laws-1">Interconnection Laws</a></h2>
<h3 id="compatibility-law"><a class="header" href="#compatibility-law">Compatibility Law</a></h3>
<p>Connected elements have the same rotational displacement and velocity</p>
<h3 id="interconnection-law"><a class="header" href="#interconnection-law">Interconnection Law</a></h3>
<p>D'alembert law for rotational systems:</p>
<p>$$\sum_i(\tau_{ext})_i - J\dot{\omega} = 0 $$</p>
<p>$J\dot{\omega}$ is considered an inertial/fictitious torque, so for a body in equilibrium, $\sum_i \tau_i = 0$.</p>
<h2 id="example-26"><a class="header" href="#example-26">Example</a></h2>
<p>Form an equation to model the system shown below.</p>
<p><img src="es197/./img/rot-ex.png" alt="" /></p>
<p>4 torques acting upon the disk:</p>
<ul>
<li>Stiffness element, $\tau=k\theta$</li>
<li>Friction element, $\tau=B\dot{\theta}$</li>
<li>Input torque $\tau(t)$</li>
<li>Inertial force $\tau = J\ddot{\theta}$</li>
</ul>
<p><img src="es197/./img/rot-ex1.png" alt="" /></p>
<p>The forces sum to zero, so:</p>
<p>$$ \tau(t) - k\theta - B\dot{\theta} - J\ddot{\theta} = 0$$</p>
<p>$$
\tau(t) = J\ddot{\theta}(t) + B\dot{\theta}(t)  + k\theta(t)
$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="electrical-systems"><a class="header" href="#electrical-systems">Electrical Systems</a></h1>
<p>Similar to mechanical systems, models of electrical systems can be constructed. Similar deal to ES191.</p>
<h2 id="variables"><a class="header" href="#variables">Variables</a></h2>
<ul>
<li>Current $I(t)$ in amps (A)</li>
<li>Voltage $e(t)$ in volts (V) -- <em>not v for voltage, e is used in systems</em></li>
<li>Power in watts $P = I(t)\cdot e(t)$</li>
</ul>
<h2 id="elements-1"><a class="header" href="#elements-1">Elements</a></h2>
<h3 id="capacitors-2"><a class="header" href="#capacitors-2">Capacitors</a></h3>
<ul>
<li>Store electrical energy in a reversible form</li>
<li>Capacitance $C$ measured in Farads (L)</li>
</ul>
<p><img src="es197/./img/capacitor.png" alt="" /></p>
<p>Elemental equation:</p>
<p>$$I(t) = C \frac{d}{dt}e_{12}(t)$$</p>
<p>Energy stored:
$$W = \frac{1}{2}Ce^2$$</p>
<h3 id="inductors-2"><a class="header" href="#inductors-2">Inductors</a></h3>
<ul>
<li>Store magnetic energy in a reversible form</li>
<li>Inductance $L$ measured in Henries (H)</li>
</ul>
<p><img src="es197/./img/inductor.png" alt="" /></p>
<p>Elemental equation:
$$e_{12}(t) = L \frac{d}{dt}I(t)$$</p>
<p>Energy Stored:
$$W = \frac{1}{2}LI^2$$</p>
<h3 id="resistors-1"><a class="header" href="#resistors-1">Resistors</a></h3>
<ul>
<li>Dissapates energy
<ul>
<li>Non-reversible</li>
</ul>
</li>
<li>Resistance $R$ measured in Ohms ($\Omega$)</li>
</ul>
<p><img src="es197/./img/resistor.png" alt="" /></p>
<p>Elemental Equation (Ohm's law):
$$e_{12}(t) = I(t) \cdot R$$</p>
<h3 id="voltage-source"><a class="header" href="#voltage-source">Voltage Source</a></h3>
<ul>
<li>Provides an input of energy to the system.</li>
<li>Input voltage $e_i(t)$</li>
</ul>
<p><img src="es197/./img/source.png" alt="" /></p>
<h2 id="kirchhoffs-laws"><a class="header" href="#kirchhoffs-laws">Kirchhoff's Laws</a></h2>
<ul>
<li>Describe how elements interconnect and transfer energy between them</li>
<li>KVL - voltages around a closed loop sum to zero</li>
<li>KCL - currents about a node sum to zero</li>
</ul>
<h2 id="example-27"><a class="header" href="#example-27">Example</a></h2>
<p>Form a differential equation to model the following electrical system/circuit:</p>
<p><img src="es197/./img/elec-example.png" alt="" /></p>
<p>Elements:</p>
<ul>
<li>Resistor: $e_r = IR$</li>
<li>Capacitor: $I = C \frac{d}{dt}e_c$</li>
<li>Inductor: $e_L = L \frac{d}{dt}I$</li>
</ul>
<p>KVL - the voltages round the loop sum to zero:</p>
<p>$$e_i - e_r - e_l - e_o = 0$$
$$e_i - IR - L\frac{dI}{dt} - e_o = 0$$</p>
<p>Using the capacitor equation, and the fact that $e_o = e_c$:</p>
<p>$$e_i - RC \frac{d}{dt}e_o - LC \frac{d^2}{dt^2}e_o - e_o = 0$$
$$LC \frac{d^2}{dt^2}e_o(t) + RC \frac{d}{dt}e_o(t) + e_o(t) = e_i(t)$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="thermal-systems"><a class="header" href="#thermal-systems">Thermal Systems</a></h1>
<ul>
<li>User to model heat transfer
<ul>
<li>For example in a house</li>
<li>Or in electronic components</li>
</ul>
</li>
<li>Determine efficiency of elements</li>
<li>Determine thermal operating ranges for components</li>
</ul>
<h2 id="variables-1"><a class="header" href="#variables-1">Variables</a></h2>
<ul>
<li>
<p>Rate of heat flow $q(t)$ in watts ($Js^{-1}$)</p>
</li>
<li>
<p>Temperature, $\theta(t)$ in Kelvins (K)</p>
</li>
<li>
<p>Analogous to current and voltage in electrical systems</p>
</li>
</ul>
<h2 id="elements-2"><a class="header" href="#elements-2">Elements</a></h2>
<h3 id="thermal-capacitor"><a class="header" href="#thermal-capacitor">Thermal Capacitor</a></h3>
<ul>
<li>Stores heat energy in a reversible way</li>
</ul>
<p><img src="es197/./img/thermal-capacitor.png" alt="" /></p>
<p>Elemental equation:
$$q_c(t) = C \frac{d\theta}{dt}$$</p>
<p>Where $q_c(t)$ is the net heat flowing in, ie $q_{in}(t)-q_{out}(t)$.</p>
<h3 id="thermal-resistor"><a class="header" href="#thermal-resistor">Thermal Resistor</a></h3>
<ul>
<li>Dissapates heat
<ul>
<li>Non-reversible</li>
</ul>
</li>
</ul>
<p><img src="es197/./img/thermal-resistor.png" alt="" /></p>
<p>Any object that restricts heat flow when heat flows from on medium to another can be modelled as a resistor. Elemental equation:
$$q(t) = \frac{1}{R}\theta_{12}(t)$$</p>
<p>Where $q_t$ is the flow of heat from the temperature $\theta_1$ on one side of the resistor to the temperature $\theta_2$ on the other.</p>
<h2 id="interconnection-laws-2"><a class="header" href="#interconnection-laws-2">Interconnection Laws</a></h2>
<p>Compatibility Law:</p>
<ul>
<li>Temperatures are identical where elements touch,</li>
<li>$\theta_1 = \theta_2 = ... = \theta_n$</li>
</ul>
<p>Equilibrium Law:</p>
<ul>
<li>Elemental heat flow rates sum to zero at connection points</li>
<li>$q_1 + q_2 + ... + q_n = 0$</li>
</ul>
<h2 id="examples-2"><a class="header" href="#examples-2">Examples</a></h2>
<p>Develop a thermal model for someone doing winter sports. Assume:</p>
<ul>
<li>Ambient temperature $\theta_a$</li>
<li>Body temperature $\theta$</li>
<li>Thermal resistance between body and ambient (the person is wearing a coat) $R$</li>
<li>Heat generated by body $q_{in}$</li>
</ul>
<p>The rate of heat flow out is the difference in ambient and body temperature accross the resistor:
$$q_{out} = \frac{\theta-\theta_a}{R}$$</p>
<p>In the thermal capacitor, the net input heat is proportional to the rate of change of temperature:
$$q_{in}-q_{out} = C \frac{d\theta}{dt}$$</p>
<p><img src="es197/./img/thermal-ex.png" alt="" /></p>
<p>Combining the two equations gives:
$$RC \frac{d}{dt}\theta + \theta = R q_{in} + \theta_a$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="data-driven-models"><a class="header" href="#data-driven-models">Data Driven Models</a></h1>
<ul>
<li>A system model can be developed from data describing the system</li>
<li>Computational techniques can be used to fit data to a model</li>
</ul>
<h2 id="modelling-approaches"><a class="header" href="#modelling-approaches">Modelling Approaches</a></h2>
<h3 id="white-box"><a class="header" href="#white-box">White Box</a></h3>
<ul>
<li>A white box model is a physical modelling approach, used where all the information about a system and its components is known.</li>
<li>For example: &quot;What is the voltage accross a 10 $\Omega$ resistor?&quot;
<ul>
<li>The value of the resistor is known, so a mathematical model can be developed using knowledge of physics (Ohm's law in this case)</li>
<li>The model is then tested against data gathered from the system</li>
</ul>
</li>
</ul>
<h3 id="grey-box"><a class="header" href="#grey-box">Grey Box</a></h3>
<ul>
<li>A grey box model is similar to white box, except where some physical parameters are unknown</li>
<li>A model is developed using known physical properties, except some parameters are left unknown</li>
<li>Data is then collected from testing and used to find parameted</li>
<li>For example: &quot;What is the force required to stretch this spring by $x$mm, when the stiffness is unknown&quot;
<ul>
<li>Using knowledge, $F=Kx$</li>
<li>Test spring to collect data</li>
<li>Find value of $K$ that best fits the data to create a model</li>
<li>Final model is then tested</li>
</ul>
</li>
<li>Physical modelling used to get the <em>form</em> of the model, testing used to <em>find unknown parameters</em></li>
<li>This, and white box, is mostly what's been done so far</li>
</ul>
<h3 id="black-box"><a class="header" href="#black-box">Black box</a></h3>
<p><em>&quot;Here is a new battery. We know nothing about it. How does it performance respond to changes in temperature?&quot;</em></p>
<ul>
<li>Used to build models of a system where the internal operation of it is completely unknown: a &quot;black box&quot;</li>
<li>Data is collected from testing the system</li>
<li>An appropriate mathematical model is selected to fit the data</li>
<li>The model is fit to the data to test how good it is</li>
<li>The model is tested on new data to see how closely it models system behaviour</li>
</ul>
<h2 id="modelling-in-matlab"><a class="header" href="#modelling-in-matlab">Modelling in Matlab</a></h2>
<h3 id="regression"><a class="header" href="#regression">Regression</a></h3>
<ul>
<li>Regression is predicting a continuous response from a set of predictor values
<ul>
<li>eg, predict extension of a spring given force, temperature, age</li>
</ul>
</li>
<li>Learn a function that maps a set of predictor variables to a set of response variables</li>
</ul>
<p>For a linear model of some data $y = p_1x + p_0$:</p>
<ul>
<li>$y$ and $x$ are the predictor variables from the data set</li>
<li>$p_1$ and $p_0$ are the unknowns to be estimated from the data</li>
<li>Polynomial models can be used for more complex data</li>
</ul>
<h3 id="in-matlab"><a class="header" href="#in-matlab">In Matlab</a></h3>
<pre><code class="language-matlab">% data points
x = 0:0.1:1.0;
y = 2 * x + 3;
%introduce some noise into the data
y_noise = y + 0.1*randn(11,1)';

%see the data
figure;
plot(x,y_noise);
axis([0 1 0 5])
</code></pre>
<p>In matlab, the <code>polyfit</code> function (<a href="https://uk.mathworks.com/help/matlab/ref/polyfit.html">matlab docs</a>) is used to fit a polynomial model of a given degree to the data.</p>
<ul>
<li>Inputs: x data, y data, polynomial degree</li>
<li>Output: coefficients of model</li>
</ul>
<pre><code class="language-matlab">P = polyfit(x,y_noise,1) % linear model
hold on;
plot(x,polyval(P,x),'r');
</code></pre>
<p><img src="es197/./img/data-model.png" alt="" /></p>
<p>In the example shown, the model ended up as $y = 1.7456x + 3.0976$, which is close, but not exact due to noise introduced into the data.</p>
<h3 id="limitations"><a class="header" href="#limitations">Limitations</a></h3>
<ul>
<li>Too complex of a model can lead to overfitting, where the model contains unwanted noise</li>
<li>To overcome this:
<ul>
<li>Use simpler model</li>
<li>Collect more data</li>
</ul>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-order-step-response"><a class="header" href="#first-order-step-response">First Order Step Response</a></h1>
<p>Modelling is about predicting the behaviour of a system. Often, need to know</p>
<ul>
<li>What is the output for a given input?</li>
<li>Is the system stable?</li>
<li>If the input changes quickly, how will the output change?</li>
</ul>
<h2 id="first-order-systems"><a class="header" href="#first-order-systems">First Order Systems</a></h2>
<p>First order systems are those with only one energy store, and can be modelled by a first order differential equation.</p>
<table><thead><tr><th><em>Type</em></th><th><em>Equation</em></th></tr></thead><tbody>
<tr><td>Electrical</td><td>$RC \frac{de_o}{dt} + e_o = e_i$</td></tr>
<tr><td>Thermal</td><td>$RC \frac{d\theta}{dt} + \theta = \theta_a$</td></tr>
<tr><td>Mechanical</td><td>$M\dot{v} + Bv = f(t)$</td></tr>
<tr><td>General</td><td>$T \frac{dy}{dt} + y = x$</td></tr>
</tbody></table>
<p>For the general form of the equation $T \frac{dy}{dt} + y = x$, the solution for a step input $x=H$ at time $t=0$, with $y(0) = 0$:
$$y = H(1 - e^{-\frac{t}{T}})$$
T is the time constant of the system.</p>
<h2 id="free-and-forced-response"><a class="header" href="#free-and-forced-response">Free and Forced Response</a></h2>
<ul>
<li>Free response:
<ul>
<li>The response of a system to its stored energy when there is no input</li>
<li>Zero Input</li>
<li>Non-zero initial Conditions</li>
<li>Homogenous differential equation</li>
</ul>
</li>
<li>Forced response:
<ul>
<li>The response of a system to an input when there is no energy initially in the system</li>
<li>Non-zero input</li>
<li>Zero initial Conditions</li>
<li>Non-homogeneous differential equation</li>
</ul>
</li>
<li>Total system response is a linear combination of the two</li>
</ul>
<h2 id="system-inputs"><a class="header" href="#system-inputs">System Inputs</a></h2>
<p>Different inputs can be used to determine characteristics of the system.</p>
<h3 id="step-input"><a class="header" href="#step-input">Step Input</a></h3>
<p>$$
u(t) =
\begin{cases}
0  &amp; t &lt; 0 \
H  &amp; t \geq 0
\end{cases}
$$</p>
<ul>
<li>A sudden increase of a constant amplitude input</li>
<li>Can see how quickly the system responds</li>
<li>Is there is any delay/oscillation?</li>
<li>Is it stable?</li>
</ul>
<h3 id="sine-wave"><a class="header" href="#sine-wave">Sine Wave</a></h3>
<ul>
<li>Can vary frequency and amplitude</li>
<li>Shows frequency response of a system</li>
</ul>
<h3 id="impulse"><a class="header" href="#impulse">Impulse</a></h3>
<p>$$
u(t) =
\begin{cases}
0  &amp; t \neq 0 \
\infty  &amp; t = 0
\end{cases}
$$</p>
<ul>
<li>A spike of infinite magnitude at an infinitely small time step</li>
</ul>
<h3 id="ramp"><a class="header" href="#ramp">Ramp</a></h3>
<p>$$
u(t) =
\begin{cases}
0  &amp; t &lt; 0 \
kt  &amp; t \geq 0
\end{cases}
$$</p>
<ul>
<li>An input that starts increasing at a constant rate, starting at $t=0$.</li>
</ul>
<h2 id="step-response-2"><a class="header" href="#step-response-2">Step Response</a></h2>
<ul>
<li>The step response of the system is the output when given a step input
<ul>
<li>System must have zero initial conditions</li>
</ul>
</li>
<li>Characteristics of a response:
<ul>
<li>Final/resting value</li>
<li>Rise time</li>
<li>Delay</li>
<li>Overshoot</li>
<li>Oscillation (frequency &amp; damping factor)</li>
<li>Stability</li>
</ul>
</li>
</ul>
<p>For a system with time constant $T=10$, the response looks something like this:</p>
<p><img src="es197/./img/step.png" alt="" /></p>
<p>The time constant $T$ of a system determines how long the system takes to respond to step input. After 1 time constant, the system is at about $1-\frac{1}{e}$ (63) % of its final value.</p>
<table><thead><tr><th>Time (s)</th><th>% of final value</th></tr></thead><tbody>
<tr><td>$0.5T$</td><td>39.3%</td></tr>
<tr><td>$T$</td><td>63.2%</td></tr>
<tr><td>$2T$</td><td>86.5%</td></tr>
<tr><td>$3T$</td><td>95.0%</td></tr>
<tr><td>$4T$</td><td>98.2%</td></tr>
<tr><td>$5T$</td><td>99.3%</td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id="second-order-step-response"><a class="header" href="#second-order-step-response">Second Order Step Response</a></h1>
<p>How 2nd order systems (those with 2 energy storing elements) respond to step inputs.</p>
<h2 id="standard-form"><a class="header" href="#standard-form">Standard form</a></h2>
<p>$$\frac{1}{\omega_n^2} \frac{d^2}{dt^2}y(t) + \frac{2\zeta}{\omega_n} \frac{d}{dt} y(t) + y(t) = u(t)$$</p>
<ul>
<li>$\omega_n$ is the undamped frequency of the system response
<ul>
<li>Indicates the speed of the response</li>
</ul>
</li>
<li>$\zeta$ is the damping factor
<ul>
<li>Indicates the shape of the response</li>
</ul>
</li>
</ul>
<h2 id="forced-response"><a class="header" href="#forced-response">Forced Response</a></h2>
<ul>
<li>Forces response is the response to a non-zero input, namely
<ul>
<li>Step</li>
<li>Sinusoidal</li>
</ul>
</li>
<li>Initial conditions are zero, it $y(0) = 0$, $\frac{d}{dt}y(0) = 0$</li>
<li>The response is the solution to a non-homogeneous second order differential equation</li>
</ul>
<h2 id="damped-response"><a class="header" href="#damped-response">Damped Response</a></h2>
<p>There are 4 different cases for system response:</p>
<table><thead><tr><th>Damping Factor</th><th>Response</th></tr></thead><tbody>
<tr><td>$\zeta = 0$</td><td>No Damping</td></tr>
<tr><td>$0 &lt; \zeta &lt; 1$</td><td>Underdamped</td></tr>
<tr><td>$\zeta = 1$</td><td>Critically Damped</td></tr>
<tr><td>$\zeta &gt; 1$</td><td>Overdamped</td></tr>
</tbody></table>
<p>The response of a system to the same input with varying damping factors is shown in the graph below, from the data book. The equations are also given in the data book.</p>
<p><img src="es197/./img/2ndorder.png" alt="" /></p>
<h3 id="undamped"><a class="header" href="#undamped">Undamped</a></h3>
<p>The system is not damped at all and is just a normal sinusoidal wave.</p>
<p>$$y(t) = H(1-\cos \omega_n t)$$</p>
<h3 id="underdamping"><a class="header" href="#underdamping">Underdamping</a></h3>
<p>The amplitude of the sinusoidal output decreases slowly over time to a final &quot;steady state&quot; value.</p>
<p>$$y(t) = H [ 1 - \frac{e^{-\zeta \omega_n t}}{\sqrt{1-\zeta^2}} \sin(\sqrt{1-\zeta^2}\omega_nt + \phi)]$$
$$\tan \phi = \frac{\sqrt{1-\zeta^2}}{\zeta}$$</p>
<h3 id="critical-damping"><a class="header" href="#critical-damping">Critical Damping</a></h3>
<p>This gives the fastest response, where the output rises to its final steady state value.</p>
<p>$$y(t) = H (1-(\omega_nt)e^{-\omega_nt})$$</p>
<h3 id="overdamping"><a class="header" href="#overdamping">Overdamping</a></h3>
<p>The output rises slowly to its steady state value
$$y(t) = H [ 1 - \frac{e^{-\zeta \omega_n t}}{\sqrt{\zeta^2 -1}} \sinh(\sqrt{\zeta^2-1}\omega_nt + \phi)]$$
$$\tan \phi = \frac{\sqrt{\zeta^2-1}}{\zeta}$$</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transfer-functions-1"><a class="header" href="#transfer-functions-1">Transfer Functions</a></h1>
<ul>
<li>A transfer function is a representation of the system which maps from input to output
<ul>
<li>Useful for system analysis</li>
<li>Carried out in the <em>Laplace Domain</em></li>
</ul>
</li>
</ul>
<h2 id="the-laplace-domain"><a class="header" href="#the-laplace-domain">The Laplace Domain</a></h2>
<ul>
<li>Problems can be easier to solve in the Laplace domain, so the equation is Laplace transformed to make it easier to work with</li>
<li>Given a problem such as &quot;what is the output $y(t)$ given a differential equation in $y$ and the step input $u(t)$?&quot;
<ul>
<li>Express step input in Laplace domain $U(s)$</li>
<li>Express differential equation in Laplace domain and find transfer function $G(s)$</li>
<li>Find output $Y(s) = U(s)G(s)$ in Laplace domain</li>
<li>Transfer back to time domain to get $y(t)$</li>
</ul>
</li>
</ul>
<table><thead><tr><th>Function</th><th>Time domain</th><th>Laplace domain</th></tr></thead><tbody>
<tr><td>Input</td><td>$u(t)$</td><td>$U(s)$</td></tr>
<tr><td>Output</td><td>$y(t)$</td><td>$Y(s)$</td></tr>
<tr><td>Transfer</td><td>$g(t)$</td><td>$G(s)$</td></tr>
</tbody></table>
<p>The laplace domain is particularly useful in this case, as a differential equation in the time domain becomes an algebraic one in the Laplace domain.
$$\mathcal{L}(\frac{dy}{dx}) = sY(s) - y(0)$$</p>
<h2 id="transfer-function-definition"><a class="header" href="#transfer-function-definition">Transfer Function Definition</a></h2>
<p>The transfer function is the <strong>ratio of output to input</strong>, given <strong>zero initial conditions</strong>.
$$G(s) = \frac{Y(s)}{U(s)} $$</p>
<p>For a general first order system of the form
$$T \frac{d}{dt}y(t) + y(t) = u(t)$$</p>
<p>The transfer function in the Laplace domain can be derived as:
$$T \cdot \mathcal{L}(\frac{d}{dt}y(t)) + \mathcal{L}(y(t)) = \mathcal{L}(u(t))$$
$$T(sY(s)) + Y(s) = U(s)$$
$$Y(s)(Ts+1) = U(s)$$
$$G(s) = \frac{Y(s)}{U(s)} = \frac{1}{Ts+1}$$</p>
<h2 id="step-input-in-the-laplace-domain"><a class="header" href="#step-input-in-the-laplace-domain">Step Input in the Laplace Domain</a></h2>
<p>Step input has a constant value $H$ for $t &gt; 0$
$$\mathcal{L}(H) = \frac{H}{s} = U(s)$$</p>
<p>For a first order system, the output will therefore be:
$$Y(s) = U(s)G(s) = \frac{H}{s} \frac{1}{Ts+1}$$
$$y(t) = \mathcal{L}^{-1}(\frac{H}{s} \cdot \frac{1}{Ts+1}) = H(1 - e^{\frac{t}{T}})$$</p>
<h2 id="example-28"><a class="header" href="#example-28">Example</a></h2>
<p>Find the transfer function for the system shown:</p>
<p><img src="es197/./img/tf-ex1.png" alt="" /></p>
<p>The system has input-output equation (in standard form):
$$\frac{J}{B}\dot{\omega}(t) + \omega(t) = \frac{1}{B}\tau(t)$$</p>
<p>Taking the Laplace transform of both sides:
$$\frac{J}{B}s\Omega(s) + \Omega(s) = \frac{1}{B}\Tau(s)$$</p>
<p>Rearranging to obtain the transfer function:
$$G(s) = \frac{\Omega(s)}{\Tau(s)} = \frac{1}{B} \cdot \frac{1}{\frac{J}{B}s + 1} = \frac{1}{Js + B}$$</p>
<h2 id="using-matlab"><a class="header" href="#using-matlab">Using Matlab</a></h2>
<p>In matlab the <code>tf</code> function (<a href="https://uk.mathworks.com/help/control/ref/tf.html">Matlab docs</a>) can be used to generate a system model using it's transfer function. For example, those code below generates a transfer function $G(s) = \frac{1}{2s+3}$, and then plots it's response to a step input of amplitude 1.</p>
<pre><code class="language-matlab">G = tf([1],[2 3]);
step(G);
</code></pre>
<h3 id="example-29"><a class="header" href="#example-29">Example</a></h3>
<p>For the system shown below, where $M=100$, $B =40$, $K=100$, plot the step response and obtain the undamped natural frequency $\omega_n$ and damping factor $\zeta$.</p>
<p><img src="es197/./img/tf-ex2.png" alt="" /></p>
<p>$$G(s) = \frac{1}{s^2M +sB + K} = \frac{1}{100s^2+40s+100}$$</p>
<pre><code class="language-matlab">system = tf([1],[100 40 100]);
step(system, 15); % plot 15 seconds of the response

%function to obtain system parameters
[wn,z] = damp(system)
</code></pre>
<p>The script will output <code>wn=1</code>, and <code>z = 0.2</code>. The plotted step response will look like:</p>
<p><img src="es197/./img/tf-ex2.1.png" alt="" /></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="first-order-frequency-response"><a class="header" href="#first-order-frequency-response">First Order Frequency Response</a></h1>
<p>Frequency response is is the response of a system to a sinusoidal/oscillating input.</p>
<h2 id="response-to-sinusoidal-input"><a class="header" href="#response-to-sinusoidal-input">Response to Sinusoidal input</a></h2>
<p>For a standard first order system $T\frac{d}{dt}y(t) + y(t) = u(t)$, with a sinusoidal input $u(t) = Asin(\omega t)$:</p>
<p>$$U(s) = \mathcal{L}(u(t)) = \frac{A\omega}{s^2 + \omega^2}$$
$$Y(s) = U(s)G(s) = \frac{A\omega}{s^2 + \omega^2} \frac{1}{Ts+1}$$
$$y(t) =  \mathcal{L}^{-1}(Y(s)) = \frac{A}{\sqrt{1+\omega^2T^2}} sin(\omega t - \tan^{-1}\omega T) + \frac{A \omega T}{1 + \omega^2 T^2} e^{-\frac{t}{T}}$$</p>
<p>The sinusoidal part of the equation is the steady-state that the response tends to, and the exponential part is the transient part that represents the rate of decay of the offset of the oscillation.</p>
<ul>
<li>The frequency of input and output is always the same
<ul>
<li>It is the amplitude and phase shift $\phi$ that change</li>
<li>These depend on the input frequency $\omega$
<ul>
<li>This dependence <em>is</em> the frequency response</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="example-30"><a class="header" href="#example-30">Example</a></h3>
<p>The example below shows an input $sin(4 \pi t)$, and its output with $G(s) = \frac{1}{s+1}$</p>
<p><img src="es197/./img/freq1-example.png" alt="" /></p>
<p>$$y(t) = 0.0793sin(4\pi t + \phi) + 0.079 e^{-t}, ;; \phi = -\tan^{-1} 4\pi = -85^{\circ}$$</p>
<p>The steady state sinusoidal and transient exponential part of this response can be seen in the equation.</p>
<h3 id="matlab-example"><a class="header" href="#matlab-example">Matlab Example</a></h3>
<p>The following code generates the following plot</p>
<pre><code class="language-matlab">system = tf(1,[1 1]);
t = 0:0.01:3; % time value vector
u = (t&gt;=1).*sin(4 * pi * t) %input signal for t &gt;= 1
y = lsim(sys,u,t); % simulate system with input u

figure;
subplot(2,1,1); plot(t,u); title(&quot;input&quot;);
subplot(2,1,2); plot(t,y,'r'); title(&quot;outputA&quot;);
</code></pre>
<p><img src="es197/./img/freq1-matlab.png" alt="" /></p>
<h2 id="gain-and-phase"><a class="header" href="#gain-and-phase">Gain and Phase</a></h2>
<p>Gain is the <em>ratio of output to input amplitude</em>, ie how much bigger or smaller the output is compared to input.</p>
<p>$$G = \frac{E}{A}(\omega)$$</p>
<p>Phase difference $\phi (\omega )$ is how much the output signal is delayed compared to the input signal. Both are functions of input frequency $\omega$.</p>
<p>The frequency response can be obtained by substituting $j \omega$ for $s$ in the transfer function. This gives a complex function as shown</p>
<p>$$G(s) = \frac{1}{Ts+1} \Longrightarrow G(j\omega) = \frac{1}{Gj\omega + 1}$$</p>
<p>Magnitude $|G(j \omega) |$ gives the amplitude of the response, and the argument of the complex number $\angle G(j \omega)$ gives the phase shift $\phi$. The substitution $s=j \omega$ is used, is because in the Laplace domain, both signals and systems are represented by functions of $s$.</p>
<ul>
<li>The $s$-plane is the complex plane on which Laplace transforms are graphed.</li>
<li>Generally, $s=\sigma + j\omega$</li>
<li>$\sigma$ is the Neper frequency, the rate at which the function decays</li>
<li>$\omega$ is the radial frequency, the rate at which the function oscillates</li>
<li>Periodic sinusoidal inputs are non decaying, so $\sigma = 0$, giving $s=j\omega$</li>
</ul>
<p>To find the frequency response parameters:</p>
<p>$$G(j\omega) = \frac{1}{1 + j \omega T} \times \frac{1 - j \omega T}{1 - j \omega T} = \frac{1 - j \omega T}{1 + \omega^2 T^2} $$
$$ = \frac{1}{1 + \omega^2 T^2} - j \frac{\omega T}{1 + \omega^2 T^2}$$
$$ = Re(G) - j Im(G)$$
$$|G(j \omega) | = \sqrt{(Re(G))^2 + (Im(G))^2} = \frac{1}{\sqrt{1+\omega^2T^2}}$$</p>
<p>$$\angle G(j\omega) = \tan^{-1} \frac{Im(G)}{Re(G)} = - \tan^{-1} \omega T$$</p>
<p>The graphs below show the frequency response in terms of $T$ for varying frequency $\omega$:</p>
<p><img src="es197/./img/freq1-plots.png" alt="" /></p>
<h3 id="example-31"><a class="header" href="#example-31">Example</a></h3>
<p>Given a transfer function $G= \frac{1}{s}$, what is the magnitude and phase of frequency response?
$$G(j \omega) = \frac{1}{j\omega} = \frac{-j}{\omega} = 0 - \frac{1}{\omega}j$$
$$|G(j\omega)| = \sqrt{\frac{1}{\omega^2}} = \frac{1}{\omega}$$
$$\angle G(j\omega) = \tan^{-1} \frac{\frac{-1}{\omega}}{0} = - \frac{\pi}{2}$$</p>
<h2 id="bode-plots"><a class="header" href="#bode-plots">Bode Plots</a></h2>
<p>Bode plots show frequency and amplitude of frequency response on a log$_{10}$ scale. Information is not spread linearly accross the frequency range, so it makes more sense to use a logarithmic scale. An important feature of bode plots is the <em>corner frequency</em>: the frequency at the point where the two asymptotes of the magnitude-frequency graph. This point is where $\omega =\frac{1}{T}$.</p>
<p><img src="es197/./img/bode-lot.png" alt="" /></p>
<p>The plot above is for the function $G(s) = \frac{1}{s+1}$. The gain is measured in decibels $dB$ for the magnitude of the response.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="second-order-frequency-response"><a class="header" href="#second-order-frequency-response">Second Order Frequency Response</a></h1>
<p>How second order systems respond to sinusoidal/oscillating input. Similar to first order.</p>
<h2 id="gain-and-phase-for-second-order-systems"><a class="header" href="#gain-and-phase-for-second-order-systems">Gain and Phase for Second Order Systems</a></h2>
<p>For a 2nd order system in standard input-output form:</p>
<p>$$\frac{1}{\omega_n^2} \frac{d^2}{dt^2}y(t) + \frac{2\zeta}{\omega_n} \frac{d}{dt} y(t) + y(t) = u(t), ;; y(0) = 0, ;; \frac{d}{dt}y(0) = 0$$
$$G(s) = \frac{\omega_n^2}{s^2 + 2\zeta\omega_n s + \omega_n^2}$$
$$G(j\omega) = \frac{\omega^2_n}{(\omega_n^2 - \omega^2) + 2j\zeta \omega_n \omega}$$</p>
<p>The gain and phase of the frequency response are therefore:</p>
<p>$$|G(j\omega)| = \frac{\omega_n^2}{[(\omega_n^2 - \omega^2)^2 + 4 \zeta^2 \omega_n^2 \omega^2]^{1/2}}$$</p>
<p>$$\phi(\omega) = \angle G(j\omega) = -\tan^{-1}\frac{2 \zeta \omega_n \omega}{\omega_n^2 - \omega^2}$$</p>
<h3 id="bode-plots-from-data-book"><a class="header" href="#bode-plots-from-data-book">Bode Plots, from Data Book</a></h3>
<p><img src="es197/./img/freq2-bode.png" alt="" /></p>
<p>The plots show gain and phase shift for varying values of $\zeta$</p>
<h2 id="example-32"><a class="header" href="#example-32">Example</a></h2>
<p>For the electrical system shown below with the values $R = 1 k ,\Omega$, $C = 0.1 , \mu F$, $L = 0.1 H$ find:</p>
<ul>
<li>The undamped natural frequency $\omega_n$</li>
<li>The damping factor $\zeta$</li>
<li>Sketch the magnitude of the frequency response $|G(j\omega)|$
<ul>
<li>At what frequency is this at it's maximum?</li>
</ul>
</li>
<li>Sketch a bode plot using matlab</li>
</ul>
<p><img src="es197/./img/freq2-ex.png" alt="" /></p>
<p>The system equation is:
$$LC \frac{d^2}{dt^2}e_o(t) + RC \frac{d}{dt}e_o(t) + e_o(t) = e_i(t)$$</p>
<p>Undamped natural frequency:
$$\frac{1}{\omega^2_n} = LC = 10^{-8} \Longrightarrow \omega_n = 10^4 ,  rads^{-1} $$</p>
<p>Damping factor:
$$\frac{2\zeta}{\omega_n} = RC = 10^4 \Longrightarrow \zeta = 0.5$$</p>
<p>Using the graph from the data book</p>
<p><img src="es197/./img/freq2-ex1.png" alt="" /></p>
<p>The graph peaks at approx $|G(j\omega)| = 1.15$, so:
$$\omega \approxeq 0.71 \omega_n = 0.71 \times 10^4 ,rads^{-1}$$
$$f = \frac{0.71 \times 10^4}{2 \pi} = 1129 ,Hz$$</p>
<p>Matlab plot:</p>
<pre><code class="language-matlab">R = 1000
C = 10e-7
L = 0.1
sys = tf([1],[L*C R*C 1]); figure; step(sys);
bode(sys);
</code></pre>
<p><img src="es197/./img/freq2-exbode.png" alt="" /></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        
                        
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                
                            </nav>

        </div>

        
        
        
                <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        
        
                <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        
                        <script type="text/javascript">
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>
                
    </body>
</html>
